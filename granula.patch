From 12fc11c0a0d947809dcd407a2a8af9fa59450a43 Mon Sep 17 00:00:00 2001
From: Wing Lung Ngai <winglung.ngai@gmail.com>
Date: Fri, 16 Oct 2015 18:38:09 +0200
Subject: [PATCH] granularized

---
 hadoop-assemblies/pom.xml                          |    4 +-
 hadoop-client/pom.xml                              |   20 +-
 .../hadoop/granular/CountTimeSerieStore.java       |  103 +
 .../apache/hadoop/granular/GranularDataStore.java  |   37 +
 .../org/apache/hadoop/granular/GranularLogger.java |   65 +
 .../hadoop/granular/GranularLoggerStore.java       |   26 +
 .../org/apache/hadoop/granular/IdentifierType.java |   11 +
 .../apache/hadoop/granular/MapReduceV2Term.java    |  106 +
 .../org/apache/hadoop/granular/MatchingMethod.java |   10 +
 .../org/apache/hadoop/granular/TimeSerieStore.java |  101 +
 .../java/org/apache/hadoop/mapred/MapTask.java     | 2015 ++++++++++++++++++++
 .../java/org/apache/hadoop/mapred/ReduceTask.java  |  659 +++++++
 .../main/java/org/apache/hadoop/mapred/Task.java   | 1656 ++++++++++++++++
 .../java/org/apache/hadoop/mapred/YarnChild.java   |  385 ++++
 .../main/java/org/apache/hadoop/mapreduce/Job.java | 1530 +++++++++++++++
 .../hadoop/mapreduce/v2/app/MRAppMaster.java       | 1486 +++++++++++++++
 .../mapreduce/v2/app/rm/RMContainerRequestor.java  |  456 +++++
 hadoop-common-project/hadoop-annotations/pom.xml   |    4 +-
 hadoop-common-project/hadoop-auth-examples/pom.xml |    4 +-
 hadoop-common-project/hadoop-auth/pom.xml          |    4 +-
 hadoop-common-project/hadoop-common/pom.xml        |    4 +-
 hadoop-common-project/hadoop-minikdc/pom.xml       |    4 +-
 hadoop-common-project/hadoop-nfs/pom.xml           |    4 +-
 hadoop-common-project/pom.xml                      |    4 +-
 hadoop-dist/pom.xml                                |    4 +-
 hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml     |    4 +-
 hadoop-hdfs-project/hadoop-hdfs-nfs/pom.xml        |    4 +-
 hadoop-hdfs-project/hadoop-hdfs/pom.xml            |    4 +-
 .../hadoop-hdfs/src/contrib/bkjournal/pom.xml      |    4 +-
 hadoop-hdfs-project/pom.xml                        |    4 +-
 .../hadoop-mapreduce-client-app/pom.xml            |    4 +-
 .../hadoop-mapreduce-client-common/pom.xml         |    4 +-
 .../hadoop-mapreduce-client-core/pom.xml           |    4 +-
 .../hadoop-mapreduce-client-hs-plugins/pom.xml     |    4 +-
 .../hadoop-mapreduce-client-hs/pom.xml             |    4 +-
 .../hadoop-mapreduce-client-jobclient/pom.xml      |    4 +-
 .../hadoop-mapreduce-client-shuffle/pom.xml        |    4 +-
 .../hadoop-mapreduce-client/pom.xml                |    4 +-
 .../hadoop-mapreduce-examples/pom.xml              |    4 +-
 hadoop-mapreduce-project/pom.xml                   |    4 +-
 hadoop-maven-plugins/pom.xml                       |    2 +-
 hadoop-minicluster/pom.xml                         |    4 +-
 hadoop-project-dist/pom.xml                        |    4 +-
 hadoop-project/pom.xml                             |    4 +-
 hadoop-tools/hadoop-archives/pom.xml               |    4 +-
 hadoop-tools/hadoop-datajoin/pom.xml               |    4 +-
 hadoop-tools/hadoop-distcp/pom.xml                 |    4 +-
 hadoop-tools/hadoop-extras/pom.xml                 |    4 +-
 hadoop-tools/hadoop-gridmix/pom.xml                |    4 +-
 hadoop-tools/hadoop-openstack/pom.xml              |    4 +-
 hadoop-tools/hadoop-pipes/pom.xml                  |    4 +-
 hadoop-tools/hadoop-rumen/pom.xml                  |    4 +-
 hadoop-tools/hadoop-sls/pom.xml                    |    4 +-
 hadoop-tools/hadoop-streaming/pom.xml              |    4 +-
 hadoop-tools/hadoop-tools-dist/pom.xml             |    4 +-
 hadoop-tools/pom.xml                               |    4 +-
 .../hadoop-yarn/hadoop-yarn-api/pom.xml            |    4 +-
 .../pom.xml                                        |    4 +-
 .../pom.xml                                        |    4 +-
 .../hadoop-yarn/hadoop-yarn-applications/pom.xml   |    4 +-
 .../hadoop-yarn/hadoop-yarn-client/pom.xml         |    4 +-
 .../hadoop-yarn/hadoop-yarn-common/pom.xml         |    4 +-
 .../pom.xml                                        |    4 +-
 .../hadoop-yarn-server-common/pom.xml              |    4 +-
 .../hadoop-yarn-server-nodemanager/pom.xml         |    4 +-
 .../hadoop-yarn-server-resourcemanager/pom.xml     |    4 +-
 .../hadoop-yarn-server-tests/pom.xml               |    4 +-
 .../hadoop-yarn-server-web-proxy/pom.xml           |    4 +-
 .../hadoop-yarn/hadoop-yarn-server/pom.xml         |    4 +-
 .../hadoop-yarn/hadoop-yarn-site/pom.xml           |    4 +-
 hadoop-yarn-project/hadoop-yarn/pom.xml            |    4 +-
 hadoop-yarn-project/pom.xml                        |    4 +-
 pom.xml                                            |    2 +-
 73 files changed, 8776 insertions(+), 114 deletions(-)
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/CountTimeSerieStore.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/GranularDataStore.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLogger.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLoggerStore.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/IdentifierType.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/MapReduceV2Term.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/MatchingMethod.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/granular/TimeSerieStore.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/mapred/MapTask.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/mapred/ReduceTask.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/mapred/Task.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/mapred/YarnChild.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/mapreduce/Job.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
 create mode 100644 hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java

diff --git a/hadoop-assemblies/pom.xml b/hadoop-assemblies/pom.xml
index 4229c39..649dd94 100644
--- a/hadoop-assemblies/pom.xml
+++ b/hadoop-assemblies/pom.xml
@@ -23,12 +23,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-assemblies</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>Apache Hadoop Assemblies</name>
   <description>Apache Hadoop Assemblies</description>
 
diff --git a/hadoop-client/pom.xml b/hadoop-client/pom.xml
index c6f6c1b..b7c569b 100644
--- a/hadoop-client/pom.xml
+++ b/hadoop-client/pom.xml
@@ -18,12 +18,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project-dist</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project-dist</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-client</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>jar</packaging>
 
   <description>Apache Hadoop Client</description>
@@ -340,5 +340,21 @@
     
   </dependencies>
 
+
+  <build>
+    <plugins>
+      <!-- Java compiler settings -->
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-compiler-plugin</artifactId>
+        <version>3.1</version>
+        <configuration>
+          <source>1.7</source>
+          <target>1.7</target>
+        </configuration>
+      </plugin>
+    </plugins>
+  </build>
+
 </project>
 
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/CountTimeSerieStore.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/CountTimeSerieStore.java
new file mode 100644
index 0000000..d45a5f0
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/CountTimeSerieStore.java
@@ -0,0 +1,103 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.granular;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Print log messages only if the time is met.  Thread-safe.
+ */
+public class CountTimeSerieStore {
+  /** Last time printed */
+  private volatile long lastPrint = System.currentTimeMillis();
+  /** Minimum interval of time to wait before printing */
+  private final int msecs;
+
+  private volatile long count;
+
+  private final int step;
+
+  private List<TimeValue> timeValues;
+
+  /**
+   * Constructor of the timed logger
+   *
+   * @param msecs Msecs to wait before printing again
+   */
+  public CountTimeSerieStore(int msecs, int step) {
+    this.msecs = msecs;
+    this.step = step;
+    timeValues = new ArrayList<>();
+  }
+
+  /**
+   * Print to the info log level if the minimum waiting time was reached.
+   *
+   */
+  public void add(long value) {
+    if (isAddable()) {
+      timeValues.add(new TimeValue(value));
+    }
+  }
+
+  public void empty() {
+    timeValues = new ArrayList<>();
+  }
+
+  /**
+   * Is the log message printable (minimum interval met)?
+   *
+   * @return True if the message is printable
+   */
+  public boolean isAddable() {
+    count++;
+    if(count % step == 0) {
+      if (System.currentTimeMillis() > lastPrint + msecs) {
+        lastPrint = System.currentTimeMillis();
+        return true;
+      }
+    }
+    return false;
+  }
+
+  public void forceadd(long value) {
+    timeValues.add(new TimeValue(value));
+  }
+
+  public String export(){
+    StringBuilder string = new StringBuilder();
+    for (TimeValue timeValue : timeValues) {
+      string.append(timeValue.timestamp + "-" + timeValue.value + "#");
+    }
+    return string.toString();
+  }
+
+
+  private class TimeValue {
+    long timestamp;
+    long value;
+
+    public TimeValue(long value) {
+      this.timestamp = System.currentTimeMillis();
+      this.value = value;
+    }
+  }
+
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularDataStore.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularDataStore.java
new file mode 100644
index 0000000..712a3b0
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularDataStore.java
@@ -0,0 +1,37 @@
+package org.apache.hadoop.granular;
+
+import java.util.concurrent.atomic.AtomicLong;
+
+/**
+ * Created by wing on 27-3-15.
+ */
+public class GranularDataStore {
+
+    public static String workerId;
+
+    public static AtomicLong ContainersLoaded = new AtomicLong();
+
+    public static AtomicLong SentMsgs = new AtomicLong();
+
+    public static AtomicLong LoadedDataVolume = new AtomicLong();
+
+    public static AtomicLong ActiveVertices = new AtomicLong();
+
+    public static AtomicLong SentMsgVolume = new AtomicLong();
+    public static AtomicLong RemoteMsgVolume = new AtomicLong();
+    public static AtomicLong ReceivedMsgVolume = new AtomicLong();
+
+    public static AtomicLong SentReqVolume = new AtomicLong();
+    public static AtomicLong RemoteReqVolume = new AtomicLong();
+    public static AtomicLong ReceivedReqsVolume = new AtomicLong();
+
+    public static long SetupStartTime = Long.MAX_VALUE;
+    public static long ZookeeperStartTime;
+    public static long ZookeeperEndTime;
+    public static long SetupEndTime = Long.MAX_VALUE;
+
+
+    public static TimeSerieStore PartitionTimeSeries = new TimeSerieStore(2 * 1000); //30 * 1000
+    public static TimeSerieStore ReceivedMsgVolumeTimeSeries = new TimeSerieStore(2 * 1000); //30 * 1000
+    public static TimeSerieStore SentMsgVolumeTimeSeries = new TimeSerieStore(2 * 1000); //30 * 1000
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLogger.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLogger.java
new file mode 100644
index 0000000..4f74484
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLogger.java
@@ -0,0 +1,65 @@
+package org.apache.hadoop.granular;
+
+import java.util.UUID;
+
+/**
+ * Created by wing on 28-1-15.
+ */
+public class GranularLogger {
+
+    public long uuid;
+
+    public String actorType;
+    public String actorId;
+    public String missionType;
+    public String missionId;
+
+    public String parentDescriptor;
+
+
+    public GranularLogger(String actorType, String actorId, String missionType, String missionId) {
+        this.uuid = UUID.randomUUID().getLeastSignificantBits() * -1l;
+        this.actorType = actorType;
+        this.actorId = actorId;
+        this.missionType = missionType;
+        this.missionId = missionId;
+        this.parentDescriptor = MapReduceV2Term.UniqueParent;
+    }
+
+    public void setParentDescriptor(String parentDescriptor) {
+        this.parentDescriptor = parentDescriptor;
+    }
+
+    public String logInfo(String name, String value) {
+
+        String escapedValue = value.replaceAll(":", "\\[COLON\\]");
+
+        String info = String.format("InfoName:%s InfoValue:%s Timestamp:%s RecordUuid:%s",
+                name, escapedValue, System.currentTimeMillis(), UUID.randomUUID().getLeastSignificantBits() * -1l);
+
+        return logRecord(info);
+    }
+
+    public String logRecord(String record) {
+        return String.format("GRANULA - %s %s ParentDescriptor:%s", record, logOperation(), parentDescriptor);
+    }
+
+
+
+    private String logOperation() {
+        return String.format("OperationUuid:%s ActorType:%s ActorId:%s MissionType:%s MissionId:%s",
+                uuid, actorType, actorId, missionType, missionId);
+    }
+
+    public void setActorId(String actorId) {
+        this.actorId = actorId;
+    }
+
+    public void setMissionId(String missionId) {
+        this.missionId = missionId;
+    }
+
+    public void resetUuid() {
+        this.uuid = UUID.randomUUID().getLeastSignificantBits() * -1l;
+    }
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLoggerStore.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLoggerStore.java
new file mode 100644
index 0000000..9c5f41f
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/GranularLoggerStore.java
@@ -0,0 +1,26 @@
+package org.apache.hadoop.granular;
+
+/**
+ * Created by wing on 10-4-15.
+ */
+public class GranularLoggerStore {
+
+    public static GranularLogger mapTaskLogger;
+    public static GranularLogger reduceTaskLogger;
+    public static GranularLogger appMasterLogger;
+    public static GranularLogger jobLogger;
+
+    public static void load() {
+
+        jobLogger =  new GranularLogger(MapReduceV2Term.MRApp, IdentifierType.Unique,
+                MapReduceV2Term.MRJob, IdentifierType.Unique);
+
+        mapTaskLogger = new GranularLogger(MapReduceV2Term.Mapper, IdentifierType.Unique,
+                MapReduceV2Term.MapTask, IdentifierType.Unique);
+        reduceTaskLogger = new GranularLogger(MapReduceV2Term.Reducer, IdentifierType.Unique,
+                MapReduceV2Term.ReduceTask, IdentifierType.Unique);
+
+        appMasterLogger = new GranularLogger(MapReduceV2Term.AppMaster, IdentifierType.Unique,
+                MapReduceV2Term.Execution, IdentifierType.Unique);
+    }
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/IdentifierType.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/IdentifierType.java
new file mode 100644
index 0000000..5c22b88
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/IdentifierType.java
@@ -0,0 +1,11 @@
+package org.apache.hadoop.granular;
+
+/**
+ * Created by wing on 4-2-15.
+ */
+public class IdentifierType {
+
+    public static String Unique = "Id.Unique";
+    public static String LocalUnique = "Id.LocalUnique";
+    public static String Unknown = "Id.Unknown";
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/MapReduceV2Term.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/MapReduceV2Term.java
new file mode 100644
index 0000000..0e7b6ce
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/MapReduceV2Term.java
@@ -0,0 +1,106 @@
+package org.apache.hadoop.granular;
+
+/**
+ * Created by wing on 11-2-15.
+ */
+public class MapReduceV2Term {
+
+    //Actors
+    public static String TopActor = "TopActor";
+    public static String AppMaster = "AppMaster";
+    public static String BspMaster = "BspMaster";
+    public static String BspWorker = "BspWorker";
+    public static String GlobalCoordinator = "GlobalCoordinator";
+    public static String UnknownActor = "ActorType.Unknown";
+    public static String UnknownId = "System.UnknownId";
+    public static String Dataloader = "Dataloader";
+
+    //Missions
+
+    public static String MRApp = "MRApp";
+    public static String MRJob = "MRJob";
+
+    public static String MapTask = "MapTask";
+    public static String Mapper = "Mapper";
+    public static String ReduceTask = "ReduceTask";
+    public static String Reducer = "Reducer";
+    public static String JobName = "JobName";
+
+    public static String Execution = "Execution";
+
+    public static String AppStartup = "AppStartup";
+    public static String ContainerAssignment = "ContainerAssignment";
+    public static String AppTermination = "AppTermination";
+    public static String ResponseTime = "ResponseTime";
+
+    public static String Setup = "Setup";
+    public static String ZookeeperSetup = "ZookeeperSetup";
+    public static String GlobalDataload = "GlobalDataload";
+    public static String DataLoad = "DataLoad";
+    public static String PostDataLoad = "PostDataLoad";
+    public static String DataOffload = "DataOffload";
+
+    public static String BspIteration = "BspIteration";
+
+    public static String GlobalSuperstep = "GlobalSuperstep";
+    public static String PrepSuperstep = "PrepSuperstep";
+    public static String Computation = "Computation";
+    public static String MsgSend = "MsgSend";
+    public static String PostSuperstep = "PostSuperstep";
+
+
+    public static String BspCleanup = "BspCleanup";
+    public static String ZookeeperCleanup = "ZookeeperCleanup";
+    public static String FinalCleanup = "FinalCleanup";
+    public static String OutputMerge = "OutputMerge";
+    public static String ZookeeperOfflining = "ZookeeperOfflining";
+    public static String ClientCleanup = "ClientCleanup";
+    public static String ServerCleanup = "ServerCleanup";
+
+    public static String UnknownMission = "MissionType.Unknown";
+    public static String UniqueParent = "UniqueParent";
+
+    //Infos
+    public static String ApplicationID = "ApplicationID";
+    public static String StartTime = "StartTime";
+    public static String EndTime = "EndTime";
+    public static String InputSplit = "InputSplit";
+
+    public static String NumContainers = "NumContainers";
+    public static String ContainerHeapSize = "ContainerHeapSize";
+    public static String ComputationClass = "ComputationClass";
+    public static String DataInputPath = "DataInputPath";
+    public static String ContainersLoaded = "ContainersLoaded";
+
+    public static String LoadedDataVolume = "LoadedDataVolume";
+
+    // dataload
+    public static String TotalNumVertices = "TotalNumVertices";
+    public static String TotalNumFinishedVertices = "TotalNumFinishedVertices";
+    public static String TotalNumEdges = "TotalNumEdges";
+    public static String TotalNumMessages = "TotalNumMessages";
+    public static String TotalNumMessageBytes = "TotalNumMessageBytes";
+    public static String LoadedVertexCount = "LoadedVertexCount";
+    public static String LoadedEdgeCount = "LoadedEdgeCount";
+    public static String LoadedDataSize = "LoadedDataSize";
+
+    // superstep
+    public static String SentMsgs = "SentMsgs";
+    public static String ActiveVertices = "ActiveVertices";
+
+    public static String SentMsgVolume = "SentMsgVolume";
+    public static String RemoteMsgVolume = "RemoteMsgVolume";
+    public static String ReceivedMsgVolume = "ReceivedMsgVolume";
+
+    public static String SentReqVolume = "SentReqVolume";
+    public static String RemoteReqVolume = "ReceivedReqVolume";
+    public static String ReceivedReqVolume = "ReceivedReqVolume";
+
+    public static String ComputeNode = "ComputeNode";
+
+
+
+
+
+
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/MatchingMethod.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/MatchingMethod.java
new file mode 100644
index 0000000..79fbd77
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/MatchingMethod.java
@@ -0,0 +1,10 @@
+package org.apache.hadoop.granular;
+
+/**
+ * Created by wing on 2-2-15.
+ */
+public class MatchingMethod {
+    public static String ByUuid = "ByUuid";
+    public static String ByLoggingCloseness = "ByLoggingCloseness";
+    public static String ByGlobalUniqueness = "ByGlobalUniqueness";
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/granular/TimeSerieStore.java b/hadoop-client/src/main/java/org/apache/hadoop/granular/TimeSerieStore.java
new file mode 100644
index 0000000..620d9b7
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/granular/TimeSerieStore.java
@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.granular;
+
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * Print log messages only if the time is met.  Thread-safe.
+ */
+public class TimeSerieStore {
+  /** Last time printed */
+  private volatile long lastPrint = System.currentTimeMillis();
+  /** Minimum interval of time to wait before printing */
+  private final int msecs;
+
+  private List<TimeValue> timeValues;
+
+  /**
+   * Constructor of the timed logger
+   *
+   * @param msecs Msecs to wait before printing again
+   */
+  public TimeSerieStore(int msecs) {
+    this.msecs = msecs;
+    timeValues = new ArrayList<>();
+  }
+
+  /**
+   * Print to the info log level if the minimum waiting time was reached.
+   *
+   */
+  public void add(long value) {
+    if (isAddable()) {
+      timeValues.add(new TimeValue(value));
+    }
+  }
+
+  public void empty() {
+    timeValues = new ArrayList<>();
+  }
+
+  /**
+   * Is the log message printable (minimum interval met)?
+   *
+   * @return True if the message is printable
+   */
+  public boolean isAddable() {
+    if (System.currentTimeMillis() > lastPrint + msecs) {
+      lastPrint = System.currentTimeMillis();
+      return true;
+    }
+
+    return false;
+  }
+
+  public void forceadd(long value) {
+    timeValues.add(new TimeValue(value));
+  }
+
+  public String export(){
+    StringBuilder string = new StringBuilder();
+    for (TimeValue timeValue : timeValues) {
+      string.append(timeValue.toString() + "#");
+    }
+    return string.toString();
+  }
+
+
+  private class TimeValue {
+    long timestamp;
+    long value;
+
+    public TimeValue(long value) {
+      this.timestamp = System.currentTimeMillis();
+      this.value = value;
+    }
+
+    @Override
+    public String toString() {
+      return timestamp + "-" + value;
+    }
+  }
+
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/mapred/MapTask.java b/hadoop-client/src/main/java/org/apache/hadoop/mapred/MapTask.java
new file mode 100644
index 0000000..d27e267
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/mapred/MapTask.java
@@ -0,0 +1,2015 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.IntBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileSystem.Statistics;
+import org.apache.hadoop.fs.LocalFileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RawLocalFileSystem;
+import org.apache.hadoop.granular.MapReduceV2Term;
+import org.apache.hadoop.granular.GranularLoggerStore;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.hadoop.io.serializer.Deserializer;
+import org.apache.hadoop.io.serializer.SerializationFactory;
+import org.apache.hadoop.io.serializer.Serializer;
+import org.apache.hadoop.mapred.IFile.Writer;
+import org.apache.hadoop.mapred.Merger.Segment;
+import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskCounter;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter;
+import org.apache.hadoop.mapreduce.lib.map.WrappedMapper;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter;
+import org.apache.hadoop.mapreduce.split.JobSplit.TaskSplitIndex;
+import org.apache.hadoop.mapreduce.task.MapContextImpl;
+import org.apache.hadoop.util.IndexedSortable;
+import org.apache.hadoop.util.IndexedSorter;
+import org.apache.hadoop.util.Progress;
+import org.apache.hadoop.util.QuickSort;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringInterner;
+import org.apache.hadoop.util.StringUtils;
+
+/** A Map task. */
+@InterfaceAudience.LimitedPrivate({"MapReduce"})
+@InterfaceStability.Unstable
+public class MapTask extends Task {
+    /**
+     * The size of each record in the index file for the map-outputs.
+     */
+    public static final int MAP_OUTPUT_INDEX_RECORD_LENGTH = 24;
+
+    private TaskSplitIndex splitMetaInfo = new TaskSplitIndex();
+    private final static int APPROX_HEADER_LENGTH = 150;
+
+    private static final Log LOG = LogFactory.getLog(MapTask.class.getName());
+
+    private Progress mapPhase;
+    private Progress sortPhase;
+
+    {   // set phase for this task
+        setPhase(TaskStatus.Phase.MAP);
+        getProgress().setStatus("map");
+    }
+
+    public MapTask() {
+        super();
+    }
+
+    public MapTask(String jobFile, TaskAttemptID taskId,
+                   int partition, TaskSplitIndex splitIndex,
+                   int numSlotsRequired) {
+        super(jobFile, taskId, partition, numSlotsRequired);
+        this.splitMetaInfo = splitIndex;
+    }
+
+    @Override
+    public boolean isMapTask() {
+        return true;
+    }
+
+    @Override
+    public void localizeConfiguration(JobConf conf)
+            throws IOException {
+        super.localizeConfiguration(conf);
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+        super.write(out);
+        if (isMapOrReduce()) {
+            splitMetaInfo.write(out);
+            splitMetaInfo = null;
+        }
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+        super.readFields(in);
+        if (isMapOrReduce()) {
+            splitMetaInfo.readFields(in);
+        }
+    }
+
+    /**
+     * This class wraps the user's record reader to update the counters and progress
+     * as records are read.
+     * @param <K>
+     * @param <V>
+     */
+    class TrackedRecordReader<K, V>
+            implements RecordReader<K,V> {
+        private RecordReader<K,V> rawIn;
+        private Counters.Counter fileInputByteCounter;
+        private Counters.Counter inputRecordCounter;
+        private TaskReporter reporter;
+        private long bytesInPrev = -1;
+        private long bytesInCurr = -1;
+        private final List<Statistics> fsStats;
+
+        TrackedRecordReader(TaskReporter reporter, JobConf job)
+                throws IOException{
+            inputRecordCounter = reporter.getCounter(TaskCounter.MAP_INPUT_RECORDS);
+            fileInputByteCounter = reporter.getCounter(FileInputFormatCounter.BYTES_READ);
+            this.reporter = reporter;
+
+            List<Statistics> matchedStats = null;
+            if (this.reporter.getInputSplit() instanceof FileSplit) {
+                matchedStats = getFsStatistics(((FileSplit) this.reporter
+                        .getInputSplit()).getPath(), job);
+            }
+            fsStats = matchedStats;
+
+            bytesInPrev = getInputBytes(fsStats);
+            rawIn = job.getInputFormat().getRecordReader(reporter.getInputSplit(),
+                    job, reporter);
+            bytesInCurr = getInputBytes(fsStats);
+            fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+        }
+
+        public K createKey() {
+            return rawIn.createKey();
+        }
+
+        public V createValue() {
+            return rawIn.createValue();
+        }
+
+        public synchronized boolean next(K key, V value)
+                throws IOException {
+            boolean ret = moveToNext(key, value);
+            if (ret) {
+                incrCounters();
+            }
+            return ret;
+        }
+
+        protected void incrCounters() {
+            inputRecordCounter.increment(1);
+        }
+
+        protected synchronized boolean moveToNext(K key, V value)
+                throws IOException {
+            bytesInPrev = getInputBytes(fsStats);
+            boolean ret = rawIn.next(key, value);
+            bytesInCurr = getInputBytes(fsStats);
+            fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+            reporter.setProgress(getProgress());
+            return ret;
+        }
+
+        public long getPos() throws IOException { return rawIn.getPos(); }
+
+        public void close() throws IOException {
+            bytesInPrev = getInputBytes(fsStats);
+            rawIn.close();
+            bytesInCurr = getInputBytes(fsStats);
+            fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+        }
+
+        public float getProgress() throws IOException {
+            return rawIn.getProgress();
+        }
+        TaskReporter getTaskReporter() {
+            return reporter;
+        }
+
+        private long getInputBytes(List<Statistics> stats) {
+            if (stats == null) return 0;
+            long bytesRead = 0;
+            for (Statistics stat: stats) {
+                bytesRead = bytesRead + stat.getBytesRead();
+            }
+            return bytesRead;
+        }
+    }
+
+    /**
+     * This class skips the records based on the failed ranges from previous
+     * attempts.
+     */
+    class SkippingRecordReader<K, V> extends TrackedRecordReader<K,V> {
+        private SkipRangeIterator skipIt;
+        private SequenceFile.Writer skipWriter;
+        private boolean toWriteSkipRecs;
+        private TaskUmbilicalProtocol umbilical;
+        private Counters.Counter skipRecCounter;
+        private long recIndex = -1;
+
+        SkippingRecordReader(TaskUmbilicalProtocol umbilical,
+                             TaskReporter reporter, JobConf job) throws IOException{
+            super(reporter, job);
+            this.umbilical = umbilical;
+            this.skipRecCounter = reporter.getCounter(TaskCounter.MAP_SKIPPED_RECORDS);
+            this.toWriteSkipRecs = toWriteSkipRecs() &&
+                    SkipBadRecords.getSkipOutputPath(conf)!=null;
+            skipIt = getSkipRanges().skipRangeIterator();
+        }
+
+        public synchronized boolean next(K key, V value)
+                throws IOException {
+            if(!skipIt.hasNext()) {
+                LOG.warn("Further records got skipped.");
+                return false;
+            }
+            boolean ret = moveToNext(key, value);
+            long nextRecIndex = skipIt.next();
+            long skip = 0;
+            while(recIndex<nextRecIndex && ret) {
+                if(toWriteSkipRecs) {
+                    writeSkippedRec(key, value);
+                }
+                ret = moveToNext(key, value);
+                skip++;
+            }
+            //close the skip writer once all the ranges are skipped
+            if(skip>0 && skipIt.skippedAllRanges() && skipWriter!=null) {
+                skipWriter.close();
+            }
+            skipRecCounter.increment(skip);
+            reportNextRecordRange(umbilical, recIndex);
+            if (ret) {
+                incrCounters();
+            }
+            return ret;
+        }
+
+        protected synchronized boolean moveToNext(K key, V value)
+                throws IOException {
+            recIndex++;
+            return super.moveToNext(key, value);
+        }
+
+        @SuppressWarnings("unchecked")
+        private void writeSkippedRec(K key, V value) throws IOException{
+            if(skipWriter==null) {
+                Path skipDir = SkipBadRecords.getSkipOutputPath(conf);
+                Path skipFile = new Path(skipDir, getTaskID().toString());
+                skipWriter =
+                        SequenceFile.createWriter(
+                                skipFile.getFileSystem(conf), conf, skipFile,
+                                (Class<K>) createKey().getClass(),
+                                (Class<V>) createValue().getClass(),
+                                CompressionType.BLOCK, getTaskReporter());
+            }
+            skipWriter.append(key, value);
+        }
+    }
+
+    @Override
+    public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)
+            throws IOException, ClassNotFoundException, InterruptedException {
+        this.umbilical = umbilical;
+
+        String[] ids = getTaskID().toString().split("_");
+        String applicationId = ids[1] + "_" + ids[2];
+        String taskId = ids[3] + "_" + ids[4] + "_" + ids[5];
+
+        GranularLoggerStore.mapTaskLogger.setActorId(applicationId + "_" + taskId);
+        LOG.info(GranularLoggerStore.mapTaskLogger.logInfo(MapReduceV2Term.StartTime, String.valueOf(System.currentTimeMillis())));
+        LOG.info(GranularLoggerStore.mapTaskLogger.logInfo(MapReduceV2Term.ApplicationID, applicationId));
+
+        if (isMapTask()) {
+            // If there are no reducers then there won't be any sort. Hence the map
+            // phase will govern the entire attempt's progress.
+            if (conf.getNumReduceTasks() == 0) {
+                mapPhase = getProgress().addPhase("map", 1.0f);
+            } else {
+                // If there are reducers then the entire attempt's progress will be
+                // split between the map phase (67%) and the sort phase (33%).
+                mapPhase = getProgress().addPhase("map", 0.667f);
+                sortPhase  = getProgress().addPhase("sort", 0.333f);
+            }
+        }
+        TaskReporter reporter = startReporter(umbilical);
+
+
+
+        boolean useNewApi = job.getUseNewMapper();
+        initialize(job, getJobID(), reporter, useNewApi);
+
+        // check if it is a cleanupJobTask
+        if (jobCleanup) {
+            runJobCleanupTask(umbilical, reporter);
+            return;
+        }
+        if (jobSetup) {
+            runJobSetupTask(umbilical, reporter);
+            return;
+        }
+        if (taskCleanup) {
+            runTaskCleanupTask(umbilical, reporter);
+            return;
+        }
+
+        if (useNewApi) {
+            runNewMapper(job, splitMetaInfo, umbilical, reporter);
+        } else {
+            runOldMapper(job, splitMetaInfo, umbilical, reporter);
+        }
+        done(umbilical, reporter);
+
+
+        LOG.info(GranularLoggerStore.mapTaskLogger.logInfo(MapReduceV2Term.EndTime, String.valueOf(System.currentTimeMillis())));
+    }
+
+    public Progress getSortPhase() {
+        return sortPhase;
+    }
+
+    @SuppressWarnings("unchecked")
+    private <T> T getSplitDetails(Path file, long offset)
+            throws IOException {
+        FileSystem fs = file.getFileSystem(conf);
+        FSDataInputStream inFile = fs.open(file);
+        inFile.seek(offset);
+        String className = StringInterner.weakIntern(Text.readString(inFile));
+        Class<T> cls;
+        try {
+            cls = (Class<T>) conf.getClassByName(className);
+        } catch (ClassNotFoundException ce) {
+            IOException wrap = new IOException("Split class " + className +
+                    " not found");
+            wrap.initCause(ce);
+            throw wrap;
+        }
+        SerializationFactory factory = new SerializationFactory(conf);
+        Deserializer<T> deserializer =
+                (Deserializer<T>) factory.getDeserializer(cls);
+        deserializer.open(inFile);
+        T split = deserializer.deserialize(null);
+        long pos = inFile.getPos();
+        getCounters().findCounter(
+                TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);
+        inFile.close();
+        return split;
+    }
+
+    @SuppressWarnings("unchecked")
+    private <KEY, VALUE> MapOutputCollector<KEY, VALUE>
+    createSortingCollector(JobConf job, TaskReporter reporter)
+            throws IOException, ClassNotFoundException {
+        MapOutputCollector<KEY, VALUE> collector
+                = (MapOutputCollector<KEY, VALUE>)
+                ReflectionUtils.newInstance(
+                        job.getClass(JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR,
+                                MapOutputBuffer.class, MapOutputCollector.class), job);
+        LOG.info("Map output collector class = " + collector.getClass().getName());
+        MapOutputCollector.Context context =
+                new MapOutputCollector.Context(this, job, reporter);
+        collector.init(context);
+        return collector;
+    }
+
+    @SuppressWarnings("unchecked")
+    private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+    void runOldMapper(final JobConf job,
+                      final TaskSplitIndex splitIndex,
+                      final TaskUmbilicalProtocol umbilical,
+                      TaskReporter reporter
+    ) throws IOException, InterruptedException,
+            ClassNotFoundException {
+        InputSplit inputSplit = getSplitDetails(new Path(splitIndex.getSplitLocation()),
+                splitIndex.getStartOffset());
+
+        updateJobWithSplit(job, inputSplit);
+        reporter.setInputSplit(inputSplit);
+
+        RecordReader<INKEY,INVALUE> in = isSkipping() ?
+                new SkippingRecordReader<INKEY,INVALUE>(umbilical, reporter, job) :
+                new TrackedRecordReader<INKEY,INVALUE>(reporter, job);
+        job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());
+
+
+        int numReduceTasks = conf.getNumReduceTasks();
+        LOG.info("numReduceTasks: " + numReduceTasks);
+        MapOutputCollector<OUTKEY, OUTVALUE> collector = null;
+        if (numReduceTasks > 0) {
+            collector = createSortingCollector(job, reporter);
+        } else {
+            collector = new DirectMapOutputCollector<OUTKEY, OUTVALUE>();
+            MapOutputCollector.Context context =
+                    new MapOutputCollector.Context(this, job, reporter);
+            collector.init(context);
+        }
+        MapRunnable<INKEY,INVALUE,OUTKEY,OUTVALUE> runner =
+                ReflectionUtils.newInstance(job.getMapRunnerClass(), job);
+
+        LOG.info("Mapper runner class = " + runner.getClass().getName());
+
+        try {
+            runner.run(in, new OldOutputCollector(collector, conf), reporter);
+            mapPhase.complete();
+            // start the sort phase only if there are reducers
+            if (numReduceTasks > 0) {
+                setPhase(TaskStatus.Phase.SORT);
+            }
+            statusUpdate(umbilical);
+            collector.flush();
+
+            in.close();
+            in = null;
+
+            collector.close();
+            collector = null;
+        } finally {
+            closeQuietly(in);
+            closeQuietly(collector);
+        }
+    }
+
+    /**
+     * Update the job with details about the file split
+     * @param job the job configuration to update
+     * @param inputSplit the file split
+     */
+    private void updateJobWithSplit(final JobConf job, InputSplit inputSplit) {
+        if (inputSplit instanceof FileSplit) {
+            FileSplit fileSplit = (FileSplit) inputSplit;
+            job.set(JobContext.MAP_INPUT_FILE, fileSplit.getPath().toString());
+            job.setLong(JobContext.MAP_INPUT_START, fileSplit.getStart());
+            job.setLong(JobContext.MAP_INPUT_PATH, fileSplit.getLength());
+        }
+        LOG.info("Processing split: " + inputSplit);
+        LOG.info(GranularLoggerStore.mapTaskLogger.logInfo(MapReduceV2Term.InputSplit, inputSplit.toString()));
+    }
+
+    static class NewTrackingRecordReader<K,V>
+            extends org.apache.hadoop.mapreduce.RecordReader<K,V> {
+        private final org.apache.hadoop.mapreduce.RecordReader<K,V> real;
+        private final org.apache.hadoop.mapreduce.Counter inputRecordCounter;
+        private final org.apache.hadoop.mapreduce.Counter fileInputByteCounter;
+        private final TaskReporter reporter;
+        private final List<Statistics> fsStats;
+
+        NewTrackingRecordReader(org.apache.hadoop.mapreduce.InputSplit split,
+                                org.apache.hadoop.mapreduce.InputFormat<K, V> inputFormat,
+                                TaskReporter reporter,
+                                org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)
+                throws InterruptedException, IOException {
+            this.reporter = reporter;
+            this.inputRecordCounter = reporter
+                    .getCounter(TaskCounter.MAP_INPUT_RECORDS);
+            this.fileInputByteCounter = reporter
+                    .getCounter(FileInputFormatCounter.BYTES_READ);
+
+            List <Statistics> matchedStats = null;
+            if (split instanceof org.apache.hadoop.mapreduce.lib.input.FileSplit) {
+                matchedStats = getFsStatistics(((org.apache.hadoop.mapreduce.lib.input.FileSplit) split)
+                        .getPath(), taskContext.getConfiguration());
+            }
+            fsStats = matchedStats;
+
+            long bytesInPrev = getInputBytes(fsStats);
+            this.real = inputFormat.createRecordReader(split, taskContext);
+            long bytesInCurr = getInputBytes(fsStats);
+            fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+        }
+
+        @Override
+        public void close() throws IOException {
+            long bytesInPrev = getInputBytes(fsStats);
+            real.close();
+            long bytesInCurr = getInputBytes(fsStats);
+            fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+        }
+
+        @Override
+        public K getCurrentKey() throws IOException, InterruptedException {
+            return real.getCurrentKey();
+        }
+
+        @Override
+        public V getCurrentValue() throws IOException, InterruptedException {
+            return real.getCurrentValue();
+        }
+
+        @Override
+        public float getProgress() throws IOException, InterruptedException {
+            return real.getProgress();
+        }
+
+        @Override
+        public void initialize(org.apache.hadoop.mapreduce.InputSplit split,
+                               org.apache.hadoop.mapreduce.TaskAttemptContext context
+        ) throws IOException, InterruptedException {
+            long bytesInPrev = getInputBytes(fsStats);
+            real.initialize(split, context);
+            long bytesInCurr = getInputBytes(fsStats);
+            fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+        }
+
+        @Override
+        public boolean nextKeyValue() throws IOException, InterruptedException {
+            long bytesInPrev = getInputBytes(fsStats);
+            boolean result = real.nextKeyValue();
+            long bytesInCurr = getInputBytes(fsStats);
+            if (result) {
+                inputRecordCounter.increment(1);
+            }
+            fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
+            reporter.setProgress(getProgress());
+            return result;
+        }
+
+        private long getInputBytes(List<Statistics> stats) {
+            if (stats == null) return 0;
+            long bytesRead = 0;
+            for (Statistics stat: stats) {
+                bytesRead = bytesRead + stat.getBytesRead();
+            }
+            return bytesRead;
+        }
+    }
+
+    /**
+     * Since the mapred and mapreduce Partitioners don't share a common interface
+     * (JobConfigurable is deprecated and a subtype of mapred.Partitioner), the
+     * partitioner lives in Old/NewOutputCollector. Note that, for map-only jobs,
+     * the configured partitioner should not be called. It's common for
+     * partitioners to compute a result mod numReduces, which causes a div0 error
+     */
+    private static class OldOutputCollector<K,V> implements OutputCollector<K,V> {
+        private final Partitioner<K,V> partitioner;
+        private final MapOutputCollector<K,V> collector;
+        private final int numPartitions;
+
+        @SuppressWarnings("unchecked")
+        OldOutputCollector(MapOutputCollector<K,V> collector, JobConf conf) {
+            numPartitions = conf.getNumReduceTasks();
+            if (numPartitions > 1) {
+                partitioner = (Partitioner<K,V>)
+                        ReflectionUtils.newInstance(conf.getPartitionerClass(), conf);
+            } else {
+                partitioner = new Partitioner<K,V>() {
+                    @Override
+                    public void configure(JobConf job) { }
+                    @Override
+                    public int getPartition(K key, V value, int numPartitions) {
+                        return numPartitions - 1;
+                    }
+                };
+            }
+            this.collector = collector;
+        }
+
+        @Override
+        public void collect(K key, V value) throws IOException {
+            try {
+                collector.collect(key, value,
+                        partitioner.getPartition(key, value, numPartitions));
+            } catch (InterruptedException ie) {
+                Thread.currentThread().interrupt();
+                throw new IOException("interrupt exception", ie);
+            }
+        }
+    }
+
+    private class NewDirectOutputCollector<K,V>
+            extends org.apache.hadoop.mapreduce.RecordWriter<K,V> {
+        private final org.apache.hadoop.mapreduce.RecordWriter out;
+
+        private final TaskReporter reporter;
+
+        private final Counters.Counter mapOutputRecordCounter;
+        private final Counters.Counter fileOutputByteCounter;
+        private final List<Statistics> fsStats;
+
+        @SuppressWarnings("unchecked")
+        NewDirectOutputCollector(MRJobConfig jobContext,
+                                 JobConf job, TaskUmbilicalProtocol umbilical, TaskReporter reporter)
+                throws IOException, ClassNotFoundException, InterruptedException {
+            this.reporter = reporter;
+            mapOutputRecordCounter = reporter
+                    .getCounter(TaskCounter.MAP_OUTPUT_RECORDS);
+            fileOutputByteCounter = reporter
+                    .getCounter(FileOutputFormatCounter.BYTES_WRITTEN);
+
+            List<Statistics> matchedStats = null;
+            if (outputFormat instanceof org.apache.hadoop.mapreduce.lib.output.FileOutputFormat) {
+                matchedStats = getFsStatistics(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
+                        .getOutputPath(taskContext), taskContext.getConfiguration());
+            }
+            fsStats = matchedStats;
+
+            long bytesOutPrev = getOutputBytes(fsStats);
+            out = outputFormat.getRecordWriter(taskContext);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        }
+
+        @Override
+        @SuppressWarnings("unchecked")
+        public void write(K key, V value)
+                throws IOException, InterruptedException {
+            reporter.progress();
+            long bytesOutPrev = getOutputBytes(fsStats);
+            out.write(key, value);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+            mapOutputRecordCounter.increment(1);
+        }
+
+        @Override
+        public void close(TaskAttemptContext context)
+                throws IOException,InterruptedException {
+            reporter.progress();
+            if (out != null) {
+                long bytesOutPrev = getOutputBytes(fsStats);
+                out.close(context);
+                long bytesOutCurr = getOutputBytes(fsStats);
+                fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+            }
+        }
+
+        private long getOutputBytes(List<Statistics> stats) {
+            if (stats == null) return 0;
+            long bytesWritten = 0;
+            for (Statistics stat: stats) {
+                bytesWritten = bytesWritten + stat.getBytesWritten();
+            }
+            return bytesWritten;
+        }
+    }
+
+    private class NewOutputCollector<K,V>
+            extends org.apache.hadoop.mapreduce.RecordWriter<K,V> {
+        private final MapOutputCollector<K,V> collector;
+        private final org.apache.hadoop.mapreduce.Partitioner<K,V> partitioner;
+        private final int partitions;
+
+        @SuppressWarnings("unchecked")
+        NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,
+                           JobConf job,
+                           TaskUmbilicalProtocol umbilical,
+                           TaskReporter reporter
+        ) throws IOException, ClassNotFoundException {
+            collector = createSortingCollector(job, reporter);
+            partitions = jobContext.getNumReduceTasks();
+            if (partitions > 1) {
+                partitioner = (org.apache.hadoop.mapreduce.Partitioner<K,V>)
+                        ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);
+            } else {
+                partitioner = new org.apache.hadoop.mapreduce.Partitioner<K,V>() {
+                    @Override
+                    public int getPartition(K key, V value, int numPartitions) {
+                        return partitions - 1;
+                    }
+                };
+            }
+        }
+
+        @Override
+        public void write(K key, V value) throws IOException, InterruptedException {
+            collector.collect(key, value,
+                    partitioner.getPartition(key, value, partitions));
+        }
+
+        @Override
+        public void close(TaskAttemptContext context
+        ) throws IOException,InterruptedException {
+            try {
+                collector.flush();
+            } catch (ClassNotFoundException cnf) {
+                throw new IOException("can't find class ", cnf);
+            }
+            collector.close();
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+    void runNewMapper(final JobConf job,
+                      final TaskSplitIndex splitIndex,
+                      final TaskUmbilicalProtocol umbilical,
+                      TaskReporter reporter
+    ) throws IOException, ClassNotFoundException,
+            InterruptedException {
+        // make a task context so we can get the classes
+        org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
+                new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,
+                        getTaskID(),
+                        reporter);
+        // make a mapper
+        org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =
+                (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)
+                        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);
+        // make the input format
+        org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =
+                (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)
+                        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);
+        // rebuild the input split
+        org.apache.hadoop.mapreduce.InputSplit split = null;
+        split = getSplitDetails(new Path(splitIndex.getSplitLocation()),
+                splitIndex.getStartOffset());
+        LOG.info("Processing split: " + split);
+
+        org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =
+                new NewTrackingRecordReader<INKEY,INVALUE>
+                        (split, inputFormat, reporter, taskContext);
+
+        job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());
+        org.apache.hadoop.mapreduce.RecordWriter output = null;
+
+        // get an output object
+        if (job.getNumReduceTasks() == 0) {
+            output =
+                    new NewDirectOutputCollector(taskContext, job, umbilical, reporter);
+        } else {
+            output = new NewOutputCollector(taskContext, job, umbilical, reporter);
+        }
+
+        org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE>
+                mapContext =
+                new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(),
+                        input, output,
+                        committer,
+                        reporter, split);
+
+        org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context
+                mapperContext =
+                new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(
+                        mapContext);
+
+        try {
+            input.initialize(split, mapperContext);
+            mapper.run(mapperContext);
+            mapPhase.complete();
+            setPhase(TaskStatus.Phase.SORT);
+            statusUpdate(umbilical);
+            input.close();
+            input = null;
+            output.close(mapperContext);
+            output = null;
+        } finally {
+            closeQuietly(input);
+            closeQuietly(output, mapperContext);
+        }
+    }
+
+    class DirectMapOutputCollector<K, V>
+            implements MapOutputCollector<K, V> {
+
+        private RecordWriter<K, V> out = null;
+
+        private TaskReporter reporter = null;
+
+        private Counters.Counter mapOutputRecordCounter;
+        private Counters.Counter fileOutputByteCounter;
+        private List<Statistics> fsStats;
+
+        public DirectMapOutputCollector() {
+        }
+
+        @SuppressWarnings("unchecked")
+        public void init(MapOutputCollector.Context context
+        ) throws IOException, ClassNotFoundException {
+            this.reporter = context.getReporter();
+            JobConf job = context.getJobConf();
+            String finalName = getOutputName(getPartition());
+            FileSystem fs = FileSystem.get(job);
+
+            OutputFormat<K, V> outputFormat = job.getOutputFormat();
+            mapOutputRecordCounter = reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);
+
+            fileOutputByteCounter = reporter
+                    .getCounter(FileOutputFormatCounter.BYTES_WRITTEN);
+
+            List<Statistics> matchedStats = null;
+            if (outputFormat instanceof FileOutputFormat) {
+                matchedStats = getFsStatistics(FileOutputFormat.getOutputPath(job), job);
+            }
+            fsStats = matchedStats;
+
+            long bytesOutPrev = getOutputBytes(fsStats);
+            out = job.getOutputFormat().getRecordWriter(fs, job, finalName, reporter);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        }
+
+        public void close() throws IOException {
+            if (this.out != null) {
+                long bytesOutPrev = getOutputBytes(fsStats);
+                out.close(this.reporter);
+                long bytesOutCurr = getOutputBytes(fsStats);
+                fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+            }
+
+        }
+
+        public void flush() throws IOException, InterruptedException,
+                ClassNotFoundException {
+        }
+
+        public void collect(K key, V value, int partition) throws IOException {
+            reporter.progress();
+            long bytesOutPrev = getOutputBytes(fsStats);
+            out.write(key, value);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+            mapOutputRecordCounter.increment(1);
+        }
+
+        private long getOutputBytes(List<Statistics> stats) {
+            if (stats == null) return 0;
+            long bytesWritten = 0;
+            for (Statistics stat: stats) {
+                bytesWritten = bytesWritten + stat.getBytesWritten();
+            }
+            return bytesWritten;
+        }
+    }
+
+    @InterfaceAudience.LimitedPrivate({"MapReduce"})
+    @InterfaceStability.Unstable
+    public static class MapOutputBuffer<K extends Object, V extends Object>
+            implements MapOutputCollector<K, V>, IndexedSortable {
+        private int partitions;
+        private JobConf job;
+        private TaskReporter reporter;
+        private Class<K> keyClass;
+        private Class<V> valClass;
+        private RawComparator<K> comparator;
+        private SerializationFactory serializationFactory;
+        private Serializer<K> keySerializer;
+        private Serializer<V> valSerializer;
+        private CombinerRunner<K,V> combinerRunner;
+        private CombineOutputCollector<K, V> combineCollector;
+
+        // Compression for map-outputs
+        private CompressionCodec codec;
+
+        // k/v accounting
+        private IntBuffer kvmeta; // metadata overlay on backing store
+        int kvstart;            // marks origin of spill metadata
+        int kvend;              // marks end of spill metadata
+        int kvindex;            // marks end of fully serialized records
+
+        int equator;            // marks origin of meta/serialization
+        int bufstart;           // marks beginning of spill
+        int bufend;             // marks beginning of collectable
+        int bufmark;            // marks end of record
+        int bufindex;           // marks end of collected
+        int bufvoid;            // marks the point where we should stop
+        // reading at the end of the buffer
+
+        byte[] kvbuffer;        // main output buffer
+        private final byte[] b0 = new byte[0];
+
+        private static final int VALSTART = 0;         // val offset in acct
+        private static final int KEYSTART = 1;         // key offset in acct
+        private static final int PARTITION = 2;        // partition offset in acct
+        private static final int VALLEN = 3;           // length of value
+        private static final int NMETA = 4;            // num meta ints
+        private static final int METASIZE = NMETA * 4; // size in bytes
+
+        // spill accounting
+        private int maxRec;
+        private int softLimit;
+        boolean spillInProgress;;
+        int bufferRemaining;
+        volatile Throwable sortSpillException = null;
+
+        int numSpills = 0;
+        private int minSpillsForCombine;
+        private IndexedSorter sorter;
+        final ReentrantLock spillLock = new ReentrantLock();
+        final Condition spillDone = spillLock.newCondition();
+        final Condition spillReady = spillLock.newCondition();
+        final BlockingBuffer bb = new BlockingBuffer();
+        volatile boolean spillThreadRunning = false;
+        final SpillThread spillThread = new SpillThread();
+
+        private FileSystem rfs;
+
+        // Counters
+        private Counters.Counter mapOutputByteCounter;
+        private Counters.Counter mapOutputRecordCounter;
+        private Counters.Counter fileOutputByteCounter;
+
+        final ArrayList<SpillRecord> indexCacheList =
+                new ArrayList<SpillRecord>();
+        private int totalIndexCacheMemory;
+        private int indexCacheMemoryLimit;
+        private static final int INDEX_CACHE_MEMORY_LIMIT_DEFAULT = 1024 * 1024;
+
+        private MapTask mapTask;
+        private MapOutputFile mapOutputFile;
+        private Progress sortPhase;
+        private Counters.Counter spilledRecordsCounter;
+
+        public MapOutputBuffer() {
+        }
+
+        @SuppressWarnings("unchecked")
+        public void init(MapOutputCollector.Context context
+        ) throws IOException, ClassNotFoundException {
+            job = context.getJobConf();
+            reporter = context.getReporter();
+            mapTask = context.getMapTask();
+            mapOutputFile = mapTask.getMapOutputFile();
+            sortPhase = mapTask.getSortPhase();
+            spilledRecordsCounter = reporter.getCounter(TaskCounter.SPILLED_RECORDS);
+            partitions = job.getNumReduceTasks();
+            rfs = ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();
+
+            //sanity checks
+            final float spillper =
+                    job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);
+            final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);
+            indexCacheMemoryLimit = job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,
+                    INDEX_CACHE_MEMORY_LIMIT_DEFAULT);
+            if (spillper > (float)1.0 || spillper <= (float)0.0) {
+                throw new IOException("Invalid \"" + JobContext.MAP_SORT_SPILL_PERCENT +
+                        "\": " + spillper);
+            }
+            if ((sortmb & 0x7FF) != sortmb) {
+                throw new IOException(
+                        "Invalid \"" + JobContext.IO_SORT_MB + "\": " + sortmb);
+            }
+            sorter = ReflectionUtils.newInstance(job.getClass("map.sort.class",
+                    QuickSort.class, IndexedSorter.class), job);
+            // buffers and accounting
+            int maxMemUsage = sortmb << 20;
+            maxMemUsage -= maxMemUsage % METASIZE;
+            kvbuffer = new byte[maxMemUsage];
+            bufvoid = kvbuffer.length;
+            kvmeta = ByteBuffer.wrap(kvbuffer)
+                    .order(ByteOrder.nativeOrder())
+                    .asIntBuffer();
+            setEquator(0);
+            bufstart = bufend = bufindex = equator;
+            kvstart = kvend = kvindex;
+
+            maxRec = kvmeta.capacity() / NMETA;
+            softLimit = (int)(kvbuffer.length * spillper);
+            bufferRemaining = softLimit;
+            LOG.info(JobContext.IO_SORT_MB + ": " + sortmb);
+            LOG.info("soft limit at " + softLimit);
+            LOG.info("bufstart = " + bufstart + "; bufvoid = " + bufvoid);
+            LOG.info("kvstart = " + kvstart + "; length = " + maxRec);
+
+            // k/v serialization
+            comparator = job.getOutputKeyComparator();
+            keyClass = (Class<K>)job.getMapOutputKeyClass();
+            valClass = (Class<V>)job.getMapOutputValueClass();
+            serializationFactory = new SerializationFactory(job);
+            keySerializer = serializationFactory.getSerializer(keyClass);
+            keySerializer.open(bb);
+            valSerializer = serializationFactory.getSerializer(valClass);
+            valSerializer.open(bb);
+
+            // output counters
+            mapOutputByteCounter = reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);
+            mapOutputRecordCounter =
+                    reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);
+            fileOutputByteCounter = reporter
+                    .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);
+
+            // compression
+            if (job.getCompressMapOutput()) {
+                Class<? extends CompressionCodec> codecClass =
+                        job.getMapOutputCompressorClass(DefaultCodec.class);
+                codec = ReflectionUtils.newInstance(codecClass, job);
+            } else {
+                codec = null;
+            }
+
+            // combiner
+            final Counters.Counter combineInputCounter =
+                    reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);
+            combinerRunner = CombinerRunner.create(job, getTaskID(),
+                    combineInputCounter,
+                    reporter, null);
+            if (combinerRunner != null) {
+                final Counters.Counter combineOutputCounter =
+                        reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);
+                combineCollector= new CombineOutputCollector<K,V>(combineOutputCounter, reporter, job);
+            } else {
+                combineCollector = null;
+            }
+            spillInProgress = false;
+            minSpillsForCombine = job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);
+            spillThread.setDaemon(true);
+            spillThread.setName("SpillThread");
+            spillLock.lock();
+            try {
+                spillThread.start();
+                while (!spillThreadRunning) {
+                    spillDone.await();
+                }
+            } catch (InterruptedException e) {
+                throw new IOException("Spill thread failed to initialize", e);
+            } finally {
+                spillLock.unlock();
+            }
+            if (sortSpillException != null) {
+                throw new IOException("Spill thread failed to initialize",
+                        sortSpillException);
+            }
+        }
+
+        /**
+         * Serialize the key, value to intermediate storage.
+         * When this method returns, kvindex must refer to sufficient unused
+         * storage to store one METADATA.
+         */
+        public synchronized void collect(K key, V value, final int partition
+        ) throws IOException {
+            reporter.progress();
+            if (key.getClass() != keyClass) {
+                throw new IOException("Type mismatch in key from map: expected "
+                        + keyClass.getName() + ", received "
+                        + key.getClass().getName());
+            }
+            if (value.getClass() != valClass) {
+                throw new IOException("Type mismatch in value from map: expected "
+                        + valClass.getName() + ", received "
+                        + value.getClass().getName());
+            }
+            if (partition < 0 || partition >= partitions) {
+                throw new IOException("Illegal partition for " + key + " (" +
+                        partition + ")");
+            }
+            checkSpillException();
+            bufferRemaining -= METASIZE;
+            if (bufferRemaining <= 0) {
+                // start spill if the thread is not running and the soft limit has been
+                // reached
+                spillLock.lock();
+                try {
+                    do {
+                        if (!spillInProgress) {
+                            final int kvbidx = 4 * kvindex;
+                            final int kvbend = 4 * kvend;
+                            // serialized, unspilled bytes always lie between kvindex and
+                            // bufindex, crossing the equator. Note that any void space
+                            // created by a reset must be included in "used" bytes
+                            final int bUsed = distanceTo(kvbidx, bufindex);
+                            final boolean bufsoftlimit = bUsed >= softLimit;
+                            if ((kvbend + METASIZE) % kvbuffer.length !=
+                                    equator - (equator % METASIZE)) {
+                                // spill finished, reclaim space
+                                resetSpill();
+                                bufferRemaining = Math.min(
+                                        distanceTo(bufindex, kvbidx) - 2 * METASIZE,
+                                        softLimit - bUsed) - METASIZE;
+                                continue;
+                            } else if (bufsoftlimit && kvindex != kvend) {
+                                // spill records, if any collected; check latter, as it may
+                                // be possible for metadata alignment to hit spill pcnt
+                                startSpill();
+                                final int avgRec = (int)
+                                        (mapOutputByteCounter.getCounter() /
+                                                mapOutputRecordCounter.getCounter());
+                                // leave at least half the split buffer for serialization data
+                                // ensure that kvindex >= bufindex
+                                final int distkvi = distanceTo(bufindex, kvbidx);
+                                final int newPos = (bufindex +
+                                        Math.max(2 * METASIZE - 1,
+                                                Math.min(distkvi / 2,
+                                                        distkvi / (METASIZE + avgRec) * METASIZE)))
+                                        % kvbuffer.length;
+                                setEquator(newPos);
+                                bufmark = bufindex = newPos;
+                                final int serBound = 4 * kvend;
+                                // bytes remaining before the lock must be held and limits
+                                // checked is the minimum of three arcs: the metadata space, the
+                                // serialization space, and the soft limit
+                                bufferRemaining = Math.min(
+                                        // metadata max
+                                        distanceTo(bufend, newPos),
+                                        Math.min(
+                                                // serialization max
+                                                distanceTo(newPos, serBound),
+                                                // soft limit
+                                                softLimit)) - 2 * METASIZE;
+                            }
+                        }
+                    } while (false);
+                } finally {
+                    spillLock.unlock();
+                }
+            }
+
+            try {
+                // serialize key bytes into buffer
+                int keystart = bufindex;
+                keySerializer.serialize(key);
+                if (bufindex < keystart) {
+                    // wrapped the key; must make contiguous
+                    bb.shiftBufferedKey();
+                    keystart = 0;
+                }
+                // serialize value bytes into buffer
+                final int valstart = bufindex;
+                valSerializer.serialize(value);
+                // It's possible for records to have zero length, i.e. the serializer
+                // will perform no writes. To ensure that the boundary conditions are
+                // checked and that the kvindex invariant is maintained, perform a
+                // zero-length write into the buffer. The logic monitoring this could be
+                // moved into collect, but this is cleaner and inexpensive. For now, it
+                // is acceptable.
+                bb.write(b0, 0, 0);
+
+                // the record must be marked after the preceding write, as the metadata
+                // for this record are not yet written
+                int valend = bb.markRecord();
+
+                mapOutputRecordCounter.increment(1);
+                mapOutputByteCounter.increment(
+                        distanceTo(keystart, valend, bufvoid));
+
+                // write accounting info
+                kvmeta.put(kvindex + PARTITION, partition);
+                kvmeta.put(kvindex + KEYSTART, keystart);
+                kvmeta.put(kvindex + VALSTART, valstart);
+                kvmeta.put(kvindex + VALLEN, distanceTo(valstart, valend));
+                // advance kvindex
+                kvindex = (kvindex - NMETA + kvmeta.capacity()) % kvmeta.capacity();
+            } catch (MapBufferTooSmallException e) {
+                LOG.info("Record too large for in-memory buffer: " + e.getMessage());
+                spillSingleRecord(key, value, partition);
+                mapOutputRecordCounter.increment(1);
+                return;
+            }
+        }
+
+        private TaskAttemptID getTaskID() {
+            return mapTask.getTaskID();
+        }
+
+        /**
+         * Set the point from which meta and serialization data expand. The meta
+         * indices are aligned with the buffer, so metadata never spans the ends of
+         * the circular buffer.
+         */
+        private void setEquator(int pos) {
+            equator = pos;
+            // set index prior to first entry, aligned at meta boundary
+            final int aligned = pos - (pos % METASIZE);
+            // Cast one of the operands to long to avoid integer overflow
+            kvindex = (int)
+                    (((long)aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;
+            LOG.info("(EQUATOR) " + pos + " kvi " + kvindex +
+                    "(" + (kvindex * 4) + ")");
+        }
+
+        /**
+         * The spill is complete, so set the buffer and meta indices to be equal to
+         * the new equator to free space for continuing collection. Note that when
+         * kvindex == kvend == kvstart, the buffer is empty.
+         */
+        private void resetSpill() {
+            final int e = equator;
+            bufstart = bufend = e;
+            final int aligned = e - (e % METASIZE);
+            // set start/end to point to first meta record
+            // Cast one of the operands to long to avoid integer overflow
+            kvstart = kvend = (int)
+                    (((long)aligned - METASIZE + kvbuffer.length) % kvbuffer.length) / 4;
+            LOG.info("(RESET) equator " + e + " kv " + kvstart + "(" +
+                    (kvstart * 4) + ")" + " kvi " + kvindex + "(" + (kvindex * 4) + ")");
+        }
+
+        /**
+         * Compute the distance in bytes between two indices in the serialization
+         * buffer.
+         * @see #distanceTo(int,int,int)
+         */
+        final int distanceTo(final int i, final int j) {
+            return distanceTo(i, j, kvbuffer.length);
+        }
+
+        /**
+         * Compute the distance between two indices in the circular buffer given the
+         * max distance.
+         */
+        int distanceTo(final int i, final int j, final int mod) {
+            return i <= j
+                    ? j - i
+                    : mod - i + j;
+        }
+
+        /**
+         * For the given meta position, return the offset into the int-sized
+         * kvmeta buffer.
+         */
+        int offsetFor(int metapos) {
+            return metapos * NMETA;
+        }
+
+        /**
+         * Compare logical range, st i, j MOD offset capacity.
+         * Compare by partition, then by key.
+         * @see IndexedSortable#compare
+         */
+        public int compare(final int mi, final int mj) {
+            final int kvi = offsetFor(mi % maxRec);
+            final int kvj = offsetFor(mj % maxRec);
+            final int kvip = kvmeta.get(kvi + PARTITION);
+            final int kvjp = kvmeta.get(kvj + PARTITION);
+            // sort by partition
+            if (kvip != kvjp) {
+                return kvip - kvjp;
+            }
+            // sort by key
+            return comparator.compare(kvbuffer,
+                    kvmeta.get(kvi + KEYSTART),
+                    kvmeta.get(kvi + VALSTART) - kvmeta.get(kvi + KEYSTART),
+                    kvbuffer,
+                    kvmeta.get(kvj + KEYSTART),
+                    kvmeta.get(kvj + VALSTART) - kvmeta.get(kvj + KEYSTART));
+        }
+
+        final byte META_BUFFER_TMP[] = new byte[METASIZE];
+        /**
+         * Swap metadata for items i, j
+         * @see IndexedSortable#swap
+         */
+        public void swap(final int mi, final int mj) {
+            int iOff = (mi % maxRec) * METASIZE;
+            int jOff = (mj % maxRec) * METASIZE;
+            System.arraycopy(kvbuffer, iOff, META_BUFFER_TMP, 0, METASIZE);
+            System.arraycopy(kvbuffer, jOff, kvbuffer, iOff, METASIZE);
+            System.arraycopy(META_BUFFER_TMP, 0, kvbuffer, jOff, METASIZE);
+        }
+
+        /**
+         * Inner class managing the spill of serialized records to disk.
+         */
+        protected class BlockingBuffer extends DataOutputStream {
+
+            public BlockingBuffer() {
+                super(new Buffer());
+            }
+
+            /**
+             * Mark end of record. Note that this is required if the buffer is to
+             * cut the spill in the proper place.
+             */
+            public int markRecord() {
+                bufmark = bufindex;
+                return bufindex;
+            }
+
+            /**
+             * Set position from last mark to end of writable buffer, then rewrite
+             * the data between last mark and kvindex.
+             * This handles a special case where the key wraps around the buffer.
+             * If the key is to be passed to a RawComparator, then it must be
+             * contiguous in the buffer. This recopies the data in the buffer back
+             * into itself, but starting at the beginning of the buffer. Note that
+             * this method should <b>only</b> be called immediately after detecting
+             * this condition. To call it at any other time is undefined and would
+             * likely result in data loss or corruption.
+             * @see #markRecord()
+             */
+            protected void shiftBufferedKey() throws IOException {
+                // spillLock unnecessary; both kvend and kvindex are current
+                int headbytelen = bufvoid - bufmark;
+                bufvoid = bufmark;
+                final int kvbidx = 4 * kvindex;
+                final int kvbend = 4 * kvend;
+                final int avail =
+                        Math.min(distanceTo(0, kvbidx), distanceTo(0, kvbend));
+                if (bufindex + headbytelen < avail) {
+                    System.arraycopy(kvbuffer, 0, kvbuffer, headbytelen, bufindex);
+                    System.arraycopy(kvbuffer, bufvoid, kvbuffer, 0, headbytelen);
+                    bufindex += headbytelen;
+                    bufferRemaining -= kvbuffer.length - bufvoid;
+                } else {
+                    byte[] keytmp = new byte[bufindex];
+                    System.arraycopy(kvbuffer, 0, keytmp, 0, bufindex);
+                    bufindex = 0;
+                    out.write(kvbuffer, bufmark, headbytelen);
+                    out.write(keytmp);
+                }
+            }
+        }
+
+        public class Buffer extends OutputStream {
+            private final byte[] scratch = new byte[1];
+
+            @Override
+            public void write(int v)
+                    throws IOException {
+                scratch[0] = (byte)v;
+                write(scratch, 0, 1);
+            }
+
+            /**
+             * Attempt to write a sequence of bytes to the collection buffer.
+             * This method will block if the spill thread is running and it
+             * cannot write.
+             * @throws MapBufferTooSmallException if record is too large to
+             *    deserialize into the collection buffer.
+             */
+            @Override
+            public void write(byte b[], int off, int len)
+                    throws IOException {
+                // must always verify the invariant that at least METASIZE bytes are
+                // available beyond kvindex, even when len == 0
+                bufferRemaining -= len;
+                if (bufferRemaining <= 0) {
+                    // writing these bytes could exhaust available buffer space or fill
+                    // the buffer to soft limit. check if spill or blocking are necessary
+                    boolean blockwrite = false;
+                    spillLock.lock();
+                    try {
+                        do {
+                            checkSpillException();
+
+                            final int kvbidx = 4 * kvindex;
+                            final int kvbend = 4 * kvend;
+                            // ser distance to key index
+                            final int distkvi = distanceTo(bufindex, kvbidx);
+                            // ser distance to spill end index
+                            final int distkve = distanceTo(bufindex, kvbend);
+
+                            // if kvindex is closer than kvend, then a spill is neither in
+                            // progress nor complete and reset since the lock was held. The
+                            // write should block only if there is insufficient space to
+                            // complete the current write, write the metadata for this record,
+                            // and write the metadata for the next record. If kvend is closer,
+                            // then the write should block if there is too little space for
+                            // either the metadata or the current write. Note that collect
+                            // ensures its metadata requirement with a zero-length write
+                            blockwrite = distkvi <= distkve
+                                    ? distkvi <= len + 2 * METASIZE
+                                    : distkve <= len || distanceTo(bufend, kvbidx) < 2 * METASIZE;
+
+                            if (!spillInProgress) {
+                                if (blockwrite) {
+                                    if ((kvbend + METASIZE) % kvbuffer.length !=
+                                            equator - (equator % METASIZE)) {
+                                        // spill finished, reclaim space
+                                        // need to use meta exclusively; zero-len rec & 100% spill
+                                        // pcnt would fail
+                                        resetSpill(); // resetSpill doesn't move bufindex, kvindex
+                                        bufferRemaining = Math.min(
+                                                distkvi - 2 * METASIZE,
+                                                softLimit - distanceTo(kvbidx, bufindex)) - len;
+                                        continue;
+                                    }
+                                    // we have records we can spill; only spill if blocked
+                                    if (kvindex != kvend) {
+                                        startSpill();
+                                        // Blocked on this write, waiting for the spill just
+                                        // initiated to finish. Instead of repositioning the marker
+                                        // and copying the partial record, we set the record start
+                                        // to be the new equator
+                                        setEquator(bufmark);
+                                    } else {
+                                        // We have no buffered records, and this record is too large
+                                        // to write into kvbuffer. We must spill it directly from
+                                        // collect
+                                        final int size = distanceTo(bufstart, bufindex) + len;
+                                        setEquator(0);
+                                        bufstart = bufend = bufindex = equator;
+                                        kvstart = kvend = kvindex;
+                                        bufvoid = kvbuffer.length;
+                                        throw new MapBufferTooSmallException(size + " bytes");
+                                    }
+                                }
+                            }
+
+                            if (blockwrite) {
+                                // wait for spill
+                                try {
+                                    while (spillInProgress) {
+                                        reporter.progress();
+                                        spillDone.await();
+                                    }
+                                } catch (InterruptedException e) {
+                                    throw new IOException(
+                                            "Buffer interrupted while waiting for the writer", e);
+                                }
+                            }
+                        } while (blockwrite);
+                    } finally {
+                        spillLock.unlock();
+                    }
+                }
+                // here, we know that we have sufficient space to write
+                if (bufindex + len > bufvoid) {
+                    final int gaplen = bufvoid - bufindex;
+                    System.arraycopy(b, off, kvbuffer, bufindex, gaplen);
+                    len -= gaplen;
+                    off += gaplen;
+                    bufindex = 0;
+                }
+                System.arraycopy(b, off, kvbuffer, bufindex, len);
+                bufindex += len;
+            }
+        }
+
+        public void flush() throws IOException, ClassNotFoundException,
+                InterruptedException {
+            LOG.info("Starting flush of map output");
+            spillLock.lock();
+            try {
+                while (spillInProgress) {
+                    reporter.progress();
+                    spillDone.await();
+                }
+                checkSpillException();
+
+                final int kvbend = 4 * kvend;
+                if ((kvbend + METASIZE) % kvbuffer.length !=
+                        equator - (equator % METASIZE)) {
+                    // spill finished
+                    resetSpill();
+                }
+                if (kvindex != kvend) {
+                    kvend = (kvindex + NMETA) % kvmeta.capacity();
+                    bufend = bufmark;
+                    LOG.info("Spilling map output");
+                    LOG.info("bufstart = " + bufstart + "; bufend = " + bufmark +
+                            "; bufvoid = " + bufvoid);
+                    LOG.info("kvstart = " + kvstart + "(" + (kvstart * 4) +
+                            "); kvend = " + kvend + "(" + (kvend * 4) +
+                            "); length = " + (distanceTo(kvend, kvstart,
+                            kvmeta.capacity()) + 1) + "/" + maxRec);
+                    sortAndSpill();
+                }
+            } catch (InterruptedException e) {
+                throw new IOException("Interrupted while waiting for the writer", e);
+            } finally {
+                spillLock.unlock();
+            }
+            assert !spillLock.isHeldByCurrentThread();
+            // shut down spill thread and wait for it to exit. Since the preceding
+            // ensures that it is finished with its work (and sortAndSpill did not
+            // throw), we elect to use an interrupt instead of setting a flag.
+            // Spilling simultaneously from this thread while the spill thread
+            // finishes its work might be both a useful way to extend this and also
+            // sufficient motivation for the latter approach.
+            try {
+                spillThread.interrupt();
+                spillThread.join();
+            } catch (InterruptedException e) {
+                throw new IOException("Spill failed", e);
+            }
+            // release sort buffer before the merge
+            kvbuffer = null;
+            mergeParts();
+            Path outputPath = mapOutputFile.getOutputFile();
+            fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());
+        }
+
+        public void close() { }
+
+        protected class SpillThread extends Thread {
+
+            @Override
+            public void run() {
+                spillLock.lock();
+                spillThreadRunning = true;
+                try {
+                    while (true) {
+                        spillDone.signal();
+                        while (!spillInProgress) {
+                            spillReady.await();
+                        }
+                        try {
+                            spillLock.unlock();
+                            sortAndSpill();
+                        } catch (Throwable t) {
+                            sortSpillException = t;
+                        } finally {
+                            spillLock.lock();
+                            if (bufend < bufstart) {
+                                bufvoid = kvbuffer.length;
+                            }
+                            kvstart = kvend;
+                            bufstart = bufend;
+                            spillInProgress = false;
+                        }
+                    }
+                } catch (InterruptedException e) {
+                    Thread.currentThread().interrupt();
+                } finally {
+                    spillLock.unlock();
+                    spillThreadRunning = false;
+                }
+            }
+        }
+
+        private void checkSpillException() throws IOException {
+            final Throwable lspillException = sortSpillException;
+            if (lspillException != null) {
+                if (lspillException instanceof Error) {
+                    final String logMsg = "Task " + getTaskID() + " failed : " +
+                            StringUtils.stringifyException(lspillException);
+                    mapTask.reportFatalError(getTaskID(), lspillException, logMsg);
+                }
+                throw new IOException("Spill failed", lspillException);
+            }
+        }
+
+        private void startSpill() {
+            assert !spillInProgress;
+            kvend = (kvindex + NMETA) % kvmeta.capacity();
+            bufend = bufmark;
+            spillInProgress = true;
+            LOG.info("Spilling map output");
+            LOG.info("bufstart = " + bufstart + "; bufend = " + bufmark +
+                    "; bufvoid = " + bufvoid);
+            LOG.info("kvstart = " + kvstart + "(" + (kvstart * 4) +
+                    "); kvend = " + kvend + "(" + (kvend * 4) +
+                    "); length = " + (distanceTo(kvend, kvstart,
+                    kvmeta.capacity()) + 1) + "/" + maxRec);
+            spillReady.signal();
+        }
+
+        private void sortAndSpill() throws IOException, ClassNotFoundException,
+                InterruptedException {
+            //approximate the length of the output file to be the length of the
+            //buffer + header lengths for the partitions
+            final long size = (bufend >= bufstart
+                    ? bufend - bufstart
+                    : (bufvoid - bufend) + bufstart) +
+                    partitions * APPROX_HEADER_LENGTH;
+            FSDataOutputStream out = null;
+            try {
+                // create spill file
+                final SpillRecord spillRec = new SpillRecord(partitions);
+                final Path filename =
+                        mapOutputFile.getSpillFileForWrite(numSpills, size);
+                out = rfs.create(filename);
+
+                final int mstart = kvend / NMETA;
+                final int mend = 1 + // kvend is a valid record
+                        (kvstart >= kvend
+                                ? kvstart
+                                : kvmeta.capacity() + kvstart) / NMETA;
+                sorter.sort(MapOutputBuffer.this, mstart, mend, reporter);
+                int spindex = mstart;
+                final IndexRecord rec = new IndexRecord();
+                final InMemValBytes value = new InMemValBytes();
+                for (int i = 0; i < partitions; ++i) {
+                    IFile.Writer<K, V> writer = null;
+                    try {
+                        long segmentStart = out.getPos();
+                        writer = new Writer<K, V>(job, out, keyClass, valClass, codec,
+                                spilledRecordsCounter);
+                        if (combinerRunner == null) {
+                            // spill directly
+                            DataInputBuffer key = new DataInputBuffer();
+                            while (spindex < mend &&
+                                    kvmeta.get(offsetFor(spindex % maxRec) + PARTITION) == i) {
+                                final int kvoff = offsetFor(spindex % maxRec);
+                                int keystart = kvmeta.get(kvoff + KEYSTART);
+                                int valstart = kvmeta.get(kvoff + VALSTART);
+                                key.reset(kvbuffer, keystart, valstart - keystart);
+                                getVBytesForOffset(kvoff, value);
+                                writer.append(key, value);
+                                ++spindex;
+                            }
+                        } else {
+                            int spstart = spindex;
+                            while (spindex < mend &&
+                                    kvmeta.get(offsetFor(spindex % maxRec)
+                                            + PARTITION) == i) {
+                                ++spindex;
+                            }
+                            // Note: we would like to avoid the combiner if we've fewer
+                            // than some threshold of records for a partition
+                            if (spstart != spindex) {
+                                combineCollector.setWriter(writer);
+                                RawKeyValueIterator kvIter =
+                                        new MRResultIterator(spstart, spindex);
+                                combinerRunner.combine(kvIter, combineCollector);
+                            }
+                        }
+
+                        // close the writer
+                        writer.close();
+
+                        // record offsets
+                        rec.startOffset = segmentStart;
+                        rec.rawLength = writer.getRawLength();
+                        rec.partLength = writer.getCompressedLength();
+                        spillRec.putIndex(rec, i);
+
+                        writer = null;
+                    } finally {
+                        if (null != writer) writer.close();
+                    }
+                }
+
+                if (totalIndexCacheMemory >= indexCacheMemoryLimit) {
+                    // create spill index file
+                    Path indexFilename =
+                            mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions
+                                    * MAP_OUTPUT_INDEX_RECORD_LENGTH);
+                    spillRec.writeToFile(indexFilename, job);
+                } else {
+                    indexCacheList.add(spillRec);
+                    totalIndexCacheMemory +=
+                            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;
+                }
+                LOG.info("Finished spill " + numSpills);
+                ++numSpills;
+            } finally {
+                if (out != null) out.close();
+            }
+        }
+
+        /**
+         * Handles the degenerate case where serialization fails to fit in
+         * the in-memory buffer, so we must spill the record from collect
+         * directly to a spill file. Consider this "losing".
+         */
+        private void spillSingleRecord(final K key, final V value,
+                                       int partition) throws IOException {
+            long size = kvbuffer.length + partitions * APPROX_HEADER_LENGTH;
+            FSDataOutputStream out = null;
+            try {
+                // create spill file
+                final SpillRecord spillRec = new SpillRecord(partitions);
+                final Path filename =
+                        mapOutputFile.getSpillFileForWrite(numSpills, size);
+                out = rfs.create(filename);
+
+                // we don't run the combiner for a single record
+                IndexRecord rec = new IndexRecord();
+                for (int i = 0; i < partitions; ++i) {
+                    IFile.Writer<K, V> writer = null;
+                    try {
+                        long segmentStart = out.getPos();
+                        // Create a new codec, don't care!
+                        writer = new IFile.Writer<K,V>(job, out, keyClass, valClass, codec,
+                                spilledRecordsCounter);
+
+                        if (i == partition) {
+                            final long recordStart = out.getPos();
+                            writer.append(key, value);
+                            // Note that our map byte count will not be accurate with
+                            // compression
+                            mapOutputByteCounter.increment(out.getPos() - recordStart);
+                        }
+                        writer.close();
+
+                        // record offsets
+                        rec.startOffset = segmentStart;
+                        rec.rawLength = writer.getRawLength();
+                        rec.partLength = writer.getCompressedLength();
+                        spillRec.putIndex(rec, i);
+
+                        writer = null;
+                    } catch (IOException e) {
+                        if (null != writer) writer.close();
+                        throw e;
+                    }
+                }
+                if (totalIndexCacheMemory >= indexCacheMemoryLimit) {
+                    // create spill index file
+                    Path indexFilename =
+                            mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions
+                                    * MAP_OUTPUT_INDEX_RECORD_LENGTH);
+                    spillRec.writeToFile(indexFilename, job);
+                } else {
+                    indexCacheList.add(spillRec);
+                    totalIndexCacheMemory +=
+                            spillRec.size() * MAP_OUTPUT_INDEX_RECORD_LENGTH;
+                }
+                ++numSpills;
+            } finally {
+                if (out != null) out.close();
+            }
+        }
+
+        /**
+         * Given an offset, populate vbytes with the associated set of
+         * deserialized value bytes. Should only be called during a spill.
+         */
+        private void getVBytesForOffset(int kvoff, InMemValBytes vbytes) {
+            // get the keystart for the next serialized value to be the end
+            // of this value. If this is the last value in the buffer, use bufend
+            final int vallen = kvmeta.get(kvoff + VALLEN);
+            assert vallen >= 0;
+            vbytes.reset(kvbuffer, kvmeta.get(kvoff + VALSTART), vallen);
+        }
+
+        /**
+         * Inner class wrapping valuebytes, used for appendRaw.
+         */
+        protected class InMemValBytes extends DataInputBuffer {
+            private byte[] buffer;
+            private int start;
+            private int length;
+
+            public void reset(byte[] buffer, int start, int length) {
+                this.buffer = buffer;
+                this.start = start;
+                this.length = length;
+
+                if (start + length > bufvoid) {
+                    this.buffer = new byte[this.length];
+                    final int taillen = bufvoid - start;
+                    System.arraycopy(buffer, start, this.buffer, 0, taillen);
+                    System.arraycopy(buffer, 0, this.buffer, taillen, length-taillen);
+                    this.start = 0;
+                }
+
+                super.reset(this.buffer, this.start, this.length);
+            }
+        }
+
+        protected class MRResultIterator implements RawKeyValueIterator {
+            private final DataInputBuffer keybuf = new DataInputBuffer();
+            private final InMemValBytes vbytes = new InMemValBytes();
+            private final int end;
+            private int current;
+            public MRResultIterator(int start, int end) {
+                this.end = end;
+                current = start - 1;
+            }
+            public boolean next() throws IOException {
+                return ++current < end;
+            }
+            public DataInputBuffer getKey() throws IOException {
+                final int kvoff = offsetFor(current % maxRec);
+                keybuf.reset(kvbuffer, kvmeta.get(kvoff + KEYSTART),
+                        kvmeta.get(kvoff + VALSTART) - kvmeta.get(kvoff + KEYSTART));
+                return keybuf;
+            }
+            public DataInputBuffer getValue() throws IOException {
+                getVBytesForOffset(offsetFor(current % maxRec), vbytes);
+                return vbytes;
+            }
+            public Progress getProgress() {
+                return null;
+            }
+            public void close() { }
+        }
+
+        private void mergeParts() throws IOException, InterruptedException,
+                ClassNotFoundException {
+            // get the approximate size of the final output/index files
+            long finalOutFileSize = 0;
+            long finalIndexFileSize = 0;
+            final Path[] filename = new Path[numSpills];
+            final TaskAttemptID mapId = getTaskID();
+
+            for(int i = 0; i < numSpills; i++) {
+                filename[i] = mapOutputFile.getSpillFile(i);
+                finalOutFileSize += rfs.getFileStatus(filename[i]).getLen();
+            }
+            if (numSpills == 1) { //the spill is the final output
+                sameVolRename(filename[0],
+                        mapOutputFile.getOutputFileForWriteInVolume(filename[0]));
+                if (indexCacheList.size() == 0) {
+                    sameVolRename(mapOutputFile.getSpillIndexFile(0),
+                            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]));
+                } else {
+                    indexCacheList.get(0).writeToFile(
+                            mapOutputFile.getOutputIndexFileForWriteInVolume(filename[0]), job);
+                }
+                sortPhase.complete();
+                return;
+            }
+
+            // read in paged indices
+            for (int i = indexCacheList.size(); i < numSpills; ++i) {
+                Path indexFileName = mapOutputFile.getSpillIndexFile(i);
+                indexCacheList.add(new SpillRecord(indexFileName, job));
+            }
+
+            //make correction in the length to include the sequence file header
+            //lengths for each partition
+            finalOutFileSize += partitions * APPROX_HEADER_LENGTH;
+            finalIndexFileSize = partitions * MAP_OUTPUT_INDEX_RECORD_LENGTH;
+            Path finalOutputFile =
+                    mapOutputFile.getOutputFileForWrite(finalOutFileSize);
+            Path finalIndexFile =
+                    mapOutputFile.getOutputIndexFileForWrite(finalIndexFileSize);
+
+            //The output stream for the final single output file
+            FSDataOutputStream finalOut = rfs.create(finalOutputFile, true, 4096);
+
+            if (numSpills == 0) {
+                //create dummy files
+                IndexRecord rec = new IndexRecord();
+                SpillRecord sr = new SpillRecord(partitions);
+                try {
+                    for (int i = 0; i < partitions; i++) {
+                        long segmentStart = finalOut.getPos();
+                        Writer<K, V> writer =
+                                new Writer<K, V>(job, finalOut, keyClass, valClass, codec, null);
+                        writer.close();
+                        rec.startOffset = segmentStart;
+                        rec.rawLength = writer.getRawLength();
+                        rec.partLength = writer.getCompressedLength();
+                        sr.putIndex(rec, i);
+                    }
+                    sr.writeToFile(finalIndexFile, job);
+                } finally {
+                    finalOut.close();
+                }
+                sortPhase.complete();
+                return;
+            }
+            {
+                sortPhase.addPhases(partitions); // Divide sort phase into sub-phases
+
+                IndexRecord rec = new IndexRecord();
+                final SpillRecord spillRec = new SpillRecord(partitions);
+                for (int parts = 0; parts < partitions; parts++) {
+                    //create the segments to be merged
+                    List<Segment<K,V>> segmentList =
+                            new ArrayList<Segment<K, V>>(numSpills);
+                    for(int i = 0; i < numSpills; i++) {
+                        IndexRecord indexRecord = indexCacheList.get(i).getIndex(parts);
+
+                        Segment<K,V> s =
+                                new Segment<K,V>(job, rfs, filename[i], indexRecord.startOffset,
+                                        indexRecord.partLength, codec, true);
+                        segmentList.add(i, s);
+
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug("MapId=" + mapId + " Reducer=" + parts +
+                                    "Spill =" + i + "(" + indexRecord.startOffset + "," +
+                                    indexRecord.rawLength + ", " + indexRecord.partLength + ")");
+                        }
+                    }
+
+                    int mergeFactor = job.getInt(JobContext.IO_SORT_FACTOR, 100);
+                    // sort the segments only if there are intermediate merges
+                    boolean sortSegments = segmentList.size() > mergeFactor;
+                    //merge
+                    @SuppressWarnings("unchecked")
+                    RawKeyValueIterator kvIter = Merger.merge(job, rfs,
+                            keyClass, valClass, codec,
+                            segmentList, mergeFactor,
+                            new Path(mapId.toString()),
+                            job.getOutputKeyComparator(), reporter, sortSegments,
+                            null, spilledRecordsCounter, sortPhase.phase(),
+                            TaskType.MAP);
+
+                    //write merged output to disk
+                    long segmentStart = finalOut.getPos();
+                    Writer<K, V> writer =
+                            new Writer<K, V>(job, finalOut, keyClass, valClass, codec,
+                                    spilledRecordsCounter);
+                    if (combinerRunner == null || numSpills < minSpillsForCombine) {
+                        Merger.writeFile(kvIter, writer, reporter, job);
+                    } else {
+                        combineCollector.setWriter(writer);
+                        combinerRunner.combine(kvIter, combineCollector);
+                    }
+
+                    //close
+                    writer.close();
+
+                    sortPhase.startNextPhase();
+
+                    // record offsets
+                    rec.startOffset = segmentStart;
+                    rec.rawLength = writer.getRawLength();
+                    rec.partLength = writer.getCompressedLength();
+                    spillRec.putIndex(rec, parts);
+                }
+                spillRec.writeToFile(finalIndexFile, job);
+                finalOut.close();
+                for(int i = 0; i < numSpills; i++) {
+                    rfs.delete(filename[i],true);
+                }
+            }
+        }
+
+        /**
+         * Rename srcPath to dstPath on the same volume. This is the same
+         * as RawLocalFileSystem's rename method, except that it will not
+         * fall back to a copy, and it will create the target directory
+         * if it doesn't exist.
+         */
+        private void sameVolRename(Path srcPath,
+                                   Path dstPath) throws IOException {
+            RawLocalFileSystem rfs = (RawLocalFileSystem)this.rfs;
+            File src = rfs.pathToFile(srcPath);
+            File dst = rfs.pathToFile(dstPath);
+            if (!dst.getParentFile().exists()) {
+                if (!dst.getParentFile().mkdirs()) {
+                    throw new IOException("Unable to rename " + src + " to "
+                            + dst + ": couldn't create parent directory");
+                }
+            }
+
+            if (!src.renameTo(dst)) {
+                throw new IOException("Unable to rename " + src + " to " + dst);
+            }
+        }
+    } // MapOutputBuffer
+
+    /**
+     * Exception indicating that the allocated sort buffer is insufficient
+     * to hold the current record.
+     */
+    @SuppressWarnings("serial")
+    private static class MapBufferTooSmallException extends IOException {
+        public MapBufferTooSmallException(String s) {
+            super(s);
+        }
+    }
+
+    private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+    void closeQuietly(RecordReader<INKEY, INVALUE> c) {
+        if (c != null) {
+            try {
+                c.close();
+            } catch (IOException ie) {
+                // Ignore
+                LOG.info("Ignoring exception during close for " + c, ie);
+            }
+        }
+    }
+
+    private <OUTKEY, OUTVALUE>
+    void closeQuietly(MapOutputCollector<OUTKEY, OUTVALUE> c) {
+        if (c != null) {
+            try {
+                c.close();
+            } catch (Exception ie) {
+                // Ignore
+                LOG.info("Ignoring exception during close for " + c, ie);
+            }
+        }
+    }
+
+    private <INKEY, INVALUE, OUTKEY, OUTVALUE>
+    void closeQuietly(
+            org.apache.hadoop.mapreduce.RecordReader<INKEY, INVALUE> c) {
+        if (c != null) {
+            try {
+                c.close();
+            } catch (Exception ie) {
+                // Ignore
+                LOG.info("Ignoring exception during close for " + c, ie);
+            }
+        }
+    }
+
+    private <INKEY, INVALUE, OUTKEY, OUTVALUE>
+    void closeQuietly(
+            org.apache.hadoop.mapreduce.RecordWriter<OUTKEY, OUTVALUE> c,
+            org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context
+                    mapperContext) {
+        if (c != null) {
+            try {
+                c.close(mapperContext);
+            } catch (Exception ie) {
+                // Ignore
+                LOG.info("Ignoring exception during close for " + c, ie);
+            }
+        }
+    }
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/mapred/ReduceTask.java b/hadoop-client/src/main/java/org/apache/hadoop/mapred/ReduceTask.java
new file mode 100644
index 0000000..ef9a7b4
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/mapred/ReduceTask.java
@@ -0,0 +1,659 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedSet;
+import java.util.TreeSet;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.FileSystem.Statistics;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.granular.MapReduceV2Term;
+import org.apache.hadoop.granular.GranularLoggerStore;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableFactories;
+import org.apache.hadoop.io.WritableFactory;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.hadoop.mapred.SortedRanges.SkipRangeIterator;
+import org.apache.hadoop.mapreduce.MRConfig;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskCounter;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter;
+import org.apache.hadoop.mapreduce.task.reduce.Shuffle;
+import org.apache.hadoop.util.Progress;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/** A Reduce task. */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public class ReduceTask extends Task {
+
+    static {                                        // register a ctor
+        WritableFactories.setFactory
+                (ReduceTask.class,
+                        new WritableFactory() {
+                            public Writable newInstance() { return new ReduceTask(); }
+                        });
+    }
+
+    private static final Log LOG = LogFactory.getLog(ReduceTask.class.getName());
+    private int numMaps;
+
+    private CompressionCodec codec;
+
+    // If this is a LocalJobRunner-based job, this will
+    // be a mapping from map task attempts to their output files.
+    // This will be null in other cases.
+    private Map<TaskAttemptID, MapOutputFile> localMapFiles;
+
+    {
+        getProgress().setStatus("reduce");
+        setPhase(TaskStatus.Phase.SHUFFLE);        // phase to start with
+    }
+
+    private Progress copyPhase;
+    private Progress sortPhase;
+    private Progress reducePhase;
+    private Counters.Counter shuffledMapsCounter =
+            getCounters().findCounter(TaskCounter.SHUFFLED_MAPS);
+    private Counters.Counter reduceShuffleBytes =
+            getCounters().findCounter(TaskCounter.REDUCE_SHUFFLE_BYTES);
+    private Counters.Counter reduceInputKeyCounter =
+            getCounters().findCounter(TaskCounter.REDUCE_INPUT_GROUPS);
+    private Counters.Counter reduceInputValueCounter =
+            getCounters().findCounter(TaskCounter.REDUCE_INPUT_RECORDS);
+    private Counters.Counter reduceOutputCounter =
+            getCounters().findCounter(TaskCounter.REDUCE_OUTPUT_RECORDS);
+    private Counters.Counter reduceCombineInputCounter =
+            getCounters().findCounter(TaskCounter.COMBINE_INPUT_RECORDS);
+    private Counters.Counter reduceCombineOutputCounter =
+            getCounters().findCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);
+    private Counters.Counter fileOutputByteCounter =
+            getCounters().findCounter(FileOutputFormatCounter.BYTES_WRITTEN);
+
+    // A custom comparator for map output files. Here the ordering is determined
+    // by the file's size and path. In case of files with same size and different
+    // file paths, the first parameter is considered smaller than the second one.
+    // In case of files with same size and path are considered equal.
+    private Comparator<FileStatus> mapOutputFileComparator =
+            new Comparator<FileStatus>() {
+                public int compare(FileStatus a, FileStatus b) {
+                    if (a.getLen() < b.getLen())
+                        return -1;
+                    else if (a.getLen() == b.getLen())
+                        if (a.getPath().toString().equals(b.getPath().toString()))
+                            return 0;
+                        else
+                            return -1;
+                    else
+                        return 1;
+                }
+            };
+
+    // A sorted set for keeping a set of map output files on disk
+    private final SortedSet<FileStatus> mapOutputFilesOnDisk =
+            new TreeSet<FileStatus>(mapOutputFileComparator);
+
+    public ReduceTask() {
+        super();
+    }
+
+    public ReduceTask(String jobFile, TaskAttemptID taskId,
+                      int partition, int numMaps, int numSlotsRequired) {
+        super(jobFile, taskId, partition, numSlotsRequired);
+        this.numMaps = numMaps;
+    }
+
+
+    /**
+     * Register the set of mapper outputs created by a LocalJobRunner-based
+     * job with this ReduceTask so it knows where to fetch from.
+     *
+     * This should not be called in normal (networked) execution.
+     */
+    public void setLocalMapFiles(Map<TaskAttemptID, MapOutputFile> mapFiles) {
+        this.localMapFiles = mapFiles;
+    }
+
+    private CompressionCodec initCodec() {
+        // check if map-outputs are to be compressed
+        if (conf.getCompressMapOutput()) {
+            Class<? extends CompressionCodec> codecClass =
+                    conf.getMapOutputCompressorClass(DefaultCodec.class);
+            return ReflectionUtils.newInstance(codecClass, conf);
+        }
+
+        return null;
+    }
+
+    @Override
+    public boolean isMapTask() {
+        return false;
+    }
+
+    public int getNumMaps() { return numMaps; }
+
+    /**
+     * Localize the given JobConf to be specific for this task.
+     */
+    @Override
+    public void localizeConfiguration(JobConf conf) throws IOException {
+        super.localizeConfiguration(conf);
+        conf.setNumMapTasks(numMaps);
+    }
+
+    @Override
+    public void write(DataOutput out) throws IOException {
+        super.write(out);
+
+        out.writeInt(numMaps);                        // write the number of maps
+    }
+
+    @Override
+    public void readFields(DataInput in) throws IOException {
+        super.readFields(in);
+
+        numMaps = in.readInt();
+    }
+
+    // Get the input files for the reducer (for local jobs).
+    private Path[] getMapFiles(FileSystem fs) throws IOException {
+        List<Path> fileList = new ArrayList<Path>();
+        for(int i = 0; i < numMaps; ++i) {
+            fileList.add(mapOutputFile.getInputFile(i));
+        }
+        return fileList.toArray(new Path[0]);
+    }
+
+    private class ReduceValuesIterator<KEY,VALUE>
+            extends ValuesIterator<KEY,VALUE> {
+        public ReduceValuesIterator (RawKeyValueIterator in,
+                                     RawComparator<KEY> comparator,
+                                     Class<KEY> keyClass,
+                                     Class<VALUE> valClass,
+                                     Configuration conf, Progressable reporter)
+                throws IOException {
+            super(in, comparator, keyClass, valClass, conf, reporter);
+        }
+
+        @Override
+        public VALUE next() {
+            reduceInputValueCounter.increment(1);
+            return moveToNext();
+        }
+
+        protected VALUE moveToNext() {
+            return super.next();
+        }
+
+        public void informReduceProgress() {
+            reducePhase.set(super.in.getProgress().getProgress()); // update progress
+            reporter.progress();
+        }
+    }
+
+    private class SkippingReduceValuesIterator<KEY,VALUE>
+            extends ReduceValuesIterator<KEY,VALUE> {
+        private SkipRangeIterator skipIt;
+        private TaskUmbilicalProtocol umbilical;
+        private Counters.Counter skipGroupCounter;
+        private Counters.Counter skipRecCounter;
+        private long grpIndex = -1;
+        private Class<KEY> keyClass;
+        private Class<VALUE> valClass;
+        private SequenceFile.Writer skipWriter;
+        private boolean toWriteSkipRecs;
+        private boolean hasNext;
+        private TaskReporter reporter;
+
+        public SkippingReduceValuesIterator(RawKeyValueIterator in,
+                                            RawComparator<KEY> comparator, Class<KEY> keyClass,
+                                            Class<VALUE> valClass, Configuration conf, TaskReporter reporter,
+                                            TaskUmbilicalProtocol umbilical) throws IOException {
+            super(in, comparator, keyClass, valClass, conf, reporter);
+            this.umbilical = umbilical;
+            this.skipGroupCounter =
+                    reporter.getCounter(TaskCounter.REDUCE_SKIPPED_GROUPS);
+            this.skipRecCounter =
+                    reporter.getCounter(TaskCounter.REDUCE_SKIPPED_RECORDS);
+            this.toWriteSkipRecs = toWriteSkipRecs() &&
+                    SkipBadRecords.getSkipOutputPath(conf)!=null;
+            this.keyClass = keyClass;
+            this.valClass = valClass;
+            this.reporter = reporter;
+            skipIt = getSkipRanges().skipRangeIterator();
+            mayBeSkip();
+        }
+
+        public void nextKey() throws IOException {
+            super.nextKey();
+            mayBeSkip();
+        }
+
+        public boolean more() {
+            return super.more() && hasNext;
+        }
+
+        private void mayBeSkip() throws IOException {
+            hasNext = skipIt.hasNext();
+            if(!hasNext) {
+                LOG.warn("Further groups got skipped.");
+                return;
+            }
+            grpIndex++;
+            long nextGrpIndex = skipIt.next();
+            long skip = 0;
+            long skipRec = 0;
+            while(grpIndex<nextGrpIndex && super.more()) {
+                while (hasNext()) {
+                    VALUE value = moveToNext();
+                    if(toWriteSkipRecs) {
+                        writeSkippedRec(getKey(), value);
+                    }
+                    skipRec++;
+                }
+                super.nextKey();
+                grpIndex++;
+                skip++;
+            }
+
+            //close the skip writer once all the ranges are skipped
+            if(skip>0 && skipIt.skippedAllRanges() && skipWriter!=null) {
+                skipWriter.close();
+            }
+            skipGroupCounter.increment(skip);
+            skipRecCounter.increment(skipRec);
+            reportNextRecordRange(umbilical, grpIndex);
+        }
+
+        @SuppressWarnings("unchecked")
+        private void writeSkippedRec(KEY key, VALUE value) throws IOException{
+            if(skipWriter==null) {
+                Path skipDir = SkipBadRecords.getSkipOutputPath(conf);
+                Path skipFile = new Path(skipDir, getTaskID().toString());
+                skipWriter = SequenceFile.createWriter(
+                        skipFile.getFileSystem(conf), conf, skipFile,
+                        keyClass, valClass,
+                        CompressionType.BLOCK, reporter);
+            }
+            skipWriter.append(key, value);
+        }
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public void run(JobConf job, final TaskUmbilicalProtocol umbilical)
+            throws IOException, InterruptedException, ClassNotFoundException {
+
+
+
+        String[] ids = getTaskID().toString().split("_");
+        String applicationId = ids[1] + "_" + ids[2];
+        String taskId = ids[3] + "_" + ids[4] + "_" + ids[5];
+
+        GranularLoggerStore.reduceTaskLogger.setActorId(applicationId + "_" + taskId);
+        LOG.info(GranularLoggerStore.reduceTaskLogger.logInfo(MapReduceV2Term.StartTime, String.valueOf(System.currentTimeMillis())));
+        LOG.info(GranularLoggerStore.reduceTaskLogger.logInfo(MapReduceV2Term.ApplicationID, applicationId));
+
+        job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());
+
+        if (isMapOrReduce()) {
+            copyPhase = getProgress().addPhase("copy");
+            sortPhase  = getProgress().addPhase("sort");
+            reducePhase = getProgress().addPhase("reduce");
+        }
+        // start thread that will handle communication with parent
+        TaskReporter reporter = startReporter(umbilical);
+
+        boolean useNewApi = job.getUseNewReducer();
+        initialize(job, getJobID(), reporter, useNewApi);
+
+        // check if it is a cleanupJobTask
+        if (jobCleanup) {
+            runJobCleanupTask(umbilical, reporter);
+            return;
+        }
+        if (jobSetup) {
+            runJobSetupTask(umbilical, reporter);
+            return;
+        }
+        if (taskCleanup) {
+            runTaskCleanupTask(umbilical, reporter);
+            return;
+        }
+
+        // Initialize the codec
+        codec = initCodec();
+        RawKeyValueIterator rIter = null;
+        ShuffleConsumerPlugin shuffleConsumerPlugin = null;
+
+        Class combinerClass = conf.getCombinerClass();
+        CombineOutputCollector combineCollector =
+                (null != combinerClass) ?
+                        new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;
+
+        Class<? extends ShuffleConsumerPlugin> clazz =
+                job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);
+
+        shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);
+        LOG.info("Using ShuffleConsumerPlugin: " + shuffleConsumerPlugin);
+
+        ShuffleConsumerPlugin.Context shuffleContext =
+                new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical,
+                        super.lDirAlloc, reporter, codec,
+                        combinerClass, combineCollector,
+                        spilledRecordsCounter, reduceCombineInputCounter,
+                        shuffledMapsCounter,
+                        reduceShuffleBytes, failedShuffleCounter,
+                        mergedMapOutputsCounter,
+                        taskStatus, copyPhase, sortPhase, this,
+                        mapOutputFile, localMapFiles);
+        shuffleConsumerPlugin.init(shuffleContext);
+
+        rIter = shuffleConsumerPlugin.run();
+
+        // free up the data structures
+        mapOutputFilesOnDisk.clear();
+
+        sortPhase.complete();                         // sort is complete
+        setPhase(TaskStatus.Phase.REDUCE);
+        statusUpdate(umbilical);
+        Class keyClass = job.getMapOutputKeyClass();
+        Class valueClass = job.getMapOutputValueClass();
+        RawComparator comparator = job.getOutputValueGroupingComparator();
+
+        if (useNewApi) {
+            runNewReducer(job, umbilical, reporter, rIter, comparator,
+                    keyClass, valueClass);
+        } else {
+            runOldReducer(job, umbilical, reporter, rIter, comparator,
+                    keyClass, valueClass);
+        }
+
+        shuffleConsumerPlugin.close();
+        done(umbilical, reporter);
+
+        LOG.info(GranularLoggerStore.reduceTaskLogger.logInfo(MapReduceV2Term.EndTime, String.valueOf(System.currentTimeMillis())));
+
+    }
+
+    @SuppressWarnings("unchecked")
+    private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+    void runOldReducer(JobConf job,
+                       TaskUmbilicalProtocol umbilical,
+                       final TaskReporter reporter,
+                       RawKeyValueIterator rIter,
+                       RawComparator<INKEY> comparator,
+                       Class<INKEY> keyClass,
+                       Class<INVALUE> valueClass) throws IOException {
+        Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =
+                ReflectionUtils.newInstance(job.getReducerClass(), job);
+        // make output collector
+        String finalName = getOutputName(getPartition());
+
+        RecordWriter<OUTKEY, OUTVALUE> out = new OldTrackingRecordWriter<OUTKEY, OUTVALUE>(
+                this, job, reporter, finalName);
+        final RecordWriter<OUTKEY, OUTVALUE> finalOut = out;
+
+        OutputCollector<OUTKEY,OUTVALUE> collector =
+                new OutputCollector<OUTKEY,OUTVALUE>() {
+                    public void collect(OUTKEY key, OUTVALUE value)
+                            throws IOException {
+                        finalOut.write(key, value);
+                        // indicate that progress update needs to be sent
+                        reporter.progress();
+                    }
+                };
+
+        // apply reduce function
+        try {
+            //increment processed counter only if skipping feature is enabled
+            boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job)>0 &&
+                    SkipBadRecords.getAutoIncrReducerProcCount(job);
+
+            ReduceValuesIterator<INKEY,INVALUE> values = isSkipping() ?
+                    new SkippingReduceValuesIterator<INKEY,INVALUE>(rIter,
+                            comparator, keyClass, valueClass,
+                            job, reporter, umbilical) :
+                    new ReduceValuesIterator<INKEY,INVALUE>(rIter,
+                            job.getOutputValueGroupingComparator(), keyClass, valueClass,
+                            job, reporter);
+            values.informReduceProgress();
+            while (values.more()) {
+                reduceInputKeyCounter.increment(1);
+                reducer.reduce(values.getKey(), values, collector, reporter);
+                if(incrProcCount) {
+                    reporter.incrCounter(SkipBadRecords.COUNTER_GROUP,
+                            SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);
+                }
+                values.nextKey();
+                values.informReduceProgress();
+            }
+
+            reducer.close();
+            reducer = null;
+
+            out.close(reporter);
+            out = null;
+        } finally {
+            IOUtils.cleanup(LOG, reducer);
+            closeQuietly(out, reporter);
+        }
+    }
+
+    static class OldTrackingRecordWriter<K, V> implements RecordWriter<K, V> {
+
+        private final RecordWriter<K, V> real;
+        private final org.apache.hadoop.mapred.Counters.Counter reduceOutputCounter;
+        private final org.apache.hadoop.mapred.Counters.Counter fileOutputByteCounter;
+        private final List<Statistics> fsStats;
+
+        @SuppressWarnings({ "deprecation", "unchecked" })
+        public OldTrackingRecordWriter(ReduceTask reduce, JobConf job,
+                                       TaskReporter reporter, String finalName) throws IOException {
+            this.reduceOutputCounter = reduce.reduceOutputCounter;
+            this.fileOutputByteCounter = reduce.fileOutputByteCounter;
+            List<Statistics> matchedStats = null;
+            if (job.getOutputFormat() instanceof FileOutputFormat) {
+                matchedStats = getFsStatistics(FileOutputFormat.getOutputPath(job), job);
+            }
+            fsStats = matchedStats;
+
+            FileSystem fs = FileSystem.get(job);
+            long bytesOutPrev = getOutputBytes(fsStats);
+            this.real = job.getOutputFormat().getRecordWriter(fs, job, finalName,
+                    reporter);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        }
+
+        @Override
+        public void write(K key, V value) throws IOException {
+            long bytesOutPrev = getOutputBytes(fsStats);
+            real.write(key, value);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+            reduceOutputCounter.increment(1);
+        }
+
+        @Override
+        public void close(Reporter reporter) throws IOException {
+            long bytesOutPrev = getOutputBytes(fsStats);
+            real.close(reporter);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        }
+
+        private long getOutputBytes(List<Statistics> stats) {
+            if (stats == null) return 0;
+            long bytesWritten = 0;
+            for (Statistics stat: stats) {
+                bytesWritten = bytesWritten + stat.getBytesWritten();
+            }
+            return bytesWritten;
+        }
+    }
+
+    static class NewTrackingRecordWriter<K,V>
+            extends org.apache.hadoop.mapreduce.RecordWriter<K,V> {
+        private final org.apache.hadoop.mapreduce.RecordWriter<K,V> real;
+        private final org.apache.hadoop.mapreduce.Counter outputRecordCounter;
+        private final org.apache.hadoop.mapreduce.Counter fileOutputByteCounter;
+        private final List<Statistics> fsStats;
+
+        @SuppressWarnings("unchecked")
+        NewTrackingRecordWriter(ReduceTask reduce,
+                                org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)
+                throws InterruptedException, IOException {
+            this.outputRecordCounter = reduce.reduceOutputCounter;
+            this.fileOutputByteCounter = reduce.fileOutputByteCounter;
+
+            List<Statistics> matchedStats = null;
+            if (reduce.outputFormat instanceof org.apache.hadoop.mapreduce.lib.output.FileOutputFormat) {
+                matchedStats = getFsStatistics(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
+                        .getOutputPath(taskContext), taskContext.getConfiguration());
+            }
+
+            fsStats = matchedStats;
+
+            long bytesOutPrev = getOutputBytes(fsStats);
+            this.real = (org.apache.hadoop.mapreduce.RecordWriter<K, V>) reduce.outputFormat
+                    .getRecordWriter(taskContext);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        }
+
+        @Override
+        public void close(TaskAttemptContext context) throws IOException,
+                InterruptedException {
+            long bytesOutPrev = getOutputBytes(fsStats);
+            real.close(context);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+        }
+
+        @Override
+        public void write(K key, V value) throws IOException, InterruptedException {
+            long bytesOutPrev = getOutputBytes(fsStats);
+            real.write(key,value);
+            long bytesOutCurr = getOutputBytes(fsStats);
+            fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);
+            outputRecordCounter.increment(1);
+        }
+
+        private long getOutputBytes(List<Statistics> stats) {
+            if (stats == null) return 0;
+            long bytesWritten = 0;
+            for (Statistics stat: stats) {
+                bytesWritten = bytesWritten + stat.getBytesWritten();
+            }
+            return bytesWritten;
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    private <INKEY,INVALUE,OUTKEY,OUTVALUE>
+    void runNewReducer(JobConf job,
+                       final TaskUmbilicalProtocol umbilical,
+                       final TaskReporter reporter,
+                       RawKeyValueIterator rIter,
+                       RawComparator<INKEY> comparator,
+                       Class<INKEY> keyClass,
+                       Class<INVALUE> valueClass
+    ) throws IOException,InterruptedException,
+            ClassNotFoundException {
+        // wrap value iterator to report progress.
+        final RawKeyValueIterator rawIter = rIter;
+        rIter = new RawKeyValueIterator() {
+            public void close() throws IOException {
+                rawIter.close();
+            }
+            public DataInputBuffer getKey() throws IOException {
+                return rawIter.getKey();
+            }
+            public Progress getProgress() {
+                return rawIter.getProgress();
+            }
+            public DataInputBuffer getValue() throws IOException {
+                return rawIter.getValue();
+            }
+            public boolean next() throws IOException {
+                boolean ret = rawIter.next();
+                reporter.setProgress(rawIter.getProgress().getProgress());
+                return ret;
+            }
+        };
+        // make a task context so we can get the classes
+        org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
+                new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,
+                        getTaskID(), reporter);
+        // make a reducer
+        org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer =
+                (org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)
+                        ReflectionUtils.newInstance(taskContext.getReducerClass(), job);
+        org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW =
+                new NewTrackingRecordWriter<OUTKEY, OUTVALUE>(this, taskContext);
+        job.setBoolean("mapred.skip.on", isSkipping());
+        job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());
+        org.apache.hadoop.mapreduce.Reducer.Context
+                reducerContext = createReduceContext(reducer, job, getTaskID(),
+                rIter, reduceInputKeyCounter,
+                reduceInputValueCounter,
+                trackedRW,
+                committer,
+                reporter, comparator, keyClass,
+                valueClass);
+        try {
+            reducer.run(reducerContext);
+        } finally {
+            trackedRW.close(reducerContext);
+        }
+    }
+
+    private <OUTKEY, OUTVALUE>
+    void closeQuietly(RecordWriter<OUTKEY, OUTVALUE> c, Reporter r) {
+        if (c != null) {
+            try {
+                c.close(r);
+            } catch (Exception e) {
+                LOG.info("Exception in closing " + c, e);
+            }
+        }
+    }
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/mapred/Task.java b/hadoop-client/src/main/java/org/apache/hadoop/mapred/Task.java
new file mode 100644
index 0000000..31593ba
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/mapred/Task.java
@@ -0,0 +1,1656 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.lang.management.GarbageCollectorMXBean;
+import java.lang.management.ManagementFactory;
+import java.text.NumberFormat;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.NoSuchElementException;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import javax.crypto.SecretKey;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalDirAllocator;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.FileSystem.Statistics;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.io.serializer.Deserializer;
+import org.apache.hadoop.io.serializer.SerializationFactory;
+import org.apache.hadoop.mapred.IFile.Writer;
+import org.apache.hadoop.mapreduce.FileSystemCounter;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.mapreduce.TaskCounter;
+import org.apache.hadoop.mapreduce.JobStatus;
+import org.apache.hadoop.mapreduce.MRConfig;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer;
+import org.apache.hadoop.mapreduce.task.ReduceContextImpl;
+import org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.util.Progress;
+import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringInterner;
+import org.apache.hadoop.util.StringUtils;
+
+/**
+ * Base class for tasks.
+ */
+@InterfaceAudience.LimitedPrivate({"MapReduce"})
+@InterfaceStability.Unstable
+abstract public class Task implements Writable, Configurable {
+    private static final Log LOG =
+            LogFactory.getLog(Task.class);
+
+    public static String MERGED_OUTPUT_PREFIX = ".merged";
+    public static final long DEFAULT_COMBINE_RECORDS_BEFORE_PROGRESS = 10000;
+
+    /**
+     * @deprecated Provided for compatibility. Use {@link TaskCounter} instead.
+     */
+    @Deprecated
+    public static enum Counter {
+        MAP_INPUT_RECORDS,
+        MAP_OUTPUT_RECORDS,
+        MAP_SKIPPED_RECORDS,
+        MAP_INPUT_BYTES,
+        MAP_OUTPUT_BYTES,
+        MAP_OUTPUT_MATERIALIZED_BYTES,
+        COMBINE_INPUT_RECORDS,
+        COMBINE_OUTPUT_RECORDS,
+        REDUCE_INPUT_GROUPS,
+        REDUCE_SHUFFLE_BYTES,
+        REDUCE_INPUT_RECORDS,
+        REDUCE_OUTPUT_RECORDS,
+        REDUCE_SKIPPED_GROUPS,
+        REDUCE_SKIPPED_RECORDS,
+        SPILLED_RECORDS,
+        SPLIT_RAW_BYTES,
+        CPU_MILLISECONDS,
+        PHYSICAL_MEMORY_BYTES,
+        VIRTUAL_MEMORY_BYTES,
+        COMMITTED_HEAP_BYTES
+    }
+
+    /**
+     * Counters to measure the usage of the different file systems.
+     * Always return the String array with two elements. First one is the name of
+     * BYTES_READ counter and second one is of the BYTES_WRITTEN counter.
+     */
+    protected static String[] getFileSystemCounterNames(String uriScheme) {
+        String scheme = uriScheme.toUpperCase();
+        return new String[]{scheme+"_BYTES_READ", scheme+"_BYTES_WRITTEN"};
+    }
+
+    /**
+     * Name of the FileSystem counters' group
+     */
+    protected static final String FILESYSTEM_COUNTER_GROUP = "FileSystemCounters";
+
+    ///////////////////////////////////////////////////////////
+    // Helper methods to construct task-output paths
+    ///////////////////////////////////////////////////////////
+
+    /** Construct output file names so that, when an output directory listing is
+     * sorted lexicographically, positions correspond to output partitions.*/
+    private static final NumberFormat NUMBER_FORMAT = NumberFormat.getInstance();
+    static {
+        NUMBER_FORMAT.setMinimumIntegerDigits(5);
+        NUMBER_FORMAT.setGroupingUsed(false);
+    }
+
+    static synchronized String getOutputName(int partition) {
+        return "part-" + NUMBER_FORMAT.format(partition);
+    }
+
+    ////////////////////////////////////////////
+    // Fields
+    ////////////////////////////////////////////
+
+    private String jobFile;                         // job configuration file
+    private String user;                            // user running the job
+    private TaskAttemptID taskId;                   // unique, includes job id
+    private int partition;                          // id within job
+    TaskStatus taskStatus;                          // current status of the task
+    protected JobStatus.State jobRunStateForCleanup;
+    protected boolean jobCleanup = false;
+    protected boolean jobSetup = false;
+    protected boolean taskCleanup = false;
+
+    // An opaque data field used to attach extra data to each task. This is used
+    // by the Hadoop scheduler for Mesos to associate a Mesos task ID with each
+    // task and recover these IDs on the TaskTracker.
+    protected BytesWritable extraData = new BytesWritable();
+
+    //skip ranges based on failed ranges from previous attempts
+    private SortedRanges skipRanges = new SortedRanges();
+    private boolean skipping = false;
+    private boolean writeSkipRecs = true;
+
+    //currently processing record start index
+    private volatile long currentRecStartIndex;
+    private Iterator<Long> currentRecIndexIterator =
+            skipRanges.skipRangeIterator();
+
+    private ResourceCalculatorProcessTree pTree;
+    private long initCpuCumulativeTime = 0;
+
+    protected JobConf conf;
+    protected MapOutputFile mapOutputFile;
+    protected LocalDirAllocator lDirAlloc;
+    private final static int MAX_RETRIES = 10;
+    protected JobContext jobContext;
+    protected TaskAttemptContext taskContext;
+    protected org.apache.hadoop.mapreduce.OutputFormat<?,?> outputFormat;
+    protected org.apache.hadoop.mapreduce.OutputCommitter committer;
+    protected final Counters.Counter spilledRecordsCounter;
+    protected final Counters.Counter failedShuffleCounter;
+    protected final Counters.Counter mergedMapOutputsCounter;
+    private int numSlotsRequired;
+    protected TaskUmbilicalProtocol umbilical;
+    protected SecretKey tokenSecret;
+    protected SecretKey shuffleSecret;
+    protected GcTimeUpdater gcUpdater;
+
+    ////////////////////////////////////////////
+    // Constructors
+    ////////////////////////////////////////////
+
+    public Task() {
+        taskStatus = TaskStatus.createTaskStatus(isMapTask());
+        taskId = new TaskAttemptID();
+        spilledRecordsCounter =
+                counters.findCounter(TaskCounter.SPILLED_RECORDS);
+        failedShuffleCounter =
+                counters.findCounter(TaskCounter.FAILED_SHUFFLE);
+        mergedMapOutputsCounter =
+                counters.findCounter(TaskCounter.MERGED_MAP_OUTPUTS);
+        gcUpdater = new GcTimeUpdater();
+    }
+
+    public Task(String jobFile, TaskAttemptID taskId, int partition,
+                int numSlotsRequired) {
+        this.jobFile = jobFile;
+        this.taskId = taskId;
+
+        this.partition = partition;
+        this.numSlotsRequired = numSlotsRequired;
+        this.taskStatus = TaskStatus.createTaskStatus(isMapTask(), this.taskId,
+                0.0f, numSlotsRequired,
+                TaskStatus.State.UNASSIGNED,
+                "", "", "",
+                isMapTask() ?
+                        TaskStatus.Phase.MAP :
+                        TaskStatus.Phase.SHUFFLE,
+                counters);
+        spilledRecordsCounter = counters.findCounter(TaskCounter.SPILLED_RECORDS);
+        failedShuffleCounter = counters.findCounter(TaskCounter.FAILED_SHUFFLE);
+        mergedMapOutputsCounter =
+                counters.findCounter(TaskCounter.MERGED_MAP_OUTPUTS);
+        gcUpdater = new GcTimeUpdater();
+    }
+
+    ////////////////////////////////////////////
+    // Accessors
+    ////////////////////////////////////////////
+    public void setJobFile(String jobFile) { this.jobFile = jobFile; }
+    public String getJobFile() { return jobFile; }
+    public TaskAttemptID getTaskID() { return taskId; }
+    public int getNumSlotsRequired() {
+        return numSlotsRequired;
+    }
+
+    Counters getCounters() { return counters; }
+
+    /**
+     * Get the job name for this task.
+     * @return the job name
+     */
+    public JobID getJobID() {
+        return taskId.getJobID();
+    }
+
+    /**
+     * Set the job token secret
+     * @param tokenSecret the secret
+     */
+    public void setJobTokenSecret(SecretKey tokenSecret) {
+        this.tokenSecret = tokenSecret;
+    }
+
+    /**
+     * Get the job token secret
+     * @return the token secret
+     */
+    public SecretKey getJobTokenSecret() {
+        return this.tokenSecret;
+    }
+
+    /**
+     * Set the secret key used to authenticate the shuffle
+     * @param shuffleSecret the secret
+     */
+    public void setShuffleSecret(SecretKey shuffleSecret) {
+        this.shuffleSecret = shuffleSecret;
+    }
+
+    /**
+     * Get the secret key used to authenticate the shuffle
+     * @return the shuffle secret
+     */
+    public SecretKey getShuffleSecret() {
+        return this.shuffleSecret;
+    }
+
+    /**
+     * Get the index of this task within the job.
+     * @return the integer part of the task id
+     */
+    public int getPartition() {
+        return partition;
+    }
+    /**
+     * Return current phase of the task.
+     * needs to be synchronized as communication thread sends the phase every second
+     * @return the curent phase of the task
+     */
+    public synchronized TaskStatus.Phase getPhase(){
+        return this.taskStatus.getPhase();
+    }
+    /**
+     * Set current phase of the task.
+     * @param phase task phase
+     */
+    protected synchronized void setPhase(TaskStatus.Phase phase){
+        this.taskStatus.setPhase(phase);
+    }
+
+    /**
+     * Get whether to write skip records.
+     */
+    protected boolean toWriteSkipRecs() {
+        return writeSkipRecs;
+    }
+
+    /**
+     * Set whether to write skip records.
+     */
+    protected void setWriteSkipRecs(boolean writeSkipRecs) {
+        this.writeSkipRecs = writeSkipRecs;
+    }
+
+    /**
+     * Report a fatal error to the parent (task) tracker.
+     */
+    protected void reportFatalError(TaskAttemptID id, Throwable throwable,
+                                    String logMsg) {
+        LOG.fatal(logMsg);
+        Throwable tCause = throwable.getCause();
+        String cause = tCause == null
+                ? StringUtils.stringifyException(throwable)
+                : StringUtils.stringifyException(tCause);
+        try {
+            umbilical.fatalError(id, cause);
+        } catch (IOException ioe) {
+            LOG.fatal("Failed to contact the tasktracker", ioe);
+            System.exit(-1);
+        }
+    }
+
+    /**
+     * Gets a handle to the Statistics instance based on the scheme associated
+     * with path.
+     *
+     * @param path the path.
+     * @param conf the configuration to extract the scheme from if not part of
+     *   the path.
+     * @return a Statistics instance, or null if none is found for the scheme.
+     */
+    protected static List<Statistics> getFsStatistics(Path path, Configuration conf) throws IOException {
+        List<Statistics> matchedStats = new ArrayList<FileSystem.Statistics>();
+        path = path.getFileSystem(conf).makeQualified(path);
+        String scheme = path.toUri().getScheme();
+        for (Statistics stats : FileSystem.getAllStatistics()) {
+            if (stats.getScheme().equals(scheme)) {
+                matchedStats.add(stats);
+            }
+        }
+        return matchedStats;
+    }
+
+    /**
+     * Get skipRanges.
+     */
+    public SortedRanges getSkipRanges() {
+        return skipRanges;
+    }
+
+    /**
+     * Set skipRanges.
+     */
+    public void setSkipRanges(SortedRanges skipRanges) {
+        this.skipRanges = skipRanges;
+    }
+
+    /**
+     * Is Task in skipping mode.
+     */
+    public boolean isSkipping() {
+        return skipping;
+    }
+
+    /**
+     * Sets whether to run Task in skipping mode.
+     * @param skipping
+     */
+    public void setSkipping(boolean skipping) {
+        this.skipping = skipping;
+    }
+
+    /**
+     * Return current state of the task.
+     * needs to be synchronized as communication thread
+     * sends the state every second
+     * @return task state
+     */
+    synchronized TaskStatus.State getState(){
+        return this.taskStatus.getRunState();
+    }
+    /**
+     * Set current state of the task.
+     * @param state
+     */
+    synchronized void setState(TaskStatus.State state){
+        this.taskStatus.setRunState(state);
+    }
+
+    void setTaskCleanupTask() {
+        taskCleanup = true;
+    }
+
+    boolean isTaskCleanupTask() {
+        return taskCleanup;
+    }
+
+    boolean isJobCleanupTask() {
+        return jobCleanup;
+    }
+
+    boolean isJobAbortTask() {
+        // the task is an abort task if its marked for cleanup and the final
+        // expected state is either failed or killed.
+        return isJobCleanupTask()
+                && (jobRunStateForCleanup == JobStatus.State.KILLED
+                || jobRunStateForCleanup == JobStatus.State.FAILED);
+    }
+
+    boolean isJobSetupTask() {
+        return jobSetup;
+    }
+
+    void setJobSetupTask() {
+        jobSetup = true;
+    }
+
+    void setJobCleanupTask() {
+        jobCleanup = true;
+    }
+
+    /**
+     * Sets the task to do job abort in the cleanup.
+     * @param status the final runstate of the job.
+     */
+    void setJobCleanupTaskState(JobStatus.State status) {
+        jobRunStateForCleanup = status;
+    }
+
+    boolean isMapOrReduce() {
+        return !jobSetup && !jobCleanup && !taskCleanup;
+    }
+
+    /**
+     * Get the name of the user running the job/task. TaskTracker needs task's
+     * user name even before it's JobConf is localized. So we explicitly serialize
+     * the user name.
+     *
+     * @return user
+     */
+    String getUser() {
+        return user;
+    }
+
+    void setUser(String user) {
+        this.user = user;
+    }
+
+    ////////////////////////////////////////////
+    // Writable methods
+    ////////////////////////////////////////////
+
+    public void write(DataOutput out) throws IOException {
+        Text.writeString(out, jobFile);
+        taskId.write(out);
+        out.writeInt(partition);
+        out.writeInt(numSlotsRequired);
+        taskStatus.write(out);
+        skipRanges.write(out);
+        out.writeBoolean(skipping);
+        out.writeBoolean(jobCleanup);
+        if (jobCleanup) {
+            WritableUtils.writeEnum(out, jobRunStateForCleanup);
+        }
+        out.writeBoolean(jobSetup);
+        out.writeBoolean(writeSkipRecs);
+        out.writeBoolean(taskCleanup);
+        Text.writeString(out, user);
+        extraData.write(out);
+    }
+
+    public void readFields(DataInput in) throws IOException {
+        jobFile = StringInterner.weakIntern(Text.readString(in));
+        taskId = TaskAttemptID.read(in);
+        partition = in.readInt();
+        numSlotsRequired = in.readInt();
+        taskStatus.readFields(in);
+        skipRanges.readFields(in);
+        currentRecIndexIterator = skipRanges.skipRangeIterator();
+        currentRecStartIndex = currentRecIndexIterator.next();
+        skipping = in.readBoolean();
+        jobCleanup = in.readBoolean();
+        if (jobCleanup) {
+            jobRunStateForCleanup =
+                    WritableUtils.readEnum(in, JobStatus.State.class);
+        }
+        jobSetup = in.readBoolean();
+        writeSkipRecs = in.readBoolean();
+        taskCleanup = in.readBoolean();
+        if (taskCleanup) {
+            setPhase(TaskStatus.Phase.CLEANUP);
+        }
+        user = StringInterner.weakIntern(Text.readString(in));
+        extraData.readFields(in);
+    }
+
+    @Override
+    public String toString() { return taskId.toString(); }
+
+    /**
+     * Localize the given JobConf to be specific for this task.
+     */
+    public void localizeConfiguration(JobConf conf) throws IOException {
+        conf.set(JobContext.TASK_ID, taskId.getTaskID().toString());
+        conf.set(JobContext.TASK_ATTEMPT_ID, taskId.toString());
+        conf.setBoolean(JobContext.TASK_ISMAP, isMapTask());
+        conf.setInt(JobContext.TASK_PARTITION, partition);
+        conf.set(JobContext.ID, taskId.getJobID().toString());
+    }
+
+    /** Run this task as a part of the named job.  This method is executed in the
+     * child process and is what invokes user-supplied map, reduce, etc. methods.
+     * @param umbilical for progress reports
+     */
+    public abstract void run(JobConf job, TaskUmbilicalProtocol umbilical)
+            throws IOException, ClassNotFoundException, InterruptedException;
+
+    /** The number of milliseconds between progress reports. */
+    public static final int PROGRESS_INTERVAL = 3000;
+
+    private transient Progress taskProgress = new Progress();
+
+    // Current counters
+    private transient Counters counters = new Counters();
+
+    /* flag to track whether task is done */
+    private AtomicBoolean taskDone = new AtomicBoolean(false);
+
+    public abstract boolean isMapTask();
+
+    public Progress getProgress() { return taskProgress; }
+
+    public void initialize(JobConf job, JobID id,
+                           Reporter reporter,
+                           boolean useNewApi) throws IOException,
+            ClassNotFoundException,
+            InterruptedException {
+        jobContext = new JobContextImpl(job, id, reporter);
+        taskContext = new TaskAttemptContextImpl(job, taskId, reporter);
+        if (getState() == TaskStatus.State.UNASSIGNED) {
+            setState(TaskStatus.State.RUNNING);
+        }
+        if (useNewApi) {
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("using new api for output committer");
+            }
+            outputFormat =
+                    ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), job);
+            committer = outputFormat.getOutputCommitter(taskContext);
+        } else {
+            committer = conf.getOutputCommitter();
+        }
+        Path outputPath = FileOutputFormat.getOutputPath(conf);
+        if (outputPath != null) {
+            if ((committer instanceof FileOutputCommitter)) {
+                FileOutputFormat.setWorkOutputPath(conf,
+                        ((FileOutputCommitter)committer).getTaskAttemptPath(taskContext));
+            } else {
+                FileOutputFormat.setWorkOutputPath(conf, outputPath);
+            }
+        }
+        committer.setupTask(taskContext);
+        Class<? extends ResourceCalculatorProcessTree> clazz =
+                conf.getClass(MRConfig.RESOURCE_CALCULATOR_PROCESS_TREE,
+                        null, ResourceCalculatorProcessTree.class);
+        pTree = ResourceCalculatorProcessTree
+                .getResourceCalculatorProcessTree(System.getenv().get("JVM_PID"), clazz, conf);
+        LOG.info(" Using ResourceCalculatorProcessTree : " + pTree);
+        if (pTree != null) {
+            pTree.updateProcessTree();
+            initCpuCumulativeTime = pTree.getCumulativeCpuTime();
+        }
+    }
+
+    public static String normalizeStatus(String status, Configuration conf) {
+        // Check to see if the status string is too long
+        // and truncate it if needed.
+        int progressStatusLength = conf.getInt(
+                MRConfig.PROGRESS_STATUS_LEN_LIMIT_KEY,
+                MRConfig.PROGRESS_STATUS_LEN_LIMIT_DEFAULT);
+        if (status.length() > progressStatusLength) {
+            LOG.warn("Task status: \"" + status + "\" truncated to max limit ("
+                    + progressStatusLength + " characters)");
+            status = status.substring(0, progressStatusLength);
+        }
+        return status;
+    }
+
+    @InterfaceAudience.LimitedPrivate({"MapReduce"})
+    @InterfaceStability.Unstable
+    public class TaskReporter
+            extends org.apache.hadoop.mapreduce.StatusReporter
+            implements Runnable, Reporter {
+        private TaskUmbilicalProtocol umbilical;
+        private InputSplit split = null;
+        private Progress taskProgress;
+        private Thread pingThread = null;
+        private boolean done = true;
+        private Object lock = new Object();
+
+        /**
+         * flag that indicates whether progress update needs to be sent to parent.
+         * If true, it has been set. If false, it has been reset.
+         * Using AtomicBoolean since we need an atomic read & reset method.
+         */
+        private AtomicBoolean progressFlag = new AtomicBoolean(false);
+
+        TaskReporter(Progress taskProgress,
+                     TaskUmbilicalProtocol umbilical) {
+            this.umbilical = umbilical;
+            this.taskProgress = taskProgress;
+        }
+
+        // getters and setters for flag
+        void setProgressFlag() {
+            progressFlag.set(true);
+        }
+        boolean resetProgressFlag() {
+            return progressFlag.getAndSet(false);
+        }
+        public void setStatus(String status) {
+            taskProgress.setStatus(normalizeStatus(status, conf));
+            // indicate that progress update needs to be sent
+            setProgressFlag();
+        }
+        public void setProgress(float progress) {
+            // set current phase progress.
+            // This method assumes that task has phases.
+            taskProgress.phase().set(progress);
+            // indicate that progress update needs to be sent
+            setProgressFlag();
+        }
+
+        public float getProgress() {
+            return taskProgress.getProgress();
+        };
+
+        public void progress() {
+            // indicate that progress update needs to be sent
+            setProgressFlag();
+        }
+        public Counters.Counter getCounter(String group, String name) {
+            Counters.Counter counter = null;
+            if (counters != null) {
+                counter = counters.findCounter(group, name);
+            }
+            return counter;
+        }
+        public Counters.Counter getCounter(Enum<?> name) {
+            return counters == null ? null : counters.findCounter(name);
+        }
+        public void incrCounter(Enum key, long amount) {
+            if (counters != null) {
+                counters.incrCounter(key, amount);
+            }
+            setProgressFlag();
+        }
+        public void incrCounter(String group, String counter, long amount) {
+            if (counters != null) {
+                counters.incrCounter(group, counter, amount);
+            }
+            if(skipping && SkipBadRecords.COUNTER_GROUP.equals(group) && (
+                    SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS.equals(counter) ||
+                            SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS.equals(counter))) {
+                //if application reports the processed records, move the
+                //currentRecStartIndex to the next.
+                //currentRecStartIndex is the start index which has not yet been
+                //finished and is still in task's stomach.
+                for(int i=0;i<amount;i++) {
+                    currentRecStartIndex = currentRecIndexIterator.next();
+                }
+            }
+            setProgressFlag();
+        }
+        public void setInputSplit(InputSplit split) {
+            this.split = split;
+        }
+        public InputSplit getInputSplit() throws UnsupportedOperationException {
+            if (split == null) {
+                throw new UnsupportedOperationException("Input only available on map");
+            } else {
+                return split;
+            }
+        }
+        /**
+         * The communication thread handles communication with the parent (Task Tracker).
+         * It sends progress updates if progress has been made or if the task needs to
+         * let the parent know that it's alive. It also pings the parent to see if it's alive.
+         */
+        public void run() {
+            final int MAX_RETRIES = 3;
+            int remainingRetries = MAX_RETRIES;
+            // get current flag value and reset it as well
+            boolean sendProgress = resetProgressFlag();
+            while (!taskDone.get()) {
+                synchronized (lock) {
+                    done = false;
+                }
+                try {
+                    boolean taskFound = true; // whether TT knows about this task
+                    // sleep for a bit
+                    synchronized(lock) {
+                        if (taskDone.get()) {
+                            break;
+                        }
+                        lock.wait(PROGRESS_INTERVAL);
+                    }
+                    if (taskDone.get()) {
+                        break;
+                    }
+
+                    if (sendProgress) {
+                        // we need to send progress update
+                        updateCounters();
+                        taskStatus.statusUpdate(taskProgress.get(),
+                                taskProgress.toString(),
+                                counters);
+                        taskFound = umbilical.statusUpdate(taskId, taskStatus);
+                        taskStatus.clearStatus();
+                    }
+                    else {
+                        // send ping
+                        taskFound = umbilical.ping(taskId);
+                    }
+
+                    // if Task Tracker is not aware of our task ID (probably because it died and
+                    // came back up), kill ourselves
+                    if (!taskFound) {
+                        LOG.warn("Parent died.  Exiting "+taskId);
+                        resetDoneFlag();
+                        System.exit(66);
+                    }
+
+                    sendProgress = resetProgressFlag();
+                    remainingRetries = MAX_RETRIES;
+                }
+                catch (Throwable t) {
+                    LOG.info("Communication exception: " + StringUtils.stringifyException(t));
+                    remainingRetries -=1;
+                    if (remainingRetries == 0) {
+                        ReflectionUtils.logThreadInfo(LOG, "Communication exception", 0);
+                        LOG.warn("Last retry, killing "+taskId);
+                        resetDoneFlag();
+                        System.exit(65);
+                    }
+                }
+            }
+            //Notify that we are done with the work
+            resetDoneFlag();
+        }
+        void resetDoneFlag() {
+            synchronized (lock) {
+                done = true;
+                lock.notify();
+            }
+        }
+        public void startCommunicationThread() {
+            if (pingThread == null) {
+                pingThread = new Thread(this, "communication thread");
+                pingThread.setDaemon(true);
+                pingThread.start();
+            }
+        }
+        public void stopCommunicationThread() throws InterruptedException {
+            if (pingThread != null) {
+                // Intent of the lock is to not send an interupt in the middle of an
+                // umbilical.ping or umbilical.statusUpdate
+                synchronized(lock) {
+                    //Interrupt if sleeping. Otherwise wait for the RPC call to return.
+                    lock.notify();
+                }
+
+                synchronized (lock) {
+                    while (!done) {
+                        lock.wait();
+                    }
+                }
+                pingThread.interrupt();
+                pingThread.join();
+            }
+        }
+    }
+
+    /**
+     *  Reports the next executing record range to TaskTracker.
+     *
+     * @param umbilical
+     * @param nextRecIndex the record index which would be fed next.
+     * @throws IOException
+     */
+    protected void reportNextRecordRange(final TaskUmbilicalProtocol umbilical,
+                                         long nextRecIndex) throws IOException{
+        //currentRecStartIndex is the start index which has not yet been finished
+        //and is still in task's stomach.
+        long len = nextRecIndex - currentRecStartIndex +1;
+        SortedRanges.Range range =
+                new SortedRanges.Range(currentRecStartIndex, len);
+        taskStatus.setNextRecordRange(range);
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("sending reportNextRecordRange " + range);
+        }
+        umbilical.reportNextRecordRange(taskId, range);
+    }
+
+    /**
+     * Create a TaskReporter and start communication thread
+     */
+    TaskReporter startReporter(final TaskUmbilicalProtocol umbilical) {
+        // start thread that will handle communication with parent
+        TaskReporter reporter = new TaskReporter(getProgress(), umbilical);
+        reporter.startCommunicationThread();
+        return reporter;
+    }
+
+    /**
+     * Update resource information counters
+     */
+    void updateResourceCounters() {
+        // Update generic resource counters
+        updateHeapUsageCounter();
+
+        // Updating resources specified in ResourceCalculatorProcessTree
+        if (pTree == null) {
+            return;
+        }
+        pTree.updateProcessTree();
+        long cpuTime = pTree.getCumulativeCpuTime();
+        long pMem = pTree.getCumulativeRssmem();
+        long vMem = pTree.getCumulativeVmem();
+        // Remove the CPU time consumed previously by JVM reuse
+        cpuTime -= initCpuCumulativeTime;
+        counters.findCounter(TaskCounter.CPU_MILLISECONDS).setValue(cpuTime);
+        counters.findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES).setValue(pMem);
+        counters.findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES).setValue(vMem);
+    }
+
+    /**
+     * An updater that tracks the amount of time this task has spent in GC.
+     */
+    class GcTimeUpdater {
+        private long lastGcMillis = 0;
+        private List<GarbageCollectorMXBean> gcBeans = null;
+
+        public GcTimeUpdater() {
+            this.gcBeans = ManagementFactory.getGarbageCollectorMXBeans();
+            getElapsedGc(); // Initialize 'lastGcMillis' with the current time spent.
+        }
+
+        /**
+         * @return the number of milliseconds that the gc has used for CPU
+         * since the last time this method was called.
+         */
+        protected long getElapsedGc() {
+            long thisGcMillis = 0;
+            for (GarbageCollectorMXBean gcBean : gcBeans) {
+                thisGcMillis += gcBean.getCollectionTime();
+            }
+
+            long delta = thisGcMillis - lastGcMillis;
+            this.lastGcMillis = thisGcMillis;
+            return delta;
+        }
+
+        /**
+         * Increment the gc-elapsed-time counter.
+         */
+        public void incrementGcCounter() {
+            if (null == counters) {
+                return; // nothing to do.
+            }
+
+            org.apache.hadoop.mapred.Counters.Counter gcCounter =
+                    counters.findCounter(TaskCounter.GC_TIME_MILLIS);
+            if (null != gcCounter) {
+                gcCounter.increment(getElapsedGc());
+            }
+        }
+    }
+
+    /**
+     * An updater that tracks the last number reported for a given file
+     * system and only creates the counters when they are needed.
+     */
+    class FileSystemStatisticUpdater {
+        private List<FileSystem.Statistics> stats;
+        private Counters.Counter readBytesCounter, writeBytesCounter,
+                readOpsCounter, largeReadOpsCounter, writeOpsCounter;
+        private String scheme;
+        FileSystemStatisticUpdater(List<FileSystem.Statistics> stats, String scheme) {
+            this.stats = stats;
+            this.scheme = scheme;
+        }
+
+        void updateCounters() {
+            if (readBytesCounter == null) {
+                readBytesCounter = counters.findCounter(scheme,
+                        FileSystemCounter.BYTES_READ);
+            }
+            if (writeBytesCounter == null) {
+                writeBytesCounter = counters.findCounter(scheme,
+                        FileSystemCounter.BYTES_WRITTEN);
+            }
+            if (readOpsCounter == null) {
+                readOpsCounter = counters.findCounter(scheme,
+                        FileSystemCounter.READ_OPS);
+            }
+            if (largeReadOpsCounter == null) {
+                largeReadOpsCounter = counters.findCounter(scheme,
+                        FileSystemCounter.LARGE_READ_OPS);
+            }
+            if (writeOpsCounter == null) {
+                writeOpsCounter = counters.findCounter(scheme,
+                        FileSystemCounter.WRITE_OPS);
+            }
+            long readBytes = 0;
+            long writeBytes = 0;
+            long readOps = 0;
+            long largeReadOps = 0;
+            long writeOps = 0;
+            for (FileSystem.Statistics stat: stats) {
+                readBytes = readBytes + stat.getBytesRead();
+                writeBytes = writeBytes + stat.getBytesWritten();
+                readOps = readOps + stat.getReadOps();
+                largeReadOps = largeReadOps + stat.getLargeReadOps();
+                writeOps = writeOps + stat.getWriteOps();
+            }
+            readBytesCounter.setValue(readBytes);
+            writeBytesCounter.setValue(writeBytes);
+            readOpsCounter.setValue(readOps);
+            largeReadOpsCounter.setValue(largeReadOps);
+            writeOpsCounter.setValue(writeOps);
+        }
+    }
+
+    /**
+     * A Map where Key-> URIScheme and value->FileSystemStatisticUpdater
+     */
+    private Map<String, FileSystemStatisticUpdater> statisticUpdaters =
+            new HashMap<String, FileSystemStatisticUpdater>();
+
+    private synchronized void updateCounters() {
+        Map<String, List<FileSystem.Statistics>> map = new
+                HashMap<String, List<FileSystem.Statistics>>();
+        for(Statistics stat: FileSystem.getAllStatistics()) {
+            String uriScheme = stat.getScheme();
+            if (map.containsKey(uriScheme)) {
+                List<FileSystem.Statistics> list = map.get(uriScheme);
+                list.add(stat);
+            } else {
+                List<FileSystem.Statistics> list = new ArrayList<FileSystem.Statistics>();
+                list.add(stat);
+                map.put(uriScheme, list);
+            }
+        }
+        for (Map.Entry<String, List<FileSystem.Statistics>> entry: map.entrySet()) {
+            FileSystemStatisticUpdater updater = statisticUpdaters.get(entry.getKey());
+            if(updater==null) {//new FileSystem has been found in the cache
+                updater = new FileSystemStatisticUpdater(entry.getValue(), entry.getKey());
+                statisticUpdaters.put(entry.getKey(), updater);
+            }
+            updater.updateCounters();
+        }
+
+        gcUpdater.incrementGcCounter();
+        updateResourceCounters();
+    }
+
+    /**
+     * Updates the {@link TaskCounter#COMMITTED_HEAP_BYTES} counter to reflect the
+     * current total committed heap space usage of this JVM.
+     */
+    @SuppressWarnings("deprecation")
+    private void updateHeapUsageCounter() {
+        long currentHeapUsage = Runtime.getRuntime().totalMemory();
+        counters.findCounter(TaskCounter.COMMITTED_HEAP_BYTES)
+                .setValue(currentHeapUsage);
+    }
+
+    public void done(TaskUmbilicalProtocol umbilical,
+                     TaskReporter reporter
+    ) throws IOException, InterruptedException {
+        LOG.info("Task:" + taskId + " is done."
+                + " And is in the process of committing");
+        updateCounters();
+
+        boolean commitRequired = isCommitRequired();
+        if (commitRequired) {
+            int retries = MAX_RETRIES;
+            setState(TaskStatus.State.COMMIT_PENDING);
+            // say the task tracker that task is commit pending
+            while (true) {
+                try {
+                    umbilical.commitPending(taskId, taskStatus);
+                    break;
+                } catch (InterruptedException ie) {
+                    // ignore
+                } catch (IOException ie) {
+                    LOG.warn("Failure sending commit pending: " +
+                            StringUtils.stringifyException(ie));
+                    if (--retries == 0) {
+                        System.exit(67);
+                    }
+                }
+            }
+            //wait for commit approval and commit
+            commit(umbilical, reporter, committer);
+        }
+        taskDone.set(true);
+        reporter.stopCommunicationThread();
+        // Make sure we send at least one set of counter increments. It's
+        // ok to call updateCounters() in this thread after comm thread stopped.
+        updateCounters();
+        sendLastUpdate(umbilical);
+        //signal the tasktracker that we are done
+        sendDone(umbilical);
+    }
+
+    /**
+     * Checks if this task has anything to commit, depending on the
+     * type of task, as well as on whether the {@link OutputCommitter}
+     * has anything to commit.
+     *
+     * @return true if the task has to commit
+     * @throws IOException
+     */
+    boolean isCommitRequired() throws IOException {
+        boolean commitRequired = false;
+        if (isMapOrReduce()) {
+            commitRequired = committer.needsTaskCommit(taskContext);
+        }
+        return commitRequired;
+    }
+
+    /**
+     * Send a status update to the task tracker
+     * @param umbilical
+     * @throws IOException
+     */
+    public void statusUpdate(TaskUmbilicalProtocol umbilical)
+            throws IOException {
+        int retries = MAX_RETRIES;
+        while (true) {
+            try {
+                if (!umbilical.statusUpdate(getTaskID(), taskStatus)) {
+                    LOG.warn("Parent died.  Exiting "+taskId);
+                    System.exit(66);
+                }
+                taskStatus.clearStatus();
+                return;
+            } catch (InterruptedException ie) {
+                Thread.currentThread().interrupt(); // interrupt ourself
+            } catch (IOException ie) {
+                LOG.warn("Failure sending status update: " +
+                        StringUtils.stringifyException(ie));
+                if (--retries == 0) {
+                    throw ie;
+                }
+            }
+        }
+    }
+
+    /**
+     * Sends last status update before sending umbilical.done();
+     */
+    private void sendLastUpdate(TaskUmbilicalProtocol umbilical)
+            throws IOException {
+        taskStatus.setOutputSize(calculateOutputSize());
+        // send a final status report
+        taskStatus.statusUpdate(taskProgress.get(),
+                taskProgress.toString(),
+                counters);
+        statusUpdate(umbilical);
+    }
+
+    /**
+     * Calculates the size of output for this task.
+     *
+     * @return -1 if it can't be found.
+     */
+    private long calculateOutputSize() throws IOException {
+        if (!isMapOrReduce()) {
+            return -1;
+        }
+
+        if (isMapTask() && conf.getNumReduceTasks() > 0) {
+            try {
+                Path mapOutput =  mapOutputFile.getOutputFile();
+                FileSystem localFS = FileSystem.getLocal(conf);
+                return localFS.getFileStatus(mapOutput).getLen();
+            } catch (IOException e) {
+                LOG.warn ("Could not find output size " , e);
+            }
+        }
+        return -1;
+    }
+
+    private void sendDone(TaskUmbilicalProtocol umbilical) throws IOException {
+        int retries = MAX_RETRIES;
+        while (true) {
+            try {
+                umbilical.done(getTaskID());
+                LOG.info("Task '" + taskId + "' done.");
+                return;
+            } catch (IOException ie) {
+                LOG.warn("Failure signalling completion: " +
+                        StringUtils.stringifyException(ie));
+                if (--retries == 0) {
+                    throw ie;
+                }
+            }
+        }
+    }
+
+    private void commit(TaskUmbilicalProtocol umbilical,
+                        TaskReporter reporter,
+                        org.apache.hadoop.mapreduce.OutputCommitter committer
+    ) throws IOException {
+        int retries = MAX_RETRIES;
+        while (true) {
+            try {
+                while (!umbilical.canCommit(taskId)) {
+                    try {
+                        Thread.sleep(1000);
+                    } catch(InterruptedException ie) {
+                        //ignore
+                    }
+                    reporter.setProgressFlag();
+                }
+                break;
+            } catch (IOException ie) {
+                LOG.warn("Failure asking whether task can commit: " +
+                        StringUtils.stringifyException(ie));
+                if (--retries == 0) {
+                    //if it couldn't query successfully then delete the output
+                    discardOutput(taskContext);
+                    System.exit(68);
+                }
+            }
+        }
+
+        // task can Commit now
+        try {
+            LOG.info("Task " + taskId + " is allowed to commit now");
+            committer.commitTask(taskContext);
+            return;
+        } catch (IOException iee) {
+            LOG.warn("Failure committing: " +
+                    StringUtils.stringifyException(iee));
+            //if it couldn't commit a successfully then delete the output
+            discardOutput(taskContext);
+            throw iee;
+        }
+    }
+
+    private
+    void discardOutput(TaskAttemptContext taskContext) {
+        try {
+            committer.abortTask(taskContext);
+        } catch (IOException ioe)  {
+            LOG.warn("Failure cleaning up: " +
+                    StringUtils.stringifyException(ioe));
+        }
+    }
+
+    protected void runTaskCleanupTask(TaskUmbilicalProtocol umbilical,
+                                      TaskReporter reporter)
+            throws IOException, InterruptedException {
+        taskCleanup(umbilical);
+        done(umbilical, reporter);
+    }
+
+    void taskCleanup(TaskUmbilicalProtocol umbilical)
+            throws IOException {
+        // set phase for this task
+        setPhase(TaskStatus.Phase.CLEANUP);
+        getProgress().setStatus("cleanup");
+        statusUpdate(umbilical);
+        LOG.info("Runnning cleanup for the task");
+        // do the cleanup
+        committer.abortTask(taskContext);
+    }
+
+    protected void runJobCleanupTask(TaskUmbilicalProtocol umbilical,
+                                     TaskReporter reporter
+    ) throws IOException, InterruptedException {
+        // set phase for this task
+        setPhase(TaskStatus.Phase.CLEANUP);
+        getProgress().setStatus("cleanup");
+        statusUpdate(umbilical);
+        // do the cleanup
+        LOG.info("Cleaning up job");
+        if (jobRunStateForCleanup == JobStatus.State.FAILED
+                || jobRunStateForCleanup == JobStatus.State.KILLED) {
+            LOG.info("Aborting job with runstate : " + jobRunStateForCleanup.name());
+            if (conf.getUseNewMapper()) {
+                committer.abortJob(jobContext, jobRunStateForCleanup);
+            } else {
+                org.apache.hadoop.mapred.OutputCommitter oldCommitter =
+                        (org.apache.hadoop.mapred.OutputCommitter)committer;
+                oldCommitter.abortJob(jobContext, jobRunStateForCleanup);
+            }
+        } else if (jobRunStateForCleanup == JobStatus.State.SUCCEEDED){
+            LOG.info("Committing job");
+            committer.commitJob(jobContext);
+        } else {
+            throw new IOException("Invalid state of the job for cleanup. State found "
+                    + jobRunStateForCleanup + " expecting "
+                    + JobStatus.State.SUCCEEDED + ", "
+                    + JobStatus.State.FAILED + " or "
+                    + JobStatus.State.KILLED);
+        }
+
+        // delete the staging area for the job
+        JobConf conf = new JobConf(jobContext.getConfiguration());
+        if (!keepTaskFiles(conf)) {
+            String jobTempDir = conf.get(MRJobConfig.MAPREDUCE_JOB_DIR);
+            Path jobTempDirPath = new Path(jobTempDir);
+            FileSystem fs = jobTempDirPath.getFileSystem(conf);
+            fs.delete(jobTempDirPath, true);
+        }
+        done(umbilical, reporter);
+    }
+
+    protected boolean keepTaskFiles(JobConf conf) {
+        return (conf.getKeepTaskFilesPattern() != null || conf
+                .getKeepFailedTaskFiles());
+    }
+
+    protected void runJobSetupTask(TaskUmbilicalProtocol umbilical,
+                                   TaskReporter reporter
+    ) throws IOException, InterruptedException {
+        // do the setup
+        getProgress().setStatus("setup");
+        committer.setupJob(jobContext);
+        done(umbilical, reporter);
+    }
+
+    public void setConf(Configuration conf) {
+        if (conf instanceof JobConf) {
+            this.conf = (JobConf) conf;
+        } else {
+            this.conf = new JobConf(conf);
+        }
+        this.mapOutputFile = ReflectionUtils.newInstance(
+                conf.getClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,
+                        MROutputFiles.class, MapOutputFile.class), conf);
+        this.lDirAlloc = new LocalDirAllocator(MRConfig.LOCAL_DIR);
+        // add the static resolutions (this is required for the junit to
+        // work on testcases that simulate multiple nodes on a single physical
+        // node.
+        String hostToResolved[] = conf.getStrings(MRConfig.STATIC_RESOLUTIONS);
+        if (hostToResolved != null) {
+            for (String str : hostToResolved) {
+                String name = str.substring(0, str.indexOf('='));
+                String resolvedName = str.substring(str.indexOf('=') + 1);
+                NetUtils.addStaticResolution(name, resolvedName);
+            }
+        }
+    }
+
+    public Configuration getConf() {
+        return this.conf;
+    }
+
+    public MapOutputFile getMapOutputFile() {
+        return mapOutputFile;
+    }
+
+    /**
+     * OutputCollector for the combiner.
+     */
+    @InterfaceAudience.Private
+    @InterfaceStability.Unstable
+    public static class CombineOutputCollector<K extends Object, V extends Object>
+            implements OutputCollector<K, V> {
+        private Writer<K, V> writer;
+        private Counters.Counter outCounter;
+        private Progressable progressable;
+        private long progressBar;
+
+        public CombineOutputCollector(Counters.Counter outCounter, Progressable progressable, Configuration conf) {
+            this.outCounter = outCounter;
+            this.progressable=progressable;
+            progressBar = conf.getLong(MRJobConfig.COMBINE_RECORDS_BEFORE_PROGRESS, DEFAULT_COMBINE_RECORDS_BEFORE_PROGRESS);
+        }
+
+        public synchronized void setWriter(Writer<K, V> writer) {
+            this.writer = writer;
+        }
+
+        public synchronized void collect(K key, V value)
+                throws IOException {
+            outCounter.increment(1);
+            writer.append(key, value);
+            if ((outCounter.getValue() % progressBar) == 0) {
+                progressable.progress();
+            }
+        }
+    }
+
+    /** Iterates values while keys match in sorted input. */
+    static class ValuesIterator<KEY,VALUE> implements Iterator<VALUE> {
+        protected RawKeyValueIterator in; //input iterator
+        private KEY key;               // current key
+        private KEY nextKey;
+        private VALUE value;             // current value
+        private boolean hasNext;                      // more w/ this key
+        private boolean more;                         // more in file
+        private RawComparator<KEY> comparator;
+        protected Progressable reporter;
+        private Deserializer<KEY> keyDeserializer;
+        private Deserializer<VALUE> valDeserializer;
+        private DataInputBuffer keyIn = new DataInputBuffer();
+        private DataInputBuffer valueIn = new DataInputBuffer();
+
+        public ValuesIterator (RawKeyValueIterator in,
+                               RawComparator<KEY> comparator,
+                               Class<KEY> keyClass,
+                               Class<VALUE> valClass, Configuration conf,
+                               Progressable reporter)
+                throws IOException {
+            this.in = in;
+            this.comparator = comparator;
+            this.reporter = reporter;
+            SerializationFactory serializationFactory = new SerializationFactory(conf);
+            this.keyDeserializer = serializationFactory.getDeserializer(keyClass);
+            this.keyDeserializer.open(keyIn);
+            this.valDeserializer = serializationFactory.getDeserializer(valClass);
+            this.valDeserializer.open(this.valueIn);
+            readNextKey();
+            key = nextKey;
+            nextKey = null; // force new instance creation
+            hasNext = more;
+        }
+
+        RawKeyValueIterator getRawIterator() { return in; }
+
+        /// Iterator methods
+
+        public boolean hasNext() { return hasNext; }
+
+        private int ctr = 0;
+        public VALUE next() {
+            if (!hasNext) {
+                throw new NoSuchElementException("iterate past last value");
+            }
+            try {
+                readNextValue();
+                readNextKey();
+            } catch (IOException ie) {
+                throw new RuntimeException("problem advancing post rec#"+ctr, ie);
+            }
+            reporter.progress();
+            return value;
+        }
+
+        public void remove() { throw new RuntimeException("not implemented"); }
+
+        /// Auxiliary methods
+
+        /** Start processing next unique key. */
+        public void nextKey() throws IOException {
+            // read until we find a new key
+            while (hasNext) {
+                readNextKey();
+            }
+            ++ctr;
+
+            // move the next key to the current one
+            KEY tmpKey = key;
+            key = nextKey;
+            nextKey = tmpKey;
+            hasNext = more;
+        }
+
+        /** True iff more keys remain. */
+        public boolean more() {
+            return more;
+        }
+
+        /** The current key. */
+        public KEY getKey() {
+            return key;
+        }
+
+        /**
+         * read the next key
+         */
+        private void readNextKey() throws IOException {
+            more = in.next();
+            if (more) {
+                DataInputBuffer nextKeyBytes = in.getKey();
+                keyIn.reset(nextKeyBytes.getData(), nextKeyBytes.getPosition(), nextKeyBytes.getLength());
+                nextKey = keyDeserializer.deserialize(nextKey);
+                hasNext = key != null && (comparator.compare(key, nextKey) == 0);
+            } else {
+                hasNext = false;
+            }
+        }
+
+        /**
+         * Read the next value
+         * @throws IOException
+         */
+        private void readNextValue() throws IOException {
+            DataInputBuffer nextValueBytes = in.getValue();
+            valueIn.reset(nextValueBytes.getData(), nextValueBytes.getPosition(), nextValueBytes.getLength());
+            value = valDeserializer.deserialize(value);
+        }
+    }
+
+    /** Iterator to return Combined values */
+    @InterfaceAudience.Private
+    @InterfaceStability.Unstable
+    public static class CombineValuesIterator<KEY,VALUE>
+            extends ValuesIterator<KEY,VALUE> {
+
+        private final Counters.Counter combineInputCounter;
+
+        public CombineValuesIterator(RawKeyValueIterator in,
+                                     RawComparator<KEY> comparator, Class<KEY> keyClass,
+                                     Class<VALUE> valClass, Configuration conf, Reporter reporter,
+                                     Counters.Counter combineInputCounter) throws IOException {
+            super(in, comparator, keyClass, valClass, conf, reporter);
+            this.combineInputCounter = combineInputCounter;
+        }
+
+        public VALUE next() {
+            combineInputCounter.increment(1);
+            return super.next();
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    protected static <INKEY,INVALUE,OUTKEY,OUTVALUE>
+    org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context
+    createReduceContext(org.apache.hadoop.mapreduce.Reducer
+                                <INKEY,INVALUE,OUTKEY,OUTVALUE> reducer,
+                        Configuration job,
+                        org.apache.hadoop.mapreduce.TaskAttemptID taskId,
+                        RawKeyValueIterator rIter,
+                        org.apache.hadoop.mapreduce.Counter inputKeyCounter,
+                        org.apache.hadoop.mapreduce.Counter inputValueCounter,
+                        org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> output,
+                        org.apache.hadoop.mapreduce.OutputCommitter committer,
+                        org.apache.hadoop.mapreduce.StatusReporter reporter,
+                        RawComparator<INKEY> comparator,
+                        Class<INKEY> keyClass, Class<INVALUE> valueClass
+    ) throws IOException, InterruptedException {
+        org.apache.hadoop.mapreduce.ReduceContext<INKEY, INVALUE, OUTKEY, OUTVALUE>
+                reduceContext =
+                new ReduceContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, taskId,
+                        rIter,
+                        inputKeyCounter,
+                        inputValueCounter,
+                        output,
+                        committer,
+                        reporter,
+                        comparator,
+                        keyClass,
+                        valueClass);
+
+        org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context
+                reducerContext =
+                new WrappedReducer<INKEY, INVALUE, OUTKEY, OUTVALUE>().getReducerContext(
+                        reduceContext);
+
+        return reducerContext;
+    }
+
+    @InterfaceAudience.LimitedPrivate({"MapReduce"})
+    @InterfaceStability.Unstable
+    public static abstract class CombinerRunner<K,V> {
+        protected final Counters.Counter inputCounter;
+        protected final JobConf job;
+        protected final TaskReporter reporter;
+
+        CombinerRunner(Counters.Counter inputCounter,
+                       JobConf job,
+                       TaskReporter reporter) {
+            this.inputCounter = inputCounter;
+            this.job = job;
+            this.reporter = reporter;
+        }
+
+        /**
+         * Run the combiner over a set of inputs.
+         * @param iterator the key/value pairs to use as input
+         * @param collector the output collector
+         */
+        public abstract void combine(RawKeyValueIterator iterator,
+                                     OutputCollector<K,V> collector
+        ) throws IOException, InterruptedException,
+                ClassNotFoundException;
+
+        @SuppressWarnings("unchecked")
+        public static <K,V>
+        CombinerRunner<K,V> create(JobConf job,
+                                   TaskAttemptID taskId,
+                                   Counters.Counter inputCounter,
+                                   TaskReporter reporter,
+                                   org.apache.hadoop.mapreduce.OutputCommitter committer
+        ) throws ClassNotFoundException {
+            Class<? extends Reducer<K,V,K,V>> cls =
+                    (Class<? extends Reducer<K,V,K,V>>) job.getCombinerClass();
+
+            if (cls != null) {
+                return new OldCombinerRunner(cls, job, inputCounter, reporter);
+            }
+            // make a task context so we can get the classes
+            org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
+                    new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, taskId,
+                            reporter);
+            Class<? extends org.apache.hadoop.mapreduce.Reducer<K,V,K,V>> newcls =
+                    (Class<? extends org.apache.hadoop.mapreduce.Reducer<K,V,K,V>>)
+                            taskContext.getCombinerClass();
+            if (newcls != null) {
+                return new NewCombinerRunner<K,V>(newcls, job, taskId, taskContext,
+                        inputCounter, reporter, committer);
+            }
+
+            return null;
+        }
+    }
+
+    @InterfaceAudience.Private
+    @InterfaceStability.Unstable
+    protected static class OldCombinerRunner<K,V> extends CombinerRunner<K,V> {
+        private final Class<? extends Reducer<K,V,K,V>> combinerClass;
+        private final Class<K> keyClass;
+        private final Class<V> valueClass;
+        private final RawComparator<K> comparator;
+
+        @SuppressWarnings("unchecked")
+        protected OldCombinerRunner(Class<? extends Reducer<K,V,K,V>> cls,
+                                    JobConf conf,
+                                    Counters.Counter inputCounter,
+                                    TaskReporter reporter) {
+            super(inputCounter, conf, reporter);
+            combinerClass = cls;
+            keyClass = (Class<K>) job.getMapOutputKeyClass();
+            valueClass = (Class<V>) job.getMapOutputValueClass();
+            comparator = (RawComparator<K>)
+                    job.getCombinerKeyGroupingComparator();
+        }
+
+        @SuppressWarnings("unchecked")
+        public void combine(RawKeyValueIterator kvIter,
+                            OutputCollector<K,V> combineCollector
+        ) throws IOException {
+            Reducer<K,V,K,V> combiner =
+                    ReflectionUtils.newInstance(combinerClass, job);
+            try {
+                CombineValuesIterator<K,V> values =
+                        new CombineValuesIterator<K,V>(kvIter, comparator, keyClass,
+                                valueClass, job, reporter,
+                                inputCounter);
+                while (values.more()) {
+                    combiner.reduce(values.getKey(), values, combineCollector,
+                            reporter);
+                    values.nextKey();
+                }
+            } finally {
+                combiner.close();
+            }
+        }
+    }
+
+    @InterfaceAudience.Private
+    @InterfaceStability.Unstable
+    protected static class NewCombinerRunner<K, V> extends CombinerRunner<K,V> {
+        private final Class<? extends org.apache.hadoop.mapreduce.Reducer<K,V,K,V>>
+                reducerClass;
+        private final org.apache.hadoop.mapreduce.TaskAttemptID taskId;
+        private final RawComparator<K> comparator;
+        private final Class<K> keyClass;
+        private final Class<V> valueClass;
+        private final org.apache.hadoop.mapreduce.OutputCommitter committer;
+
+        @SuppressWarnings("unchecked")
+        NewCombinerRunner(Class reducerClass,
+                          JobConf job,
+                          org.apache.hadoop.mapreduce.TaskAttemptID taskId,
+                          org.apache.hadoop.mapreduce.TaskAttemptContext context,
+                          Counters.Counter inputCounter,
+                          TaskReporter reporter,
+                          org.apache.hadoop.mapreduce.OutputCommitter committer) {
+            super(inputCounter, job, reporter);
+            this.reducerClass = reducerClass;
+            this.taskId = taskId;
+            keyClass = (Class<K>) context.getMapOutputKeyClass();
+            valueClass = (Class<V>) context.getMapOutputValueClass();
+            comparator = (RawComparator<K>) context.getCombinerKeyGroupingComparator();
+            this.committer = committer;
+        }
+
+        private static class OutputConverter<K,V>
+                extends org.apache.hadoop.mapreduce.RecordWriter<K,V> {
+            OutputCollector<K,V> output;
+            OutputConverter(OutputCollector<K,V> output) {
+                this.output = output;
+            }
+
+            @Override
+            public void close(org.apache.hadoop.mapreduce.TaskAttemptContext context){
+            }
+
+            @Override
+            public void write(K key, V value
+            ) throws IOException, InterruptedException {
+                output.collect(key,value);
+            }
+        }
+
+        @SuppressWarnings("unchecked")
+        @Override
+        public void combine(RawKeyValueIterator iterator,
+                            OutputCollector<K,V> collector
+        ) throws IOException, InterruptedException,
+                ClassNotFoundException {
+            // make a reducer
+            org.apache.hadoop.mapreduce.Reducer<K,V,K,V> reducer =
+                    (org.apache.hadoop.mapreduce.Reducer<K,V,K,V>)
+                            ReflectionUtils.newInstance(reducerClass, job);
+            org.apache.hadoop.mapreduce.Reducer.Context
+                    reducerContext = createReduceContext(reducer, job, taskId,
+                    iterator, null, inputCounter,
+                    new OutputConverter(collector),
+                    committer,
+                    reporter, comparator, keyClass,
+                    valueClass);
+            reducer.run(reducerContext);
+        }
+    }
+
+    BytesWritable getExtraData() {
+        return extraData;
+    }
+
+    void setExtraData(BytesWritable extraData) {
+        this.extraData = extraData;
+    }
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/mapred/YarnChild.java b/hadoop-client/src/main/java/org/apache/hadoop/mapred/YarnChild.java
new file mode 100644
index 0000000..c1e896b
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/mapred/YarnChild.java
@@ -0,0 +1,385 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapred;
+
+import static java.util.concurrent.TimeUnit.MILLISECONDS;
+
+import java.io.IOException;
+import java.io.OutputStream;
+import java.net.InetSocketAddress;
+import java.net.URI;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.ScheduledExecutorService;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSError;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.LocalDirAllocator;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.granular.GranularLoggerStore;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.ipc.RPC;
+import org.apache.hadoop.mapreduce.MRConfig;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.hadoop.mapreduce.counters.Limits;
+import org.apache.hadoop.mapreduce.filecache.DistributedCache;
+import org.apache.hadoop.mapreduce.security.TokenCache;
+import org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier;
+import org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager;
+import org.apache.hadoop.mapreduce.v2.util.MRApps;
+import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;
+import org.apache.hadoop.metrics2.source.JvmMetrics;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.util.DiskChecker.DiskErrorException;
+import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.yarn.YarnUncaughtExceptionHandler;
+import org.apache.hadoop.yarn.api.ApplicationConstants;
+import org.apache.hadoop.yarn.api.ApplicationConstants.Environment;
+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
+import org.apache.hadoop.yarn.util.ConverterUtils;
+
+/**
+ * The main() for MapReduce task processes.
+ */
+class YarnChild {
+
+    private static final Log LOG = LogFactory.getLog(YarnChild.class);
+
+    static volatile TaskAttemptID taskid = null;
+
+    public static void main(String[] args) throws Throwable {
+        Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());
+        LOG.debug("Child starting");
+
+        GranularLoggerStore.load();
+
+        final JobConf job = new JobConf(MRJobConfig.JOB_CONF_FILE);
+        // Initing with our JobConf allows us to avoid loading confs twice
+        Limits.init(job);
+        UserGroupInformation.setConfiguration(job);
+
+        String host = args[0];
+        int port = Integer.parseInt(args[1]);
+        final InetSocketAddress address =
+                NetUtils.createSocketAddrForHost(host, port);
+        final TaskAttemptID firstTaskid = TaskAttemptID.forName(args[2]);
+        int jvmIdInt = Integer.parseInt(args[3]);
+        JVMId jvmId = new JVMId(firstTaskid.getJobID(),
+                firstTaskid.getTaskType() == TaskType.MAP, jvmIdInt);
+
+        // initialize metrics
+        DefaultMetricsSystem.initialize(
+                StringUtils.camelize(firstTaskid.getTaskType().name()) +"Task");
+
+        // Security framework already loaded the tokens into current ugi
+        Credentials credentials =
+                UserGroupInformation.getCurrentUser().getCredentials();
+        LOG.info("Executing with tokens:");
+        for (Token<?> token: credentials.getAllTokens()) {
+            LOG.info(token);
+        }
+
+        // Create TaskUmbilicalProtocol as actual task owner.
+        UserGroupInformation taskOwner =
+                UserGroupInformation.createRemoteUser(firstTaskid.getJobID().toString());
+        Token<JobTokenIdentifier> jt = TokenCache.getJobToken(credentials);
+        SecurityUtil.setTokenService(jt, address);
+        taskOwner.addToken(jt);
+        final TaskUmbilicalProtocol umbilical =
+                taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>() {
+                    @Override
+                    public TaskUmbilicalProtocol run() throws Exception {
+                        return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,
+                                TaskUmbilicalProtocol.versionID, address, job);
+                    }
+                });
+
+        // report non-pid to application master
+        JvmContext context = new JvmContext(jvmId, "-1000");
+        LOG.debug("PID: " + System.getenv().get("JVM_PID"));
+        Task task = null;
+        UserGroupInformation childUGI = null;
+        ScheduledExecutorService logSyncer = null;
+
+        try {
+            int idleLoopCount = 0;
+            JvmTask myTask = null;;
+            // poll for new task
+            for (int idle = 0; null == myTask; ++idle) {
+                long sleepTimeMilliSecs = Math.min(idle * 500, 1500);
+                LOG.info("Sleeping for " + sleepTimeMilliSecs
+                        + "ms before retrying again. Got null now.");
+                MILLISECONDS.sleep(sleepTimeMilliSecs);
+                myTask = umbilical.getTask(context);
+            }
+            if (myTask.shouldDie()) {
+                return;
+            }
+
+            task = myTask.getTask();
+            YarnChild.taskid = task.getTaskID();
+
+            // Create the job-conf and set credentials
+            configureTask(job, task, credentials, jt);
+
+            // Initiate Java VM metrics
+            JvmMetrics.initSingleton(jvmId.toString(), job.getSessionId());
+            childUGI = UserGroupInformation.createRemoteUser(System
+                    .getenv(ApplicationConstants.Environment.USER.toString()));
+            // Add tokens to new user so that it may execute its task correctly.
+            childUGI.addCredentials(credentials);
+
+            // set job classloader if configured before invoking the task
+            MRApps.setJobClassLoader(job);
+
+            logSyncer = TaskLog.createLogSyncer();
+
+            // Create a final reference to the task for the doAs block
+            final Task taskFinal = task;
+            childUGI.doAs(new PrivilegedExceptionAction<Object>() {
+                @Override
+                public Object run() throws Exception {
+                    // use job-specified working directory
+                    FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());
+                    taskFinal.run(job, umbilical); // run the task
+                    return null;
+                }
+            });
+        } catch (FSError e) {
+            LOG.fatal("FSError from child", e);
+            umbilical.fsError(taskid, e.getMessage());
+        } catch (Exception exception) {
+            LOG.warn("Exception running child : "
+                    + StringUtils.stringifyException(exception));
+            try {
+                if (task != null) {
+                    // do cleanup for the task
+                    if (childUGI == null) { // no need to job into doAs block
+                        task.taskCleanup(umbilical);
+                    } else {
+                        final Task taskFinal = task;
+                        childUGI.doAs(new PrivilegedExceptionAction<Object>() {
+                            @Override
+                            public Object run() throws Exception {
+                                taskFinal.taskCleanup(umbilical);
+                                return null;
+                            }
+                        });
+                    }
+                }
+            } catch (Exception e) {
+                LOG.info("Exception cleaning up: " + StringUtils.stringifyException(e));
+            }
+            // Report back any failures, for diagnostic purposes
+            if (taskid != null) {
+                umbilical.fatalError(taskid, StringUtils.stringifyException(exception));
+            }
+        } catch (Throwable throwable) {
+            LOG.fatal("Error running child : "
+                    + StringUtils.stringifyException(throwable));
+            if (taskid != null) {
+                Throwable tCause = throwable.getCause();
+                String cause = tCause == null
+                        ? throwable.getMessage()
+                        : StringUtils.stringifyException(tCause);
+                umbilical.fatalError(taskid, cause);
+            }
+        } finally {
+            RPC.stopProxy(umbilical);
+            DefaultMetricsSystem.shutdown();
+            TaskLog.syncLogsShutdown(logSyncer);
+        }
+    }
+
+    /**
+     * Configure mapred-local dirs. This config is used by the task for finding
+     * out an output directory.
+     * @throws IOException
+     */
+    private static void configureLocalDirs(Task task, JobConf job) throws IOException {
+        String[] localSysDirs = StringUtils.getTrimmedStrings(
+                System.getenv(Environment.LOCAL_DIRS.name()));
+        job.setStrings(MRConfig.LOCAL_DIR, localSysDirs);
+        LOG.info(MRConfig.LOCAL_DIR + " for child: " + job.get(MRConfig.LOCAL_DIR));
+        LocalDirAllocator lDirAlloc = new LocalDirAllocator(MRConfig.LOCAL_DIR);
+        Path workDir = null;
+        // First, try to find the JOB_LOCAL_DIR on this host.
+        try {
+            workDir = lDirAlloc.getLocalPathToRead("work", job);
+        } catch (DiskErrorException e) {
+            // DiskErrorException means dir not found. If not found, it will
+            // be created below.
+        }
+        if (workDir == null) {
+            // JOB_LOCAL_DIR doesn't exist on this host -- Create it.
+            workDir = lDirAlloc.getLocalPathForWrite("work", job);
+            FileSystem lfs = FileSystem.getLocal(job).getRaw();
+            boolean madeDir = false;
+            try {
+                madeDir = lfs.mkdirs(workDir);
+            } catch (FileAlreadyExistsException e) {
+                // Since all tasks will be running in their own JVM, the race condition
+                // exists where multiple tasks could be trying to create this directory
+                // at the same time. If this task loses the race, it's okay because
+                // the directory already exists.
+                madeDir = true;
+                workDir = lDirAlloc.getLocalPathToRead("work", job);
+            }
+            if (!madeDir) {
+                throw new IOException("Mkdirs failed to create "
+                        + workDir.toString());
+            }
+        }
+        job.set(MRJobConfig.JOB_LOCAL_DIR,workDir.toString());
+    }
+
+    private static void configureTask(JobConf job, Task task,
+                                      Credentials credentials, Token<JobTokenIdentifier> jt) throws IOException {
+        job.setCredentials(credentials);
+
+        ApplicationAttemptId appAttemptId =
+                ConverterUtils.toContainerId(
+                        System.getenv(Environment.CONTAINER_ID.name()))
+                        .getApplicationAttemptId();
+        LOG.debug("APPLICATION_ATTEMPT_ID: " + appAttemptId);
+        // Set it in conf, so as to be able to be used the the OutputCommitter.
+        job.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,
+                appAttemptId.getAttemptId());
+
+        // set tcp nodelay
+        job.setBoolean("ipc.client.tcpnodelay", true);
+        job.setClass(MRConfig.TASK_LOCAL_OUTPUT_CLASS,
+                YarnOutputFiles.class, MapOutputFile.class);
+        // set the jobToken and shuffle secrets into task
+        task.setJobTokenSecret(
+                JobTokenSecretManager.createSecretKey(jt.getPassword()));
+        byte[] shuffleSecret = TokenCache.getShuffleSecretKey(credentials);
+        if (shuffleSecret == null) {
+            LOG.warn("Shuffle secret missing from task credentials."
+                    + " Using job token secret as shuffle secret.");
+            shuffleSecret = jt.getPassword();
+        }
+        task.setShuffleSecret(
+                JobTokenSecretManager.createSecretKey(shuffleSecret));
+
+        // setup the child's MRConfig.LOCAL_DIR.
+        configureLocalDirs(task, job);
+
+        // setup the child's attempt directories
+        // Do the task-type specific localization
+        task.localizeConfiguration(job);
+
+        // Set up the DistributedCache related configs
+        setupDistributedCacheConfig(job);
+
+        // Overwrite the localized task jobconf which is linked to in the current
+        // work-dir.
+        Path localTaskFile = new Path(MRJobConfig.JOB_CONF_FILE);
+        writeLocalJobFile(localTaskFile, job);
+        task.setJobFile(localTaskFile.toString());
+        task.setConf(job);
+    }
+
+    /**
+     * Set up the DistributedCache related configs to make
+     * {@link DistributedCache#getLocalCacheFiles(Configuration)}
+     * and
+     * {@link DistributedCache#getLocalCacheArchives(Configuration)}
+     * working.
+     * @param job
+     * @throws IOException
+     */
+    private static void setupDistributedCacheConfig(final JobConf job)
+            throws IOException {
+
+        String localWorkDir = System.getenv("PWD");
+        //        ^ ^ all symlinks are created in the current work-dir
+
+        // Update the configuration object with localized archives.
+        URI[] cacheArchives = DistributedCache.getCacheArchives(job);
+        if (cacheArchives != null) {
+            List<String> localArchives = new ArrayList<String>();
+            for (int i = 0; i < cacheArchives.length; ++i) {
+                URI u = cacheArchives[i];
+                Path p = new Path(u);
+                Path name =
+                        new Path((null == u.getFragment()) ? p.getName()
+                                : u.getFragment());
+                String linkName = name.toUri().getPath();
+                localArchives.add(new Path(localWorkDir, linkName).toUri().getPath());
+            }
+            if (!localArchives.isEmpty()) {
+                job.set(MRJobConfig.CACHE_LOCALARCHIVES, StringUtils
+                        .arrayToString(localArchives.toArray(new String[localArchives
+                                .size()])));
+            }
+        }
+
+        // Update the configuration object with localized files.
+        URI[] cacheFiles = DistributedCache.getCacheFiles(job);
+        if (cacheFiles != null) {
+            List<String> localFiles = new ArrayList<String>();
+            for (int i = 0; i < cacheFiles.length; ++i) {
+                URI u = cacheFiles[i];
+                Path p = new Path(u);
+                Path name =
+                        new Path((null == u.getFragment()) ? p.getName()
+                                : u.getFragment());
+                String linkName = name.toUri().getPath();
+                localFiles.add(new Path(localWorkDir, linkName).toUri().getPath());
+            }
+            if (!localFiles.isEmpty()) {
+                job.set(MRJobConfig.CACHE_LOCALFILES,
+                        StringUtils.arrayToString(localFiles
+                                .toArray(new String[localFiles.size()])));
+            }
+        }
+    }
+
+    private static final FsPermission urw_gr =
+            FsPermission.createImmutable((short) 0640);
+
+    /**
+     * Write the task specific job-configuration file.
+     * @throws IOException
+     */
+    private static void writeLocalJobFile(Path jobFile, JobConf conf)
+            throws IOException {
+        FileSystem localFs = FileSystem.getLocal(conf);
+        localFs.delete(jobFile);
+        OutputStream out = null;
+        try {
+            out = FileSystem.create(localFs, jobFile, urw_gr);
+            conf.writeXml(out);
+        } finally {
+            IOUtils.cleanup(LOG, out);
+        }
+    }
+
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/Job.java b/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/Job.java
new file mode 100644
index 0000000..ff142e1
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/Job.java
@@ -0,0 +1,1530 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce;
+
+import java.io.IOException;
+import java.net.URI;
+import java.security.PrivilegedExceptionAction;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.classification.InterfaceAudience.Private;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configuration.IntegerRanges;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.granular.MapReduceV2Term;
+import org.apache.hadoop.granular.GranularLoggerStore;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.filecache.DistributedCache;
+import org.apache.hadoop.mapreduce.protocol.ClientProtocol;
+import org.apache.hadoop.mapreduce.task.JobContextImpl;
+import org.apache.hadoop.mapreduce.util.ConfigUtil;
+import org.apache.hadoop.util.StringUtils;
+
+/**
+ * The job submitter's view of the Job.
+ *
+ * <p>It allows the user to configure the
+ * job, submit it, control its execution, and query the state. The set methods
+ * only work until the job is submitted, afterwards they will throw an 
+ * IllegalStateException. </p>
+ *
+ * <p>
+ * Normally the user creates the application, describes various facets of the
+ * job via {@link Job} and then submits the job and monitor its progress.</p>
+ *
+ * <p>Here is an example on how to submit a job:</p>
+ * <p><blockquote><pre>
+ *     // Create a new Job
+ *     Job job = new Job(new Configuration());
+ *     job.setJarByClass(MyJob.class);
+ *
+ *     // Specify various job-specific parameters     
+ *     job.setJobName("myjob");
+ *
+ *     job.setInputPath(new Path("in"));
+ *     job.setOutputPath(new Path("out"));
+ *
+ *     job.setMapperClass(MyJob.MyMapper.class);
+ *     job.setReducerClass(MyJob.MyReducer.class);
+ *
+ *     // Submit the job, then poll for progress until the job is complete
+ *     job.waitForCompletion(true);
+ * </pre></blockquote></p>
+ *
+ *
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public class Job extends JobContextImpl implements JobContext {
+    private static final Log LOG = LogFactory.getLog(Job.class);
+
+    @InterfaceStability.Evolving
+    public static enum JobState {DEFINE, RUNNING};
+    private static final long MAX_JOBSTATUS_AGE = 1000 * 2;
+    public static final String OUTPUT_FILTER = "mapreduce.client.output.filter";
+    /** Key in mapred-*.xml that sets completionPollInvervalMillis */
+    public static final String COMPLETION_POLL_INTERVAL_KEY =
+            "mapreduce.client.completion.pollinterval";
+
+    /** Default completionPollIntervalMillis is 5000 ms. */
+    static final int DEFAULT_COMPLETION_POLL_INTERVAL = 5000;
+    /** Key in mapred-*.xml that sets progMonitorPollIntervalMillis */
+    public static final String PROGRESS_MONITOR_POLL_INTERVAL_KEY =
+            "mapreduce.client.progressmonitor.pollinterval";
+    /** Default progMonitorPollIntervalMillis is 1000 ms. */
+    static final int DEFAULT_MONITOR_POLL_INTERVAL = 1000;
+
+    public static final String USED_GENERIC_PARSER =
+            "mapreduce.client.genericoptionsparser.used";
+    public static final String SUBMIT_REPLICATION =
+            "mapreduce.client.submit.file.replication";
+    private static final String TASKLOG_PULL_TIMEOUT_KEY =
+            "mapreduce.client.tasklog.timeout";
+    private static final int DEFAULT_TASKLOG_TIMEOUT = 60000;
+
+    @InterfaceStability.Evolving
+    public static enum TaskStatusFilter { NONE, KILLED, FAILED, SUCCEEDED, ALL }
+
+    static {
+        ConfigUtil.loadResources();
+    }
+
+    private JobState state = JobState.DEFINE;
+    private JobStatus status;
+    private long statustime;
+    private Cluster cluster;
+
+    @Deprecated
+    public Job() throws IOException {
+        this(new Configuration());
+    }
+
+    @Deprecated
+    public Job(Configuration conf) throws IOException {
+        this(new JobConf(conf));
+    }
+
+    @Deprecated
+    public Job(Configuration conf, String jobName) throws IOException {
+        this(conf);
+        setJobName(jobName);
+    }
+
+    Job(JobConf conf) throws IOException {
+        super(conf, null);
+        // propagate existing user credentials to job
+        this.credentials.mergeAll(this.ugi.getCredentials());
+        this.cluster = null;
+    }
+
+    Job(JobStatus status, JobConf conf) throws IOException {
+        this(conf);
+        setJobID(status.getJobID());
+        this.status = status;
+        state = JobState.RUNNING;
+    }
+
+
+    /**
+     * Creates a new {@link Job} with no particular {@link Cluster} .
+     * A Cluster will be created with a generic {@link Configuration}.
+     *
+     * @return the {@link Job} , with no connection to a cluster yet.
+     * @throws IOException
+     */
+    public static Job getInstance() throws IOException {
+        // create with a null Cluster
+        return getInstance(new Configuration());
+    }
+
+    /**
+     * Creates a new {@link Job} with no particular {@link Cluster} and a
+     * given {@link Configuration}.
+     *
+     * The <code>Job</code> makes a copy of the <code>Configuration</code> so
+     * that any necessary internal modifications do not reflect on the incoming
+     * parameter.
+     *
+     * A Cluster will be created from the conf parameter only when it's needed.
+     *
+     * @param conf the configuration
+     * @return the {@link Job} , with no connection to a cluster yet.
+     * @throws IOException
+     */
+    public static Job getInstance(Configuration conf) throws IOException {
+        // create with a null Cluster
+        JobConf jobConf = new JobConf(conf);
+        return new Job(jobConf);
+    }
+
+
+    /**
+     * Creates a new {@link Job} with no particular {@link Cluster} and a given jobName.
+     * A Cluster will be created from the conf parameter only when it's needed.
+     *
+     * The <code>Job</code> makes a copy of the <code>Configuration</code> so
+     * that any necessary internal modifications do not reflect on the incoming
+     * parameter.
+     *
+     * @param conf the configuration
+     * @return the {@link Job} , with no connection to a cluster yet.
+     * @throws IOException
+     */
+    public static Job getInstance(Configuration conf, String jobName)
+            throws IOException {
+        // create with a null Cluster
+        Job result = getInstance(conf);
+        result.setJobName(jobName);
+        return result;
+    }
+
+    /**
+     * Creates a new {@link Job} with no particular {@link Cluster} and given
+     * {@link Configuration} and {@link JobStatus}.
+     * A Cluster will be created from the conf parameter only when it's needed.
+     *
+     * The <code>Job</code> makes a copy of the <code>Configuration</code> so
+     * that any necessary internal modifications do not reflect on the incoming
+     * parameter.
+     *
+     * @param status job status
+     * @param conf job configuration
+     * @return the {@link Job} , with no connection to a cluster yet.
+     * @throws IOException
+     */
+    public static Job getInstance(JobStatus status, Configuration conf)
+            throws IOException {
+        return new Job(status, new JobConf(conf));
+    }
+
+    /**
+     * Creates a new {@link Job} with no particular {@link Cluster}.
+     * A Cluster will be created from the conf parameter only when it's needed.
+     *
+     * The <code>Job</code> makes a copy of the <code>Configuration</code> so
+     * that any necessary internal modifications do not reflect on the incoming
+     * parameter.
+     *
+     * @param ignored
+     * @return the {@link Job} , with no connection to a cluster yet.
+     * @throws IOException
+     * @deprecated Use {@link #getInstance()}
+     */
+    @Deprecated
+    public static Job getInstance(Cluster ignored) throws IOException {
+        return getInstance();
+    }
+
+    /**
+     * Creates a new {@link Job} with no particular {@link Cluster} and given
+     * {@link Configuration}.
+     * A Cluster will be created from the conf parameter only when it's needed.
+     *
+     * The <code>Job</code> makes a copy of the <code>Configuration</code> so
+     * that any necessary internal modifications do not reflect on the incoming
+     * parameter.
+     *
+     * @param ignored
+     * @param conf job configuration
+     * @return the {@link Job} , with no connection to a cluster yet.
+     * @throws IOException
+     * @deprecated Use {@link #getInstance(Configuration)}
+     */
+    @Deprecated
+    public static Job getInstance(Cluster ignored, Configuration conf)
+            throws IOException {
+        return getInstance(conf);
+    }
+
+    /**
+     * Creates a new {@link Job} with no particular {@link Cluster} and given
+     * {@link Configuration} and {@link JobStatus}.
+     * A Cluster will be created from the conf parameter only when it's needed.
+     *
+     * The <code>Job</code> makes a copy of the <code>Configuration</code> so
+     * that any necessary internal modifications do not reflect on the incoming
+     * parameter.
+     *
+     * @param cluster cluster
+     * @param status job status
+     * @param conf job configuration
+     * @return the {@link Job} , with no connection to a cluster yet.
+     * @throws IOException
+     */
+    @Private
+    public static Job getInstance(Cluster cluster, JobStatus status,
+                                  Configuration conf) throws IOException {
+        Job job = getInstance(status, conf);
+        job.setCluster(cluster);
+        return job;
+    }
+
+    private void ensureState(JobState state) throws IllegalStateException {
+        if (state != this.state) {
+            throw new IllegalStateException("Job in state "+ this.state +
+                    " instead of " + state);
+        }
+
+        if (state == JobState.RUNNING && cluster == null) {
+            throw new IllegalStateException
+                    ("Job in state " + this.state
+                            + ", but it isn't attached to any job tracker!");
+        }
+    }
+
+    /**
+     * Some methods rely on having a recent job status object.  Refresh
+     * it, if necessary
+     */
+    synchronized void ensureFreshStatus()
+            throws IOException {
+        if (System.currentTimeMillis() - statustime > MAX_JOBSTATUS_AGE) {
+            updateStatus();
+        }
+    }
+
+    /** Some methods need to update status immediately. So, refresh
+     * immediately
+     * @throws IOException
+     */
+    synchronized void updateStatus() throws IOException {
+        try {
+            this.status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {
+                @Override
+                public JobStatus run() throws IOException, InterruptedException {
+                    return cluster.getClient().getJobStatus(status.getJobID());
+                }
+            });
+        }
+        catch (InterruptedException ie) {
+            throw new IOException(ie);
+        }
+        if (this.status == null) {
+            throw new IOException("Job status not available ");
+        }
+        this.statustime = System.currentTimeMillis();
+    }
+
+    public JobStatus getStatus() throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status;
+    }
+
+    private void setStatus(JobStatus status) {
+        this.status = status;
+    }
+
+    /**
+     * Returns the current state of the Job.
+     *
+     * @return JobStatus#State
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public JobStatus.State getJobState()
+            throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.getState();
+    }
+
+    /**
+     * Get the URL where some job progress information will be displayed.
+     *
+     * @return the URL where some job progress information will be displayed.
+     */
+    public String getTrackingURL(){
+        ensureState(JobState.RUNNING);
+        return status.getTrackingUrl().toString();
+    }
+
+    /**
+     * Get the path of the submitted job configuration.
+     *
+     * @return the path of the submitted job configuration.
+     */
+    public String getJobFile() {
+        ensureState(JobState.RUNNING);
+        return status.getJobFile();
+    }
+
+    /**
+     * Get start time of the job.
+     *
+     * @return the start time of the job
+     */
+    public long getStartTime() {
+        ensureState(JobState.RUNNING);
+        return status.getStartTime();
+    }
+
+    /**
+     * Get finish time of the job.
+     *
+     * @return the finish time of the job
+     */
+    public long getFinishTime() throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.getFinishTime();
+    }
+
+    /**
+     * Get scheduling info of the job.
+     *
+     * @return the scheduling info of the job
+     */
+    public String getSchedulingInfo() {
+        ensureState(JobState.RUNNING);
+        return status.getSchedulingInfo();
+    }
+
+    /**
+     * Get scheduling info of the job.
+     *
+     * @return the scheduling info of the job
+     */
+    public JobPriority getPriority() throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.getPriority();
+    }
+
+    /**
+     * The user-specified job name.
+     */
+    public String getJobName() {
+        if (state == JobState.DEFINE) {
+            return super.getJobName();
+        }
+        ensureState(JobState.RUNNING);
+        return status.getJobName();
+    }
+
+    public String getHistoryUrl() throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.getHistoryFile();
+    }
+
+    public boolean isRetired() throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.isRetired();
+    }
+
+    @Private
+    public Cluster getCluster() {
+        return cluster;
+    }
+
+    /** Only for mocks in unit tests. */
+    @Private
+    private void setCluster(Cluster cluster) {
+        this.cluster = cluster;
+    }
+
+    /**
+     * Dump stats to screen.
+     */
+    @Override
+    public String toString() {
+        ensureState(JobState.RUNNING);
+        String reasonforFailure = " ";
+        int numMaps = 0;
+        int numReduces = 0;
+        try {
+            updateStatus();
+            if (status.getState().equals(JobStatus.State.FAILED))
+                reasonforFailure = getTaskFailureEventString();
+            numMaps = getTaskReports(TaskType.MAP).length;
+            numReduces = getTaskReports(TaskType.REDUCE).length;
+        } catch (IOException e) {
+        } catch (InterruptedException ie) {
+        }
+        StringBuffer sb = new StringBuffer();
+        sb.append("Job: ").append(status.getJobID()).append("\n");
+        sb.append("Job File: ").append(status.getJobFile()).append("\n");
+        sb.append("Job Tracking URL : ").append(status.getTrackingUrl());
+        sb.append("\n");
+        sb.append("Uber job : ").append(status.isUber()).append("\n");
+        sb.append("Number of maps: ").append(numMaps).append("\n");
+        sb.append("Number of reduces: ").append(numReduces).append("\n");
+        sb.append("map() completion: ");
+        sb.append(status.getMapProgress()).append("\n");
+        sb.append("reduce() completion: ");
+        sb.append(status.getReduceProgress()).append("\n");
+        sb.append("Job state: ");
+        sb.append(status.getState()).append("\n");
+        sb.append("retired: ").append(status.isRetired()).append("\n");
+        sb.append("reason for failure: ").append(reasonforFailure);
+        return sb.toString();
+    }
+
+    /**
+     * @return taskid which caused job failure
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    String getTaskFailureEventString() throws IOException,
+            InterruptedException {
+        int failCount = 1;
+        TaskCompletionEvent lastEvent = null;
+        TaskCompletionEvent[] events = ugi.doAs(new
+                                                        PrivilegedExceptionAction<TaskCompletionEvent[]>() {
+                                                            @Override
+                                                            public TaskCompletionEvent[] run() throws IOException,
+                                                                    InterruptedException {
+                                                                return cluster.getClient().getTaskCompletionEvents(
+                                                                        status.getJobID(), 0, 10);
+                                                            }
+                                                        });
+        for (TaskCompletionEvent event : events) {
+            if (event.getStatus().equals(TaskCompletionEvent.Status.FAILED)) {
+                failCount++;
+                lastEvent = event;
+            }
+        }
+        if (lastEvent == null) {
+            return "There are no failed tasks for the job. "
+                    + "Job is failed due to some other reason and reason "
+                    + "can be found in the logs.";
+        }
+        String[] taskAttemptID = lastEvent.getTaskAttemptId().toString().split("_", 2);
+        String taskID = taskAttemptID[1].substring(0, taskAttemptID[1].length()-2);
+        return (" task " + taskID + " failed " +
+                failCount + " times " + "For details check tasktracker at: " +
+                lastEvent.getTaskTrackerHttp());
+    }
+
+    /**
+     * Get the information of the current state of the tasks of a job.
+     *
+     * @param type Type of the task
+     * @return the list of all of the map tips.
+     * @throws IOException
+     */
+    public TaskReport[] getTaskReports(TaskType type)
+            throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        final TaskType tmpType = type;
+        return ugi.doAs(new PrivilegedExceptionAction<TaskReport[]>() {
+            public TaskReport[] run() throws IOException, InterruptedException {
+                return cluster.getClient().getTaskReports(getJobID(), tmpType);
+            }
+        });
+    }
+
+    /**
+     * Get the <i>progress</i> of the job's map-tasks, as a float between 0.0
+     * and 1.0.  When all map tasks have completed, the function returns 1.0.
+     *
+     * @return the progress of the job's map-tasks.
+     * @throws IOException
+     */
+    public float mapProgress() throws IOException {
+        ensureState(JobState.RUNNING);
+        ensureFreshStatus();
+        return status.getMapProgress();
+    }
+
+    /**
+     * Get the <i>progress</i> of the job's reduce-tasks, as a float between 0.0
+     * and 1.0.  When all reduce tasks have completed, the function returns 1.0.
+     *
+     * @return the progress of the job's reduce-tasks.
+     * @throws IOException
+     */
+    public float reduceProgress() throws IOException {
+        ensureState(JobState.RUNNING);
+        ensureFreshStatus();
+        return status.getReduceProgress();
+    }
+
+    /**
+     * Get the <i>progress</i> of the job's cleanup-tasks, as a float between 0.0
+     * and 1.0.  When all cleanup tasks have completed, the function returns 1.0.
+     *
+     * @return the progress of the job's cleanup-tasks.
+     * @throws IOException
+     */
+    public float cleanupProgress() throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        ensureFreshStatus();
+        return status.getCleanupProgress();
+    }
+
+    /**
+     * Get the <i>progress</i> of the job's setup-tasks, as a float between 0.0
+     * and 1.0.  When all setup tasks have completed, the function returns 1.0.
+     *
+     * @return the progress of the job's setup-tasks.
+     * @throws IOException
+     */
+    public float setupProgress() throws IOException {
+        ensureState(JobState.RUNNING);
+        ensureFreshStatus();
+        return status.getSetupProgress();
+    }
+
+    /**
+     * Check if the job is finished or not.
+     * This is a non-blocking call.
+     *
+     * @return <code>true</code> if the job is complete, else <code>false</code>.
+     * @throws IOException
+     */
+    public boolean isComplete() throws IOException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.isJobComplete();
+    }
+
+    /**
+     * Check if the job completed successfully.
+     *
+     * @return <code>true</code> if the job succeeded, else <code>false</code>.
+     * @throws IOException
+     */
+    public boolean isSuccessful() throws IOException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.getState() == JobStatus.State.SUCCEEDED;
+    }
+
+    /**
+     * Kill the running job.  Blocks until all job tasks have been
+     * killed as well.  If the job is no longer running, it simply returns.
+     *
+     * @throws IOException
+     */
+    public void killJob() throws IOException {
+        ensureState(JobState.RUNNING);
+        try {
+            cluster.getClient().killJob(getJobID());
+        }
+        catch (InterruptedException ie) {
+            throw new IOException(ie);
+        }
+    }
+
+    /**
+     * Set the priority of a running job.
+     * @param priority the new priority for the job.
+     * @throws IOException
+     */
+    public void setPriority(JobPriority priority)
+            throws IOException, InterruptedException {
+        if (state == JobState.DEFINE) {
+            conf.setJobPriority(
+                    org.apache.hadoop.mapred.JobPriority.valueOf(priority.name()));
+        } else {
+            ensureState(JobState.RUNNING);
+            final JobPriority tmpPriority = priority;
+            ugi.doAs(new PrivilegedExceptionAction<Object>() {
+                @Override
+                public Object run() throws IOException, InterruptedException {
+                    cluster.getClient().setJobPriority(getJobID(), tmpPriority.toString());
+                    return null;
+                }
+            });
+        }
+    }
+
+    /**
+     * Get events indicating completion (success/failure) of component tasks.
+     *
+     * @param startFrom index to start fetching events from
+     * @param numEvents number of events to fetch
+     * @return an array of {@link TaskCompletionEvent}s
+     * @throws IOException
+     */
+    public TaskCompletionEvent[] getTaskCompletionEvents(final int startFrom,
+                                                         final int numEvents) throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        return ugi.doAs(new PrivilegedExceptionAction<TaskCompletionEvent[]>() {
+            @Override
+            public TaskCompletionEvent[] run() throws IOException, InterruptedException {
+                return cluster.getClient().getTaskCompletionEvents(getJobID(),
+                        startFrom, numEvents);
+            }
+        });
+    }
+
+    /**
+     * Get events indicating completion (success/failure) of component tasks.
+     *
+     * @param startFrom index to start fetching events from
+     * @return an array of {@link org.apache.hadoop.mapred.TaskCompletionEvent}s
+     * @throws IOException
+     */
+    public org.apache.hadoop.mapred.TaskCompletionEvent[]
+    getTaskCompletionEvents(final int startFrom) throws IOException {
+        try {
+            TaskCompletionEvent[] events = getTaskCompletionEvents(startFrom, 10);
+            org.apache.hadoop.mapred.TaskCompletionEvent[] retEvents =
+                    new org.apache.hadoop.mapred.TaskCompletionEvent[events.length];
+            for (int i = 0; i < events.length; i++) {
+                retEvents[i] = org.apache.hadoop.mapred.TaskCompletionEvent.downgrade
+                        (events[i]);
+            }
+            return retEvents;
+        } catch (InterruptedException ie) {
+            throw new IOException(ie);
+        }
+    }
+
+    /**
+     * Kill indicated task attempt.
+     * @param taskId the id of the task to kill.
+     * @param shouldFail if <code>true</code> the task is failed and added
+     *                   to failed tasks list, otherwise it is just killed,
+     *                   w/o affecting job failure status.
+     */
+    @Private
+    public boolean killTask(final TaskAttemptID taskId,
+                            final boolean shouldFail) throws IOException {
+        ensureState(JobState.RUNNING);
+        try {
+            return ugi.doAs(new PrivilegedExceptionAction<Boolean>() {
+                public Boolean run() throws IOException, InterruptedException {
+                    return cluster.getClient().killTask(taskId, shouldFail);
+                }
+            });
+        }
+        catch (InterruptedException ie) {
+            throw new IOException(ie);
+        }
+    }
+
+    /**
+     * Kill indicated task attempt.
+     *
+     * @param taskId the id of the task to be terminated.
+     * @throws IOException
+     */
+    public void killTask(final TaskAttemptID taskId)
+            throws IOException {
+        killTask(taskId, false);
+    }
+
+    /**
+     * Fail indicated task attempt.
+     *
+     * @param taskId the id of the task to be terminated.
+     * @throws IOException
+     */
+    public void failTask(final TaskAttemptID taskId)
+            throws IOException {
+        killTask(taskId, true);
+    }
+
+    /**
+     * Gets the counters for this job. May return null if the job has been
+     * retired and the job is no longer in the completed job store.
+     *
+     * @return the counters for this job.
+     * @throws IOException
+     */
+    public Counters getCounters()
+            throws IOException {
+        ensureState(JobState.RUNNING);
+        try {
+            return ugi.doAs(new PrivilegedExceptionAction<Counters>() {
+                @Override
+                public Counters run() throws IOException, InterruptedException {
+                    return cluster.getClient().getJobCounters(getJobID());
+                }
+            });
+        }
+        catch (InterruptedException ie) {
+            throw new IOException(ie);
+        }
+    }
+
+    /**
+     * Gets the diagnostic messages for a given task attempt.
+     * @param taskid
+     * @return the list of diagnostic messages for the task
+     * @throws IOException
+     */
+    public String[] getTaskDiagnostics(final TaskAttemptID taskid)
+            throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        return ugi.doAs(new PrivilegedExceptionAction<String[]>() {
+            @Override
+            public String[] run() throws IOException, InterruptedException {
+                return cluster.getClient().getTaskDiagnostics(taskid);
+            }
+        });
+    }
+
+    /**
+     * Set the number of reduce tasks for the job.
+     * @param tasks the number of reduce tasks
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setNumReduceTasks(int tasks) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setNumReduceTasks(tasks);
+    }
+
+    /**
+     * Set the current working directory for the default file system.
+     *
+     * @param dir the new current working directory.
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setWorkingDirectory(Path dir) throws IOException {
+        ensureState(JobState.DEFINE);
+        conf.setWorkingDirectory(dir);
+    }
+
+    /**
+     * Set the {@link InputFormat} for the job.
+     * @param cls the <code>InputFormat</code> to use
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setInputFormatClass(Class<? extends InputFormat> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setClass(INPUT_FORMAT_CLASS_ATTR, cls,
+                InputFormat.class);
+    }
+
+    /**
+     * Set the {@link OutputFormat} for the job.
+     * @param cls the <code>OutputFormat</code> to use
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setOutputFormatClass(Class<? extends OutputFormat> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setClass(OUTPUT_FORMAT_CLASS_ATTR, cls,
+                OutputFormat.class);
+    }
+
+    /**
+     * Set the {@link Mapper} for the job.
+     * @param cls the <code>Mapper</code> to use
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setMapperClass(Class<? extends Mapper> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setClass(MAP_CLASS_ATTR, cls, Mapper.class);
+    }
+
+    /**
+     * Set the Jar by finding where a given class came from.
+     * @param cls the example class
+     */
+    public void setJarByClass(Class<?> cls) {
+        ensureState(JobState.DEFINE);
+        conf.setJarByClass(cls);
+    }
+
+    /**
+     * Set the job jar
+     */
+    public void setJar(String jar) {
+        ensureState(JobState.DEFINE);
+        conf.setJar(jar);
+    }
+
+    /**
+     * Set the reported username for this job.
+     *
+     * @param user the username for this job.
+     */
+    public void setUser(String user) {
+        ensureState(JobState.DEFINE);
+        conf.setUser(user);
+    }
+
+    /**
+     * Set the combiner class for the job.
+     * @param cls the combiner to use
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setCombinerClass(Class<? extends Reducer> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setClass(COMBINE_CLASS_ATTR, cls, Reducer.class);
+    }
+
+    /**
+     * Set the {@link Reducer} for the job.
+     * @param cls the <code>Reducer</code> to use
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setReducerClass(Class<? extends Reducer> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setClass(REDUCE_CLASS_ATTR, cls, Reducer.class);
+    }
+
+    /**
+     * Set the {@link Partitioner} for the job.
+     * @param cls the <code>Partitioner</code> to use
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setPartitionerClass(Class<? extends Partitioner> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setClass(PARTITIONER_CLASS_ATTR, cls,
+                Partitioner.class);
+    }
+
+    /**
+     * Set the key class for the map output data. This allows the user to
+     * specify the map output key class to be different than the final output
+     * value class.
+     *
+     * @param theClass the map output key class.
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setMapOutputKeyClass(Class<?> theClass
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setMapOutputKeyClass(theClass);
+    }
+
+    /**
+     * Set the value class for the map output data. This allows the user to
+     * specify the map output value class to be different than the final output
+     * value class.
+     *
+     * @param theClass the map output value class.
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setMapOutputValueClass(Class<?> theClass
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setMapOutputValueClass(theClass);
+    }
+
+    /**
+     * Set the key class for the job output data.
+     *
+     * @param theClass the key class for the job output data.
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setOutputKeyClass(Class<?> theClass
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setOutputKeyClass(theClass);
+    }
+
+    /**
+     * Set the value class for job outputs.
+     *
+     * @param theClass the value class for job outputs.
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setOutputValueClass(Class<?> theClass
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setOutputValueClass(theClass);
+    }
+
+    /**
+     * Define the comparator that controls which keys are grouped together
+     * for a single call to combiner,
+     * {@link Reducer#reduce(Object, Iterable,
+     * org.apache.hadoop.mapreduce.Reducer.Context)}
+     *
+     * @param cls the raw comparator to use
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setCombinerKeyGroupingComparatorClass(
+            Class<? extends RawComparator> cls) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setCombinerKeyGroupingComparator(cls);
+    }
+
+    /**
+     * Define the comparator that controls how the keys are sorted before they
+     * are passed to the {@link Reducer}.
+     * @param cls the raw comparator
+     * @throws IllegalStateException if the job is submitted
+     * @see #setCombinerKeyGroupingComparatorClass(Class)
+     */
+    public void setSortComparatorClass(Class<? extends RawComparator> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setOutputKeyComparatorClass(cls);
+    }
+
+    /**
+     * Define the comparator that controls which keys are grouped together
+     * for a single call to
+     * {@link Reducer#reduce(Object, Iterable,
+     *                       org.apache.hadoop.mapreduce.Reducer.Context)}
+     * @param cls the raw comparator to use
+     * @throws IllegalStateException if the job is submitted
+     * @see #setCombinerKeyGroupingComparatorClass(Class)
+     */
+    public void setGroupingComparatorClass(Class<? extends RawComparator> cls
+    ) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setOutputValueGroupingComparator(cls);
+    }
+
+    /**
+     * Set the user-specified job name.
+     *
+     * @param name the job's new name.
+     * @throws IllegalStateException if the job is submitted
+     */
+    public void setJobName(String name) throws IllegalStateException {
+        ensureState(JobState.DEFINE);
+        conf.setJobName(name);
+    }
+
+    /**
+     * Turn speculative execution on or off for this job.
+     *
+     * @param speculativeExecution <code>true</code> if speculative execution
+     *                             should be turned on, else <code>false</code>.
+     */
+    public void setSpeculativeExecution(boolean speculativeExecution) {
+        ensureState(JobState.DEFINE);
+        conf.setSpeculativeExecution(speculativeExecution);
+    }
+
+    /**
+     * Turn speculative execution on or off for this job for map tasks.
+     *
+     * @param speculativeExecution <code>true</code> if speculative execution
+     *                             should be turned on for map tasks,
+     *                             else <code>false</code>.
+     */
+    public void setMapSpeculativeExecution(boolean speculativeExecution) {
+        ensureState(JobState.DEFINE);
+        conf.setMapSpeculativeExecution(speculativeExecution);
+    }
+
+    /**
+     * Turn speculative execution on or off for this job for reduce tasks.
+     *
+     * @param speculativeExecution <code>true</code> if speculative execution
+     *                             should be turned on for reduce tasks,
+     *                             else <code>false</code>.
+     */
+    public void setReduceSpeculativeExecution(boolean speculativeExecution) {
+        ensureState(JobState.DEFINE);
+        conf.setReduceSpeculativeExecution(speculativeExecution);
+    }
+
+    /**
+     * Specify whether job-setup and job-cleanup is needed for the job
+     *
+     * @param needed If <code>true</code>, job-setup and job-cleanup will be
+     *               considered from {@link OutputCommitter}
+     *               else ignored.
+     */
+    public void setJobSetupCleanupNeeded(boolean needed) {
+        ensureState(JobState.DEFINE);
+        conf.setBoolean(SETUP_CLEANUP_NEEDED, needed);
+    }
+
+    /**
+     * Set the given set of archives
+     * @param archives The list of archives that need to be localized
+     */
+    public void setCacheArchives(URI[] archives) {
+        ensureState(JobState.DEFINE);
+        DistributedCache.setCacheArchives(archives, conf);
+    }
+
+    /**
+     * Set the given set of files
+     * @param files The list of files that need to be localized
+     */
+    public void setCacheFiles(URI[] files) {
+        ensureState(JobState.DEFINE);
+        DistributedCache.setCacheFiles(files, conf);
+    }
+
+    /**
+     * Add a archives to be localized
+     * @param uri The uri of the cache to be localized
+     */
+    public void addCacheArchive(URI uri) {
+        ensureState(JobState.DEFINE);
+        DistributedCache.addCacheArchive(uri, conf);
+    }
+
+    /**
+     * Add a file to be localized
+     * @param uri The uri of the cache to be localized
+     */
+    public void addCacheFile(URI uri) {
+        ensureState(JobState.DEFINE);
+        DistributedCache.addCacheFile(uri, conf);
+    }
+
+    /**
+     * Add an file path to the current set of classpath entries It adds the file
+     * to cache as well.
+     *
+     * Files added with this method will not be unpacked while being added to the
+     * classpath.
+     * To add archives to classpath, use the {@link #addArchiveToClassPath(Path)}
+     * method instead.
+     *
+     * @param file Path of the file to be added
+     */
+    public void addFileToClassPath(Path file)
+            throws IOException {
+        ensureState(JobState.DEFINE);
+        DistributedCache.addFileToClassPath(file, conf, file.getFileSystem(conf));
+    }
+
+    /**
+     * Add an archive path to the current set of classpath entries. It adds the
+     * archive to cache as well.
+     *
+     * Archive files will be unpacked and added to the classpath
+     * when being distributed.
+     *
+     * @param archive Path of the archive to be added
+     */
+    public void addArchiveToClassPath(Path archive)
+            throws IOException {
+        ensureState(JobState.DEFINE);
+        DistributedCache.addArchiveToClassPath(archive, conf, archive.getFileSystem(conf));
+    }
+
+    /**
+     * Originally intended to enable symlinks, but currently symlinks cannot be
+     * disabled.
+     */
+    @Deprecated
+    public void createSymlink() {
+        ensureState(JobState.DEFINE);
+        DistributedCache.createSymlink(conf);
+    }
+
+    /**
+     * Expert: Set the number of maximum attempts that will be made to run a
+     * map task.
+     *
+     * @param n the number of attempts per map task.
+     */
+    public void setMaxMapAttempts(int n) {
+        ensureState(JobState.DEFINE);
+        conf.setMaxMapAttempts(n);
+    }
+
+    /**
+     * Expert: Set the number of maximum attempts that will be made to run a
+     * reduce task.
+     *
+     * @param n the number of attempts per reduce task.
+     */
+    public void setMaxReduceAttempts(int n) {
+        ensureState(JobState.DEFINE);
+        conf.setMaxReduceAttempts(n);
+    }
+
+    /**
+     * Set whether the system should collect profiler information for some of
+     * the tasks in this job? The information is stored in the user log
+     * directory.
+     * @param newValue true means it should be gathered
+     */
+    public void setProfileEnabled(boolean newValue) {
+        ensureState(JobState.DEFINE);
+        conf.setProfileEnabled(newValue);
+    }
+
+    /**
+     * Set the profiler configuration arguments. If the string contains a '%s' it
+     * will be replaced with the name of the profiling output file when the task
+     * runs.
+     *
+     * This value is passed to the task child JVM on the command line.
+     *
+     * @param value the configuration string
+     */
+    public void setProfileParams(String value) {
+        ensureState(JobState.DEFINE);
+        conf.setProfileParams(value);
+    }
+
+    /**
+     * Set the ranges of maps or reduces to profile. setProfileEnabled(true)
+     * must also be called.
+     * @param newValue a set of integer ranges of the map ids
+     */
+    public void setProfileTaskRange(boolean isMap, String newValue) {
+        ensureState(JobState.DEFINE);
+        conf.setProfileTaskRange(isMap, newValue);
+    }
+
+    private void ensureNotSet(String attr, String msg) throws IOException {
+        if (conf.get(attr) != null) {
+            throw new IOException(attr + " is incompatible with " + msg + " mode.");
+        }
+    }
+
+    /**
+     * Sets the flag that will allow the JobTracker to cancel the HDFS delegation
+     * tokens upon job completion. Defaults to true.
+     */
+    public void setCancelDelegationTokenUponJobCompletion(boolean value) {
+        ensureState(JobState.DEFINE);
+        conf.setBoolean(JOB_CANCEL_DELEGATION_TOKEN, value);
+    }
+
+    /**
+     * Default to the new APIs unless they are explicitly set or the old mapper or
+     * reduce attributes are used.
+     * @throws IOException if the configuration is inconsistant
+     */
+    private void setUseNewAPI() throws IOException {
+        int numReduces = conf.getNumReduceTasks();
+        String oldMapperClass = "mapred.mapper.class";
+        String oldReduceClass = "mapred.reducer.class";
+        conf.setBooleanIfUnset("mapred.mapper.new-api",
+                conf.get(oldMapperClass) == null);
+        if (conf.getUseNewMapper()) {
+            String mode = "new map API";
+            ensureNotSet("mapred.input.format.class", mode);
+            ensureNotSet(oldMapperClass, mode);
+            if (numReduces != 0) {
+                ensureNotSet("mapred.partitioner.class", mode);
+            } else {
+                ensureNotSet("mapred.output.format.class", mode);
+            }
+        } else {
+            String mode = "map compatability";
+            ensureNotSet(INPUT_FORMAT_CLASS_ATTR, mode);
+            ensureNotSet(MAP_CLASS_ATTR, mode);
+            if (numReduces != 0) {
+                ensureNotSet(PARTITIONER_CLASS_ATTR, mode);
+            } else {
+                ensureNotSet(OUTPUT_FORMAT_CLASS_ATTR, mode);
+            }
+        }
+        if (numReduces != 0) {
+            conf.setBooleanIfUnset("mapred.reducer.new-api",
+                    conf.get(oldReduceClass) == null);
+            if (conf.getUseNewReducer()) {
+                String mode = "new reduce API";
+                ensureNotSet("mapred.output.format.class", mode);
+                ensureNotSet(oldReduceClass, mode);
+            } else {
+                String mode = "reduce compatability";
+                ensureNotSet(OUTPUT_FORMAT_CLASS_ATTR, mode);
+                ensureNotSet(REDUCE_CLASS_ATTR, mode);
+            }
+        }
+    }
+
+    private synchronized void connect()
+            throws IOException, InterruptedException, ClassNotFoundException {
+        if (cluster == null) {
+            cluster =
+                    ugi.doAs(new PrivilegedExceptionAction<Cluster>() {
+                        public Cluster run()
+                                throws IOException, InterruptedException,
+                                ClassNotFoundException {
+                            return new Cluster(getConfiguration());
+                        }
+                    });
+        }
+    }
+
+    boolean isConnected() {
+        return cluster != null;
+    }
+
+    /** Only for mocking via unit tests. */
+    @Private
+    public JobSubmitter getJobSubmitter(FileSystem fs,
+                                        ClientProtocol submitClient) throws IOException {
+        return new JobSubmitter(fs, submitClient);
+    }
+    /**
+     * Submit the job to the cluster and return immediately.
+     * @throws IOException
+     */
+    public void submit()
+            throws IOException, InterruptedException, ClassNotFoundException {
+        ensureState(JobState.DEFINE);
+        setUseNewAPI();
+        connect();
+        final JobSubmitter submitter =
+                getJobSubmitter(cluster.getFileSystem(), cluster.getClient());
+        status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {
+            public JobStatus run() throws IOException, InterruptedException,
+                    ClassNotFoundException {
+                return submitter.submitJobInternal(Job.this, cluster);
+            }
+        });
+        state = JobState.RUNNING;
+        LOG.info("The url to track the job: " + getTrackingURL());
+    }
+
+    /**
+     * Submit the job to the cluster and wait for it to finish.
+     * @param verbose print the progress to the user
+     * @return true if the job succeeded
+     * @throws IOException thrown if the communication with the
+     *         <code>JobTracker</code> is lost
+     */
+    public boolean waitForCompletion(boolean verbose
+    ) throws IOException, InterruptedException,
+            ClassNotFoundException {
+        if (state == JobState.DEFINE) {
+            submit();
+        }
+        if (verbose) {
+            monitorAndPrintJob();
+        } else {
+            // get the completion poll interval from the client.
+            int completionPollIntervalMillis =
+                    Job.getCompletionPollInterval(cluster.getConf());
+            while (!isComplete()) {
+                try {
+                    Thread.sleep(completionPollIntervalMillis);
+                } catch (InterruptedException ie) {
+                }
+            }
+        }
+        return isSuccessful();
+    }
+
+    /**
+     * Monitor a job and print status in real-time as progress is made and tasks
+     * fail.
+     * @return true if the job succeeded
+     * @throws IOException if communication to the JobTracker fails
+     */
+    public boolean monitorAndPrintJob()
+            throws IOException, InterruptedException {
+        String lastReport = null;
+        Job.TaskStatusFilter filter;
+        Configuration clientConf = getConfiguration();
+        filter = Job.getTaskOutputFilter(clientConf);
+        JobID jobId = getJobID();
+        LOG.info("Running job: " + jobId);
+
+        String[] ids = jobId.toString().split("_");
+        String jobApplicationId = ids[1] + "_" + ids[2];
+        GranularLoggerStore.load();
+        GranularLoggerStore.jobLogger.setActorId(jobApplicationId);
+        LOG.info(GranularLoggerStore.jobLogger.logInfo(MapReduceV2Term.StartTime, String.valueOf(System.currentTimeMillis())));
+        LOG.info(GranularLoggerStore.jobLogger.logInfo(MapReduceV2Term.JobName, getJobName()));
+
+
+        int eventCounter = 0;
+        boolean profiling = getProfileEnabled();
+        IntegerRanges mapRanges = getProfileTaskRange(true);
+        IntegerRanges reduceRanges = getProfileTaskRange(false);
+        int progMonitorPollIntervalMillis =
+                Job.getProgressPollInterval(clientConf);
+    /* make sure to report full progress after the job is done */
+        boolean reportedAfterCompletion = false;
+        boolean reportedUberMode = false;
+        while (!isComplete() || !reportedAfterCompletion) {
+            if (isComplete()) {
+                reportedAfterCompletion = true;
+            } else {
+                Thread.sleep(progMonitorPollIntervalMillis);
+            }
+            if (status.getState() == JobStatus.State.PREP) {
+                continue;
+            }
+            if (!reportedUberMode) {
+                reportedUberMode = true;
+                LOG.info("Job " + jobId + " running in uber mode : " + isUber());
+            }
+            String report =
+                    (" map " + StringUtils.formatPercent(mapProgress(), 0)+
+                            " reduce " +
+                            StringUtils.formatPercent(reduceProgress(), 0));
+            if (!report.equals(lastReport)) {
+                LOG.info(report);
+                lastReport = report;
+            }
+
+            TaskCompletionEvent[] events =
+                    getTaskCompletionEvents(eventCounter, 10);
+            eventCounter += events.length;
+            printTaskEvents(events, filter, profiling, mapRanges, reduceRanges);
+        }
+        boolean success = isSuccessful();
+        if (success) {
+            LOG.info("Job " + jobId + " completed successfully");
+            LOG.info(GranularLoggerStore.jobLogger.logInfo(MapReduceV2Term.EndTime, String.valueOf(System.currentTimeMillis())));
+        } else {
+            LOG.info("Job " + jobId + " failed with state " + status.getState() +
+                    " due to: " + status.getFailureInfo());
+        }
+        Counters counters = getCounters();
+        if (counters != null) {
+            LOG.info(counters.toString());
+        }
+        return success;
+    }
+
+    /**
+     * @return true if the profile parameters indicate that this is using
+     * hprof, which generates profile files in a particular location
+     * that we can retrieve to the client.
+     */
+    private boolean shouldDownloadProfile() {
+        // Check the argument string that was used to initialize profiling.
+        // If this indicates hprof and file-based output, then we're ok to
+        // download.
+        String profileParams = getProfileParams();
+
+        if (null == profileParams) {
+            return false;
+        }
+
+        // Split this on whitespace.
+        String [] parts = profileParams.split("[ \\t]+");
+
+        // If any of these indicate hprof, and the use of output files, return true.
+        boolean hprofFound = false;
+        boolean fileFound = false;
+        for (String p : parts) {
+            if (p.startsWith("-agentlib:hprof") || p.startsWith("-Xrunhprof")) {
+                hprofFound = true;
+
+                // This contains a number of comma-delimited components, one of which
+                // may specify the file to write to. Make sure this is present and
+                // not empty.
+                String [] subparts = p.split(",");
+                for (String sub : subparts) {
+                    if (sub.startsWith("file=") && sub.length() != "file=".length()) {
+                        fileFound = true;
+                    }
+                }
+            }
+        }
+
+        return hprofFound && fileFound;
+    }
+
+    private void printTaskEvents(TaskCompletionEvent[] events,
+                                 Job.TaskStatusFilter filter, boolean profiling, IntegerRanges mapRanges,
+                                 IntegerRanges reduceRanges) throws IOException, InterruptedException {
+        for (TaskCompletionEvent event : events) {
+            switch (filter) {
+                case NONE:
+                    break;
+                case SUCCEEDED:
+                    if (event.getStatus() ==
+                            TaskCompletionEvent.Status.SUCCEEDED) {
+                        LOG.info(event.toString());
+                    }
+                    break;
+                case FAILED:
+                    if (event.getStatus() ==
+                            TaskCompletionEvent.Status.FAILED) {
+                        LOG.info(event.toString());
+                        // Displaying the task diagnostic information
+                        TaskAttemptID taskId = event.getTaskAttemptId();
+                        String[] taskDiagnostics = getTaskDiagnostics(taskId);
+                        if (taskDiagnostics != null) {
+                            for (String diagnostics : taskDiagnostics) {
+                                System.err.println(diagnostics);
+                            }
+                        }
+                    }
+                    break;
+                case KILLED:
+                    if (event.getStatus() == TaskCompletionEvent.Status.KILLED){
+                        LOG.info(event.toString());
+                    }
+                    break;
+                case ALL:
+                    LOG.info(event.toString());
+                    break;
+            }
+        }
+    }
+
+    /** The interval at which monitorAndPrintJob() prints status */
+    public static int getProgressPollInterval(Configuration conf) {
+        // Read progress monitor poll interval from config. Default is 1 second.
+        int progMonitorPollIntervalMillis = conf.getInt(
+                PROGRESS_MONITOR_POLL_INTERVAL_KEY, DEFAULT_MONITOR_POLL_INTERVAL);
+        if (progMonitorPollIntervalMillis < 1) {
+            LOG.warn(PROGRESS_MONITOR_POLL_INTERVAL_KEY +
+                    " has been set to an invalid value; "
+                    + " replacing with " + DEFAULT_MONITOR_POLL_INTERVAL);
+            progMonitorPollIntervalMillis = DEFAULT_MONITOR_POLL_INTERVAL;
+        }
+        return progMonitorPollIntervalMillis;
+    }
+
+    /** The interval at which waitForCompletion() should check. */
+    public static int getCompletionPollInterval(Configuration conf) {
+        int completionPollIntervalMillis = conf.getInt(
+                COMPLETION_POLL_INTERVAL_KEY, DEFAULT_COMPLETION_POLL_INTERVAL);
+        if (completionPollIntervalMillis < 1) {
+            LOG.warn(COMPLETION_POLL_INTERVAL_KEY +
+                    " has been set to an invalid value; "
+                    + "replacing with " + DEFAULT_COMPLETION_POLL_INTERVAL);
+            completionPollIntervalMillis = DEFAULT_COMPLETION_POLL_INTERVAL;
+        }
+        return completionPollIntervalMillis;
+    }
+
+    /**
+     * Get the task output filter.
+     *
+     * @param conf the configuration.
+     * @return the filter level.
+     */
+    public static TaskStatusFilter getTaskOutputFilter(Configuration conf) {
+        return TaskStatusFilter.valueOf(conf.get(Job.OUTPUT_FILTER, "FAILED"));
+    }
+
+    /**
+     * Modify the Configuration to set the task output filter.
+     *
+     * @param conf the Configuration to modify.
+     * @param newValue the value to set.
+     */
+    public static void setTaskOutputFilter(Configuration conf,
+                                           TaskStatusFilter newValue) {
+        conf.set(Job.OUTPUT_FILTER, newValue.toString());
+    }
+
+    public boolean isUber() throws IOException, InterruptedException {
+        ensureState(JobState.RUNNING);
+        updateStatus();
+        return status.isUber();
+    }
+
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java b/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
new file mode 100644
index 0000000..712e608
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
@@ -0,0 +1,1486 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.v2.app;
+
+import java.io.IOException;
+import java.lang.reflect.Constructor;
+import java.lang.reflect.InvocationTargetException;
+import java.security.PrivilegedExceptionAction;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.granular.MapReduceV2Term;
+import org.apache.hadoop.granular.GranularLoggerStore;
+import org.apache.hadoop.mapred.FileOutputCommitter;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.LocalContainerLauncher;
+import org.apache.hadoop.mapred.TaskAttemptListenerImpl;
+import org.apache.hadoop.mapred.TaskLog;
+import org.apache.hadoop.mapred.TaskUmbilicalProtocol;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.OutputCommitter;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TypeConverter;
+import org.apache.hadoop.mapreduce.jobhistory.AMStartedEvent;
+import org.apache.hadoop.mapreduce.jobhistory.EventReader;
+import org.apache.hadoop.mapreduce.jobhistory.EventType;
+import org.apache.hadoop.mapreduce.jobhistory.HistoryEvent;
+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService;
+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent;
+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler;
+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser;
+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.JobInfo;
+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.TaskAttemptInfo;
+import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.TaskInfo;
+import org.apache.hadoop.mapreduce.security.TokenCache;
+import org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager;
+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
+import org.apache.hadoop.mapreduce.v2.api.records.AMInfo;
+import org.apache.hadoop.mapreduce.v2.api.records.JobId;
+import org.apache.hadoop.mapreduce.v2.api.records.JobReport;
+import org.apache.hadoop.mapreduce.v2.api.records.JobState;
+import org.apache.hadoop.mapreduce.v2.api.records.TaskId;
+import org.apache.hadoop.mapreduce.v2.api.records.TaskState;
+import org.apache.hadoop.mapreduce.v2.api.records.TaskType;
+import org.apache.hadoop.mapreduce.v2.app.client.ClientService;
+import org.apache.hadoop.mapreduce.v2.app.client.MRClientService;
+import org.apache.hadoop.mapreduce.v2.app.commit.CommitterEvent;
+import org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler;
+import org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType;
+import org.apache.hadoop.mapreduce.v2.app.job.Job;
+import org.apache.hadoop.mapreduce.v2.app.job.JobStateInternal;
+import org.apache.hadoop.mapreduce.v2.app.job.Task;
+import org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt;
+import org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent;
+import org.apache.hadoop.mapreduce.v2.app.job.event.JobEventType;
+import org.apache.hadoop.mapreduce.v2.app.job.event.JobFinishEvent;
+import org.apache.hadoop.mapreduce.v2.app.job.event.JobStartEvent;
+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent;
+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType;
+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent;
+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType;
+import org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl;
+import org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncher;
+import org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent;
+import org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl;
+import org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator;
+import org.apache.hadoop.mapreduce.v2.app.metrics.MRAppMetrics;
+import org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocator;
+import org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent;
+import org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator;
+import org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator;
+import org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor;
+import org.apache.hadoop.mapreduce.v2.app.rm.RMHeartbeatHandler;
+import org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator;
+import org.apache.hadoop.mapreduce.v2.app.speculate.Speculator;
+import org.apache.hadoop.mapreduce.v2.app.speculate.SpeculatorEvent;
+import org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils;
+import org.apache.hadoop.mapreduce.v2.util.MRApps;
+import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;
+import org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil;
+import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.service.AbstractService;
+import org.apache.hadoop.service.CompositeService;
+import org.apache.hadoop.service.Service;
+import org.apache.hadoop.service.ServiceOperations;
+import org.apache.hadoop.util.ExitUtil;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.ShutdownHookManager;
+import org.apache.hadoop.util.StringInterner;
+import org.apache.hadoop.yarn.YarnUncaughtExceptionHandler;
+import org.apache.hadoop.yarn.api.ApplicationConstants;
+import org.apache.hadoop.yarn.api.ApplicationConstants.Environment;
+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
+import org.apache.hadoop.yarn.api.records.ApplicationId;
+import org.apache.hadoop.yarn.api.records.ContainerId;
+import org.apache.hadoop.yarn.conf.YarnConfiguration;
+import org.apache.hadoop.yarn.event.AsyncDispatcher;
+import org.apache.hadoop.yarn.event.Dispatcher;
+import org.apache.hadoop.yarn.event.Event;
+import org.apache.hadoop.yarn.event.EventHandler;
+import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;
+import org.apache.hadoop.yarn.security.AMRMTokenIdentifier;
+import org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager;
+import org.apache.hadoop.yarn.util.Clock;
+import org.apache.hadoop.yarn.util.ConverterUtils;
+import org.apache.hadoop.yarn.util.SystemClock;
+import org.apache.log4j.LogManager;
+
+import com.google.common.annotations.VisibleForTesting;
+
+/**
+ * The Map-Reduce Application Master.
+ * The state machine is encapsulated in the implementation of Job interface.
+ * All state changes happens via Job interface. Each event 
+ * results in a Finite State Transition in Job.
+ *
+ * MR AppMaster is the composition of loosely coupled services. The services 
+ * interact with each other via events. The components resembles the 
+ * Actors model. The component acts on received event and send out the 
+ * events to other components.
+ * This keeps it highly concurrent with no or minimal synchronization needs.
+ *
+ * The events are dispatched by a central Dispatch mechanism. All components
+ * register to the Dispatcher.
+ *
+ * The information is shared across different components using AppContext.
+ */
+
+@SuppressWarnings("rawtypes")
+public class MRAppMaster extends CompositeService {
+
+    private static final Log LOG = LogFactory.getLog(MRAppMaster.class);
+
+    /**
+     * Priority of the MRAppMaster shutdown hook.
+     */
+    public static final int SHUTDOWN_HOOK_PRIORITY = 30;
+
+    private Clock clock;
+    private final long startTime;
+    private final long appSubmitTime;
+    private String appName;
+    private final ApplicationAttemptId appAttemptID;
+    private final ContainerId containerID;
+    private final String nmHost;
+    private final int nmPort;
+    private final int nmHttpPort;
+    protected final MRAppMetrics metrics;
+    private final int maxAppAttempts;
+    private Map<TaskId, TaskInfo> completedTasksFromPreviousRun;
+    private List<AMInfo> amInfos;
+    private AppContext context;
+    private Dispatcher dispatcher;
+    private ClientService clientService;
+    private ContainerAllocator containerAllocator;
+    private ContainerLauncher containerLauncher;
+    private EventHandler<CommitterEvent> committerEventHandler;
+    private Speculator speculator;
+    private TaskAttemptListener taskAttemptListener;
+    private JobTokenSecretManager jobTokenSecretManager =
+            new JobTokenSecretManager();
+    private JobId jobId;
+    private boolean newApiCommitter;
+    private OutputCommitter committer;
+    private JobEventDispatcher jobEventDispatcher;
+    private JobHistoryEventHandler jobHistoryEventHandler;
+    private SpeculatorEventDispatcher speculatorEventDispatcher;
+
+    private Job job;
+    private Credentials jobCredentials = new Credentials(); // Filled during init
+    protected UserGroupInformation currentUser; // Will be setup during init
+
+    @VisibleForTesting
+    protected volatile boolean isLastAMRetry = false;
+    //Something happened and we should shut down right after we start up.
+    boolean errorHappenedShutDown = false;
+    private String shutDownMessage = null;
+    JobStateInternal forcedState = null;
+    private final ScheduledExecutorService logSyncer;
+
+    private long recoveredJobStartTime = 0;
+
+    @VisibleForTesting
+    protected AtomicBoolean successfullyUnregistered =
+            new AtomicBoolean(false);
+
+    public MRAppMaster(ApplicationAttemptId applicationAttemptId,
+                       ContainerId containerId, String nmHost, int nmPort, int nmHttpPort,
+                       long appSubmitTime, int maxAppAttempts) {
+        this(applicationAttemptId, containerId, nmHost, nmPort, nmHttpPort,
+                new SystemClock(), appSubmitTime, maxAppAttempts);
+    }
+
+    public MRAppMaster(ApplicationAttemptId applicationAttemptId,
+                       ContainerId containerId, String nmHost, int nmPort, int nmHttpPort,
+                       Clock clock, long appSubmitTime, int maxAppAttempts) {
+        super(MRAppMaster.class.getName());
+        this.clock = clock;
+        this.startTime = clock.getTime();
+        this.appSubmitTime = appSubmitTime;
+        this.appAttemptID = applicationAttemptId;
+        this.containerID = containerId;
+        this.nmHost = nmHost;
+        this.nmPort = nmPort;
+        this.nmHttpPort = nmHttpPort;
+        this.metrics = MRAppMetrics.create();
+        this.maxAppAttempts = maxAppAttempts;
+        logSyncer = TaskLog.createLogSyncer();
+        LOG.info("Created MRAppMaster for application " + applicationAttemptId);
+
+        String[] ids = appAttemptID.toString().split("_");
+        String appId = ids[1] + "_" + ids[2];
+        GranularLoggerStore.appMasterLogger.setActorId(appId);
+        LOG.info(GranularLoggerStore.appMasterLogger.logInfo(MapReduceV2Term.StartTime, String.valueOf(System.currentTimeMillis())));
+    }
+
+    @Override
+    protected void serviceInit(final Configuration conf) throws Exception {
+        conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);
+
+        initJobCredentialsAndUGI(conf);
+
+        context = new RunningAppContext(conf);
+
+        ((RunningAppContext)context).computeIsLastAMRetry();
+        LOG.info("The specific max attempts: " + maxAppAttempts +
+                " for application: " + appAttemptID.getApplicationId().getId() +
+                ". Attempt num: " + appAttemptID.getAttemptId() +
+                " is last retry: " + isLastAMRetry);
+
+        // Job name is the same as the app name util we support DAG of jobs
+        // for an app later
+        appName = conf.get(MRJobConfig.JOB_NAME, "<missing app name>");
+
+        conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, appAttemptID.getAttemptId());
+
+        newApiCommitter = false;
+        jobId = MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),
+                appAttemptID.getApplicationId().getId());
+        int numReduceTasks = conf.getInt(MRJobConfig.NUM_REDUCES, 0);
+        if ((numReduceTasks > 0 &&
+                conf.getBoolean("mapred.reducer.new-api", false)) ||
+                (numReduceTasks == 0 &&
+                        conf.getBoolean("mapred.mapper.new-api", false)))  {
+            newApiCommitter = true;
+            LOG.info("Using mapred newApiCommitter.");
+        }
+
+        boolean copyHistory = false;
+        try {
+            String user = UserGroupInformation.getCurrentUser().getShortUserName();
+            Path stagingDir = MRApps.getStagingAreaDir(conf, user);
+            FileSystem fs = getFileSystem(conf);
+            boolean stagingExists = fs.exists(stagingDir);
+            Path startCommitFile = MRApps.getStartJobCommitFile(conf, user, jobId);
+            boolean commitStarted = fs.exists(startCommitFile);
+            Path endCommitSuccessFile = MRApps.getEndJobCommitSuccessFile(conf, user, jobId);
+            boolean commitSuccess = fs.exists(endCommitSuccessFile);
+            Path endCommitFailureFile = MRApps.getEndJobCommitFailureFile(conf, user, jobId);
+            boolean commitFailure = fs.exists(endCommitFailureFile);
+            if(!stagingExists) {
+                isLastAMRetry = true;
+                LOG.info("Attempt num: " + appAttemptID.getAttemptId() +
+                        " is last retry: " + isLastAMRetry +
+                        " because the staging dir doesn't exist.");
+                errorHappenedShutDown = true;
+                forcedState = JobStateInternal.ERROR;
+                shutDownMessage = "Staging dir does not exist " + stagingDir;
+                LOG.fatal(shutDownMessage);
+            } else if (commitStarted) {
+                //A commit was started so this is the last time, we just need to know
+                // what result we will use to notify, and how we will unregister
+                errorHappenedShutDown = true;
+                isLastAMRetry = true;
+                LOG.info("Attempt num: " + appAttemptID.getAttemptId() +
+                        " is last retry: " + isLastAMRetry +
+                        " because a commit was started.");
+                copyHistory = true;
+                if (commitSuccess) {
+                    shutDownMessage = "We crashed after successfully committing. Recovering.";
+                    forcedState = JobStateInternal.SUCCEEDED;
+                } else if (commitFailure) {
+                    shutDownMessage = "We crashed after a commit failure.";
+                    forcedState = JobStateInternal.FAILED;
+                } else {
+                    //The commit is still pending, commit error
+                    shutDownMessage = "We crashed durring a commit";
+                    forcedState = JobStateInternal.ERROR;
+                }
+            }
+        } catch (IOException e) {
+            throw new YarnRuntimeException("Error while initializing", e);
+        }
+
+        if (errorHappenedShutDown) {
+            dispatcher = createDispatcher();
+            addIfService(dispatcher);
+
+            NoopEventHandler eater = new NoopEventHandler();
+            //We do not have a JobEventDispatcher in this path
+            dispatcher.register(JobEventType.class, eater);
+
+            EventHandler<JobHistoryEvent> historyService = null;
+            if (copyHistory) {
+                historyService =
+                        createJobHistoryHandler(context);
+                dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,
+                        historyService);
+            } else {
+                dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,
+                        eater);
+            }
+
+            if (copyHistory) {
+                // Now that there's a FINISHING state for application on RM to give AMs
+                // plenty of time to clean up after unregister it's safe to clean staging
+                // directory after unregistering with RM. So, we start the staging-dir
+                // cleaner BEFORE the ContainerAllocator so that on shut-down,
+                // ContainerAllocator unregisters first and then the staging-dir cleaner
+                // deletes staging directory.
+                addService(createStagingDirCleaningService());
+            }
+
+            // service to allocate containers from RM (if non-uber) or to fake it (uber)
+            containerAllocator = createContainerAllocator(null, context);
+            addIfService(containerAllocator);
+            dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);
+
+            if (copyHistory) {
+                // Add the JobHistoryEventHandler last so that it is properly stopped first.
+                // This will guarantee that all history-events are flushed before AM goes
+                // ahead with shutdown.
+                // Note: Even though JobHistoryEventHandler is started last, if any
+                // component creates a JobHistoryEvent in the meanwhile, it will be just be
+                // queued inside the JobHistoryEventHandler
+                addIfService(historyService);
+
+                JobHistoryCopyService cpHist = new JobHistoryCopyService(appAttemptID,
+                        dispatcher.getEventHandler());
+                addIfService(cpHist);
+            }
+        } else {
+            committer = createOutputCommitter(conf);
+
+            dispatcher = createDispatcher();
+            addIfService(dispatcher);
+
+            //service to handle requests from JobClient
+            clientService = createClientService(context);
+            // Init ClientService separately so that we stop it separately, since this
+            // service needs to wait some time before it stops so clients can know the
+            // final states
+            clientService.init(conf);
+
+            containerAllocator = createContainerAllocator(clientService, context);
+
+            //service to handle the output committer
+            committerEventHandler = createCommitterEventHandler(context, committer);
+            addIfService(committerEventHandler);
+
+            //service to handle requests to TaskUmbilicalProtocol
+            taskAttemptListener = createTaskAttemptListener(context);
+            addIfService(taskAttemptListener);
+
+            //service to log job history events
+            EventHandler<JobHistoryEvent> historyService =
+                    createJobHistoryHandler(context);
+            dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,
+                    historyService);
+
+            this.jobEventDispatcher = new JobEventDispatcher();
+
+            //register the event dispatchers
+            dispatcher.register(JobEventType.class, jobEventDispatcher);
+            dispatcher.register(TaskEventType.class, new TaskEventDispatcher());
+            dispatcher.register(TaskAttemptEventType.class,
+                    new TaskAttemptEventDispatcher());
+            dispatcher.register(CommitterEventType.class, committerEventHandler);
+
+            if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false)
+                    || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false)) {
+                //optional service to speculate on task attempts' progress
+                speculator = createSpeculator(conf, context);
+                addIfService(speculator);
+            }
+
+            speculatorEventDispatcher = new SpeculatorEventDispatcher(conf);
+            dispatcher.register(Speculator.EventType.class,
+                    speculatorEventDispatcher);
+
+            // Now that there's a FINISHING state for application on RM to give AMs
+            // plenty of time to clean up after unregister it's safe to clean staging
+            // directory after unregistering with RM. So, we start the staging-dir
+            // cleaner BEFORE the ContainerAllocator so that on shut-down,
+            // ContainerAllocator unregisters first and then the staging-dir cleaner
+            // deletes staging directory.
+            addService(createStagingDirCleaningService());
+
+            // service to allocate containers from RM (if non-uber) or to fake it (uber)
+            addIfService(containerAllocator);
+            dispatcher.register(ContainerAllocator.EventType.class, containerAllocator);
+
+            // corresponding service to launch allocated containers via NodeManager
+            containerLauncher = createContainerLauncher(context);
+            addIfService(containerLauncher);
+            dispatcher.register(ContainerLauncher.EventType.class, containerLauncher);
+
+            // Add the JobHistoryEventHandler last so that it is properly stopped first.
+            // This will guarantee that all history-events are flushed before AM goes
+            // ahead with shutdown.
+            // Note: Even though JobHistoryEventHandler is started last, if any
+            // component creates a JobHistoryEvent in the meanwhile, it will be just be
+            // queued inside the JobHistoryEventHandler
+            addIfService(historyService);
+        }
+        super.serviceInit(conf);
+    } // end of init()
+
+    protected Dispatcher createDispatcher() {
+        return new AsyncDispatcher();
+    }
+
+    private OutputCommitter createOutputCommitter(Configuration conf) {
+        OutputCommitter committer = null;
+
+        LOG.info("OutputCommitter set in config "
+                + conf.get("mapred.output.committer.class"));
+
+        if (newApiCommitter) {
+            org.apache.hadoop.mapreduce.v2.api.records.TaskId taskID = MRBuilderUtils
+                    .newTaskId(jobId, 0, TaskType.MAP);
+            org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID = MRBuilderUtils
+                    .newTaskAttemptId(taskID, 0);
+            TaskAttemptContext taskContext = new TaskAttemptContextImpl(conf,
+                    TypeConverter.fromYarn(attemptID));
+            OutputFormat outputFormat;
+            try {
+                outputFormat = ReflectionUtils.newInstance(taskContext
+                        .getOutputFormatClass(), conf);
+                committer = outputFormat.getOutputCommitter(taskContext);
+            } catch (Exception e) {
+                throw new YarnRuntimeException(e);
+            }
+        } else {
+            committer = ReflectionUtils.newInstance(conf.getClass(
+                    "mapred.output.committer.class", FileOutputCommitter.class,
+                    org.apache.hadoop.mapred.OutputCommitter.class), conf);
+        }
+        LOG.info("OutputCommitter is " + committer.getClass().getName());
+        return committer;
+    }
+
+    protected boolean keepJobFiles(JobConf conf) {
+        return (conf.getKeepTaskFilesPattern() != null || conf
+                .getKeepFailedTaskFiles());
+    }
+
+    /**
+     * Create the default file System for this job.
+     * @param conf the conf object
+     * @return the default filesystem for this job
+     * @throws IOException
+     */
+    protected FileSystem getFileSystem(Configuration conf) throws IOException {
+        return FileSystem.get(conf);
+    }
+
+    protected Credentials getCredentials() {
+        return jobCredentials;
+    }
+
+    /**
+     * clean up staging directories for the job.
+     * @throws IOException
+     */
+    public void cleanupStagingDir() throws IOException {
+    /* make sure we clean the staging files */
+        String jobTempDir = null;
+        FileSystem fs = getFileSystem(getConfig());
+        try {
+            if (!keepJobFiles(new JobConf(getConfig()))) {
+                jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);
+                if (jobTempDir == null) {
+                    LOG.warn("Job Staging directory is null");
+                    return;
+                }
+                Path jobTempDirPath = new Path(jobTempDir);
+                LOG.info("Deleting staging directory " + FileSystem.getDefaultUri(getConfig()) +
+                        " " + jobTempDir);
+
+                fs.delete(jobTempDirPath, true);
+
+
+                LOG.info(GranularLoggerStore.appMasterLogger.logInfo(MapReduceV2Term.EndTime, String.valueOf(System.currentTimeMillis())));
+            }
+        } catch(IOException io) {
+            LOG.error("Failed to cleanup staging dir " + jobTempDir, io);
+        }
+    }
+
+    /**
+     * Exit call. Just in a function call to enable testing.
+     */
+    protected void sysexit() {
+        System.exit(0);
+    }
+
+    @VisibleForTesting
+    public void shutDownJob() {
+        // job has finished
+        // this is the only job, so shut down the Appmaster
+        // note in a workflow scenario, this may lead to creation of a new
+        // job (FIXME?)
+
+        try {
+            //if isLastAMRetry comes as true, should never set it to false
+            if ( !isLastAMRetry){
+                if (((JobImpl)job).getInternalState() != JobStateInternal.REBOOT) {
+                    LOG.info("We are finishing cleanly so this is the last retry");
+                    isLastAMRetry = true;
+                }
+            }
+            notifyIsLastAMRetry(isLastAMRetry);
+            // Stop all services
+            // This will also send the final report to the ResourceManager
+            LOG.info("Calling stop for all the services");
+            MRAppMaster.this.stop();
+
+            if (isLastAMRetry) {
+                // Send job-end notification when it is safe to report termination to
+                // users and it is the last AM retry
+                if (getConfig().get(MRJobConfig.MR_JOB_END_NOTIFICATION_URL) != null) {
+                    try {
+                        LOG.info("Job end notification started for jobID : "
+                                + job.getReport().getJobId());
+                        JobEndNotifier notifier = new JobEndNotifier();
+                        notifier.setConf(getConfig());
+                        JobReport report = job.getReport();
+                        // If unregistration fails, the final state is unavailable. However,
+                        // at the last AM Retry, the client will finally be notified FAILED
+                        // from RM, so we should let users know FAILED via notifier as well
+                        if (!context.hasSuccessfullyUnregistered()) {
+                            report.setJobState(JobState.FAILED);
+                        }
+                        notifier.notify(report);
+                    } catch (InterruptedException ie) {
+                        LOG.warn("Job end notification interrupted for jobID : "
+                                + job.getReport().getJobId(), ie);
+                    }
+                }
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {
+                e.printStackTrace();
+            }
+            clientService.stop();
+        } catch (Throwable t) {
+            LOG.warn("Graceful stop failed ", t);
+        }
+
+    }
+
+    private class JobFinishEventHandler implements EventHandler<JobFinishEvent> {
+        @Override
+        public void handle(JobFinishEvent event) {
+            // Create a new thread to shutdown the AM. We should not do it in-line
+            // to avoid blocking the dispatcher itself.
+            new Thread() {
+
+                @Override
+                public void run() {
+                    shutDownJob();
+                }
+            }.start();
+        }
+    }
+
+    /**
+     * create an event handler that handles the job finish event.
+     * @return the job finish event handler.
+     */
+    protected EventHandler<JobFinishEvent> createJobFinishEventHandler() {
+        return new JobFinishEventHandler();
+    }
+
+    /** Create and initialize (but don't start) a single job.
+     * @param forcedState a state to force the job into or null for normal operation.
+     * @param diagnostic a diagnostic message to include with the job.
+     */
+    protected Job createJob(Configuration conf, JobStateInternal forcedState,
+                            String diagnostic) {
+
+        // create single job
+        Job newJob =
+                new JobImpl(jobId, appAttemptID, conf, dispatcher.getEventHandler(),
+                        taskAttemptListener, jobTokenSecretManager, jobCredentials, clock,
+                        completedTasksFromPreviousRun, metrics,
+                        committer, newApiCommitter,
+                        currentUser.getUserName(), appSubmitTime, amInfos, context,
+                        forcedState, diagnostic);
+        ((RunningAppContext) context).jobs.put(newJob.getID(), newJob);
+
+        dispatcher.register(JobFinishEvent.Type.class,
+                createJobFinishEventHandler());
+        return newJob;
+    } // end createJob()
+
+
+    /**
+     * Obtain the tokens needed by the job and put them in the UGI
+     * @param conf
+     */
+    protected void initJobCredentialsAndUGI(Configuration conf) {
+
+        try {
+            this.currentUser = UserGroupInformation.getCurrentUser();
+            this.jobCredentials = ((JobConf)conf).getCredentials();
+        } catch (IOException e) {
+            throw new YarnRuntimeException(e);
+        }
+    }
+
+    protected EventHandler<JobHistoryEvent> createJobHistoryHandler(
+            AppContext context) {
+        this.jobHistoryEventHandler = new JobHistoryEventHandler(context,
+                getStartCount());
+        return this.jobHistoryEventHandler;
+    }
+
+    protected AbstractService createStagingDirCleaningService() {
+        return new StagingDirCleaningService();
+    }
+
+    protected Speculator createSpeculator(Configuration conf, AppContext context) {
+        Class<? extends Speculator> speculatorClass;
+
+        try {
+            speculatorClass
+                    // "yarn.mapreduce.job.speculator.class"
+                    = conf.getClass(MRJobConfig.MR_AM_JOB_SPECULATOR,
+                    DefaultSpeculator.class,
+                    Speculator.class);
+            Constructor<? extends Speculator> speculatorConstructor
+                    = speculatorClass.getConstructor
+                    (Configuration.class, AppContext.class);
+            Speculator result = speculatorConstructor.newInstance(conf, context);
+
+            return result;
+        } catch (InstantiationException ex) {
+            LOG.error("Can't make a speculator -- check "
+                    + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);
+            throw new YarnRuntimeException(ex);
+        } catch (IllegalAccessException ex) {
+            LOG.error("Can't make a speculator -- check "
+                    + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);
+            throw new YarnRuntimeException(ex);
+        } catch (InvocationTargetException ex) {
+            LOG.error("Can't make a speculator -- check "
+                    + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);
+            throw new YarnRuntimeException(ex);
+        } catch (NoSuchMethodException ex) {
+            LOG.error("Can't make a speculator -- check "
+                    + MRJobConfig.MR_AM_JOB_SPECULATOR, ex);
+            throw new YarnRuntimeException(ex);
+        }
+    }
+
+    protected TaskAttemptListener createTaskAttemptListener(AppContext context) {
+        TaskAttemptListener lis =
+                new TaskAttemptListenerImpl(context, jobTokenSecretManager,
+                        getRMHeartbeatHandler());
+        return lis;
+    }
+
+    protected EventHandler<CommitterEvent> createCommitterEventHandler(
+            AppContext context, OutputCommitter committer) {
+        return new CommitterEventHandler(context, committer,
+                getRMHeartbeatHandler());
+    }
+
+    protected ContainerAllocator createContainerAllocator(
+            final ClientService clientService, final AppContext context) {
+        return new ContainerAllocatorRouter(clientService, context);
+    }
+
+    protected RMHeartbeatHandler getRMHeartbeatHandler() {
+        return (RMHeartbeatHandler) containerAllocator;
+    }
+
+    protected ContainerLauncher
+    createContainerLauncher(final AppContext context) {
+        return new ContainerLauncherRouter(context);
+    }
+
+    //TODO:should have an interface for MRClientService
+    protected ClientService createClientService(AppContext context) {
+        return new MRClientService(context);
+    }
+
+    public ApplicationId getAppID() {
+        return appAttemptID.getApplicationId();
+    }
+
+    public ApplicationAttemptId getAttemptID() {
+        return appAttemptID;
+    }
+
+    public JobId getJobId() {
+        return jobId;
+    }
+
+    public OutputCommitter getCommitter() {
+        return committer;
+    }
+
+    public boolean isNewApiCommitter() {
+        return newApiCommitter;
+    }
+
+    public int getStartCount() {
+        return appAttemptID.getAttemptId();
+    }
+
+    public AppContext getContext() {
+        return context;
+    }
+
+    public Dispatcher getDispatcher() {
+        return dispatcher;
+    }
+
+    public Map<TaskId, TaskInfo> getCompletedTaskFromPreviousRun() {
+        return completedTasksFromPreviousRun;
+    }
+
+    public List<AMInfo> getAllAMInfos() {
+        return amInfos;
+    }
+
+    public ContainerAllocator getContainerAllocator() {
+        return containerAllocator;
+    }
+
+    public ContainerLauncher getContainerLauncher() {
+        return containerLauncher;
+    }
+
+    public TaskAttemptListener getTaskAttemptListener() {
+        return taskAttemptListener;
+    }
+
+    public Boolean isLastAMRetry() {
+        return isLastAMRetry;
+    }
+
+    /**
+     * By the time life-cycle of this router starts, job-init would have already
+     * happened.
+     */
+    private final class ContainerAllocatorRouter extends AbstractService
+            implements ContainerAllocator, RMHeartbeatHandler {
+        private final ClientService clientService;
+        private final AppContext context;
+        private ContainerAllocator containerAllocator;
+
+        ContainerAllocatorRouter(ClientService clientService,
+                                 AppContext context) {
+            super(ContainerAllocatorRouter.class.getName());
+            this.clientService = clientService;
+            this.context = context;
+        }
+
+        @Override
+        protected void serviceStart() throws Exception {
+            if (job.isUber()) {
+                this.containerAllocator = new LocalContainerAllocator(
+                        this.clientService, this.context, nmHost, nmPort, nmHttpPort
+                        , containerID);
+            } else {
+                this.containerAllocator = new RMContainerAllocator(
+                        this.clientService, this.context);
+            }
+            ((Service)this.containerAllocator).init(getConfig());
+            ((Service)this.containerAllocator).start();
+            super.serviceStart();
+        }
+
+        @Override
+        protected void serviceStop() throws Exception {
+            ServiceOperations.stop((Service) this.containerAllocator);
+            super.serviceStop();
+        }
+
+        @Override
+        public void handle(ContainerAllocatorEvent event) {
+            this.containerAllocator.handle(event);
+        }
+
+        public void setSignalled(boolean isSignalled) {
+            ((RMCommunicator) containerAllocator).setSignalled(isSignalled);
+        }
+
+        public void setShouldUnregister(boolean shouldUnregister) {
+            ((RMCommunicator) containerAllocator).setShouldUnregister(shouldUnregister);
+        }
+
+        @Override
+        public long getLastHeartbeatTime() {
+            return ((RMCommunicator) containerAllocator).getLastHeartbeatTime();
+        }
+
+        @Override
+        public void runOnNextHeartbeat(Runnable callback) {
+            ((RMCommunicator) containerAllocator).runOnNextHeartbeat(callback);
+        }
+    }
+
+    /**
+     * By the time life-cycle of this router starts, job-init would have already
+     * happened.
+     */
+    private final class ContainerLauncherRouter extends AbstractService
+            implements ContainerLauncher {
+        private final AppContext context;
+        private ContainerLauncher containerLauncher;
+
+        ContainerLauncherRouter(AppContext context) {
+            super(ContainerLauncherRouter.class.getName());
+            this.context = context;
+        }
+
+        @Override
+        protected void serviceStart() throws Exception {
+            if (job.isUber()) {
+                this.containerLauncher = new LocalContainerLauncher(context,
+                        (TaskUmbilicalProtocol) taskAttemptListener);
+            } else {
+                this.containerLauncher = new ContainerLauncherImpl(context);
+            }
+            ((Service)this.containerLauncher).init(getConfig());
+            ((Service)this.containerLauncher).start();
+            super.serviceStart();
+        }
+
+        @Override
+        public void handle(ContainerLauncherEvent event) {
+            this.containerLauncher.handle(event);
+        }
+
+        @Override
+        protected void serviceStop() throws Exception {
+            ServiceOperations.stop((Service) this.containerLauncher);
+            super.serviceStop();
+        }
+    }
+
+    private final class StagingDirCleaningService extends AbstractService {
+        StagingDirCleaningService() {
+            super(StagingDirCleaningService.class.getName());
+        }
+
+        @Override
+        protected void serviceStop() throws Exception {
+            try {
+                if(isLastAMRetry) {
+                    cleanupStagingDir();
+                } else {
+                    LOG.info("Skipping cleaning up the staging dir. "
+                            + "assuming AM will be retried.");
+                }
+            } catch (IOException io) {
+                LOG.error("Failed to cleanup staging dir: ", io);
+            }
+            super.serviceStop();
+        }
+    }
+
+    public class RunningAppContext implements AppContext {
+
+        private final Map<JobId, Job> jobs = new ConcurrentHashMap<JobId, Job>();
+        private final Configuration conf;
+        private final ClusterInfo clusterInfo = new ClusterInfo();
+        private final ClientToAMTokenSecretManager clientToAMTokenSecretManager;
+
+        public RunningAppContext(Configuration config) {
+            this.conf = config;
+            this.clientToAMTokenSecretManager =
+                    new ClientToAMTokenSecretManager(appAttemptID, null);
+        }
+
+        @Override
+        public ApplicationAttemptId getApplicationAttemptId() {
+            return appAttemptID;
+        }
+
+        @Override
+        public ApplicationId getApplicationID() {
+            return appAttemptID.getApplicationId();
+        }
+
+        @Override
+        public String getApplicationName() {
+            return appName;
+        }
+
+        @Override
+        public long getStartTime() {
+            return startTime;
+        }
+
+        @Override
+        public Job getJob(JobId jobID) {
+            return jobs.get(jobID);
+        }
+
+        @Override
+        public Map<JobId, Job> getAllJobs() {
+            return jobs;
+        }
+
+        @Override
+        public EventHandler getEventHandler() {
+            return dispatcher.getEventHandler();
+        }
+
+        @Override
+        public CharSequence getUser() {
+            return this.conf.get(MRJobConfig.USER_NAME);
+        }
+
+        @Override
+        public Clock getClock() {
+            return clock;
+        }
+
+        @Override
+        public ClusterInfo getClusterInfo() {
+            return this.clusterInfo;
+        }
+
+        @Override
+        public Set<String> getBlacklistedNodes() {
+            return ((RMContainerRequestor) containerAllocator).getBlacklistedNodes();
+        }
+
+        @Override
+        public ClientToAMTokenSecretManager getClientToAMTokenSecretManager() {
+            return clientToAMTokenSecretManager;
+        }
+
+        @Override
+        public boolean isLastAMRetry(){
+            return isLastAMRetry;
+        }
+
+        @Override
+        public boolean hasSuccessfullyUnregistered() {
+            return successfullyUnregistered.get();
+        }
+
+        public void markSuccessfulUnregistration() {
+            successfullyUnregistered.set(true);
+        }
+
+        public void computeIsLastAMRetry() {
+            isLastAMRetry = appAttemptID.getAttemptId() >= maxAppAttempts;
+        }
+    }
+
+    @SuppressWarnings("unchecked")
+    @Override
+    protected void serviceStart() throws Exception {
+
+        amInfos = new LinkedList<AMInfo>();
+        completedTasksFromPreviousRun = new HashMap<TaskId, TaskInfo>();
+        processRecovery();
+
+        // Current an AMInfo for the current AM generation.
+        AMInfo amInfo =
+                MRBuilderUtils.newAMInfo(appAttemptID, startTime, containerID, nmHost,
+                        nmPort, nmHttpPort);
+
+        // /////////////////// Create the job itself.
+        job = createJob(getConfig(), forcedState, shutDownMessage);
+
+        // End of creating the job.
+
+        // Send out an MR AM inited event for all previous AMs.
+        for (AMInfo info : amInfos) {
+            dispatcher.getEventHandler().handle(
+                    new JobHistoryEvent(job.getID(), new AMStartedEvent(info
+                            .getAppAttemptId(), info.getStartTime(), info.getContainerId(),
+                            info.getNodeManagerHost(), info.getNodeManagerPort(), info
+                            .getNodeManagerHttpPort())));
+        }
+
+        // Send out an MR AM inited event for this AM.
+        dispatcher.getEventHandler().handle(
+                new JobHistoryEvent(job.getID(), new AMStartedEvent(amInfo
+                        .getAppAttemptId(), amInfo.getStartTime(), amInfo.getContainerId(),
+                        amInfo.getNodeManagerHost(), amInfo.getNodeManagerPort(), amInfo
+                        .getNodeManagerHttpPort(), this.forcedState == null ? null
+                        : this.forcedState.toString())));
+        amInfos.add(amInfo);
+
+        // metrics system init is really init & start.
+        // It's more test friendly to put it here.
+        DefaultMetricsSystem.initialize("MRAppMaster");
+
+        if (!errorHappenedShutDown) {
+            // create a job event for job intialization
+            JobEvent initJobEvent = new JobEvent(job.getID(), JobEventType.JOB_INIT);
+            // Send init to the job (this does NOT trigger job execution)
+            // This is a synchronous call, not an event through dispatcher. We want
+            // job-init to be done completely here.
+            jobEventDispatcher.handle(initJobEvent);
+
+
+            // JobImpl's InitTransition is done (call above is synchronous), so the
+            // "uber-decision" (MR-1220) has been made.  Query job and switch to
+            // ubermode if appropriate (by registering different container-allocator
+            // and container-launcher services/event-handlers).
+
+            if (job.isUber()) {
+                speculatorEventDispatcher.disableSpeculation();
+                LOG.info("MRAppMaster uberizing job " + job.getID()
+                        + " in local container (\"uber-AM\") on node "
+                        + nmHost + ":" + nmPort + ".");
+            } else {
+                // send init to speculator only for non-uber jobs.
+                // This won't yet start as dispatcher isn't started yet.
+                dispatcher.getEventHandler().handle(
+                        new SpeculatorEvent(job.getID(), clock.getTime()));
+                LOG.info("MRAppMaster launching normal, non-uberized, multi-container "
+                        + "job " + job.getID() + ".");
+            }
+            // Start ClientService here, since it's not initialized if
+            // errorHappenedShutDown is true
+            clientService.start();
+        }
+        //start all the components
+        super.serviceStart();
+
+        // set job classloader if configured
+        MRApps.setJobClassLoader(getConfig());
+        // All components have started, start the job.
+        startJobs();
+    }
+
+    @Override
+    public void stop() {
+        super.stop();
+        TaskLog.syncLogsShutdown(logSyncer);
+    }
+
+    private void processRecovery() {
+        if (appAttemptID.getAttemptId() == 1) {
+            return;  // no need to recover on the first attempt
+        }
+
+        boolean recoveryEnabled = getConfig().getBoolean(
+                MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE,
+                MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE_DEFAULT);
+        boolean recoverySupportedByCommitter =
+                committer != null && committer.isRecoverySupported();
+
+        // If a shuffle secret was not provided by the job client then this app
+        // attempt will generate one.  However that disables recovery if there
+        // are reducers as the shuffle secret would be app attempt specific.
+        int numReduceTasks = getConfig().getInt(MRJobConfig.NUM_REDUCES, 0);
+        boolean shuffleKeyValidForRecovery =
+                TokenCache.getShuffleSecretKey(jobCredentials) != null;
+
+        if (recoveryEnabled && recoverySupportedByCommitter
+                && (numReduceTasks <= 0 || shuffleKeyValidForRecovery)) {
+            LOG.info("Recovery is enabled. "
+                    + "Will try to recover from previous life on best effort basis.");
+            try {
+                parsePreviousJobHistory();
+            } catch (IOException e) {
+                LOG.warn("Unable to parse prior job history, aborting recovery", e);
+                // try to get just the AMInfos
+                amInfos.addAll(readJustAMInfos());
+            }
+        } else {
+            LOG.info("Will not try to recover. recoveryEnabled: "
+                    + recoveryEnabled + " recoverySupportedByCommitter: "
+                    + recoverySupportedByCommitter + " numReduceTasks: "
+                    + numReduceTasks + " shuffleKeyValidForRecovery: "
+                    + shuffleKeyValidForRecovery + " ApplicationAttemptID: "
+                    + appAttemptID.getAttemptId());
+            // Get the amInfos anyways whether recovery is enabled or not
+            amInfos.addAll(readJustAMInfos());
+        }
+    }
+
+    private static FSDataInputStream getPreviousJobHistoryStream(
+            Configuration conf, ApplicationAttemptId appAttemptId)
+            throws IOException {
+        Path historyFile = JobHistoryUtils.getPreviousJobHistoryPath(conf,
+                appAttemptId);
+        LOG.info("Previous history file is at " + historyFile);
+        return historyFile.getFileSystem(conf).open(historyFile);
+    }
+
+    private void parsePreviousJobHistory() throws IOException {
+        FSDataInputStream in = getPreviousJobHistoryStream(getConfig(),
+                appAttemptID);
+        JobHistoryParser parser = new JobHistoryParser(in);
+        JobInfo jobInfo = parser.parse();
+        Exception parseException = parser.getParseException();
+        if (parseException != null) {
+            LOG.info("Got an error parsing job-history file" +
+                    ", ignoring incomplete events.", parseException);
+        }
+        Map<org.apache.hadoop.mapreduce.TaskID, TaskInfo> taskInfos = jobInfo
+                .getAllTasks();
+        for (TaskInfo taskInfo : taskInfos.values()) {
+            if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {
+                Iterator<Entry<TaskAttemptID, TaskAttemptInfo>> taskAttemptIterator =
+                        taskInfo.getAllTaskAttempts().entrySet().iterator();
+                while (taskAttemptIterator.hasNext()) {
+                    Map.Entry<TaskAttemptID, TaskAttemptInfo> currentEntry = taskAttemptIterator.next();
+                    if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {
+                        taskAttemptIterator.remove();
+                    }
+                }
+                completedTasksFromPreviousRun
+                        .put(TypeConverter.toYarn(taskInfo.getTaskId()), taskInfo);
+                LOG.info("Read from history task "
+                        + TypeConverter.toYarn(taskInfo.getTaskId()));
+            }
+        }
+        LOG.info("Read completed tasks from history "
+                + completedTasksFromPreviousRun.size());
+        recoveredJobStartTime = jobInfo.getLaunchTime();
+
+        // recover AMInfos
+        List<JobHistoryParser.AMInfo> jhAmInfoList = jobInfo.getAMInfos();
+        if (jhAmInfoList != null) {
+            for (JobHistoryParser.AMInfo jhAmInfo : jhAmInfoList) {
+                AMInfo amInfo = MRBuilderUtils.newAMInfo(jhAmInfo.getAppAttemptId(),
+                        jhAmInfo.getStartTime(), jhAmInfo.getContainerId(),
+                        jhAmInfo.getNodeManagerHost(), jhAmInfo.getNodeManagerPort(),
+                        jhAmInfo.getNodeManagerHttpPort());
+                amInfos.add(amInfo);
+            }
+        }
+    }
+
+    private List<AMInfo> readJustAMInfos() {
+        List<AMInfo> amInfos = new ArrayList<AMInfo>();
+        FSDataInputStream inputStream = null;
+        try {
+            inputStream = getPreviousJobHistoryStream(getConfig(), appAttemptID);
+            EventReader jobHistoryEventReader = new EventReader(inputStream);
+
+            // All AMInfos are contiguous. Track when the first AMStartedEvent
+            // appears.
+            boolean amStartedEventsBegan = false;
+
+            HistoryEvent event;
+            while ((event = jobHistoryEventReader.getNextEvent()) != null) {
+                if (event.getEventType() == EventType.AM_STARTED) {
+                    if (!amStartedEventsBegan) {
+                        // First AMStartedEvent.
+                        amStartedEventsBegan = true;
+                    }
+                    AMStartedEvent amStartedEvent = (AMStartedEvent) event;
+                    amInfos.add(MRBuilderUtils.newAMInfo(
+                            amStartedEvent.getAppAttemptId(), amStartedEvent.getStartTime(),
+                            amStartedEvent.getContainerId(),
+                            StringInterner.weakIntern(amStartedEvent.getNodeManagerHost()),
+                            amStartedEvent.getNodeManagerPort(),
+                            amStartedEvent.getNodeManagerHttpPort()));
+                } else if (amStartedEventsBegan) {
+                    // This means AMStartedEvents began and this event is a
+                    // non-AMStarted event.
+                    // No need to continue reading all the other events.
+                    break;
+                }
+            }
+        } catch (IOException e) {
+            LOG.warn("Could not parse the old history file. "
+                    + "Will not have old AMinfos ", e);
+        } finally {
+            if (inputStream != null) {
+                IOUtils.closeQuietly(inputStream);
+            }
+        }
+        return amInfos;
+    }
+
+    /**
+     * This can be overridden to instantiate multiple jobs and create a
+     * workflow.
+     *
+     * TODO:  Rework the design to actually support this.  Currently much of the
+     * job stuff has been moved to init() above to support uberization (MR-1220).
+     * In a typical workflow, one presumably would want to uberize only a subset
+     * of the jobs (the "small" ones), which is awkward with the current design.
+     */
+    @SuppressWarnings("unchecked")
+    protected void startJobs() {
+        /** create a job-start event to get this ball rolling */
+        JobEvent startJobEvent = new JobStartEvent(job.getID(),
+                recoveredJobStartTime);
+        /** send the job-start event. this triggers the job execution. */
+        dispatcher.getEventHandler().handle(startJobEvent);
+    }
+
+    private class JobEventDispatcher implements EventHandler<JobEvent> {
+        @SuppressWarnings("unchecked")
+        @Override
+        public void handle(JobEvent event) {
+            ((EventHandler<JobEvent>)context.getJob(event.getJobId())).handle(event);
+        }
+    }
+
+    private class TaskEventDispatcher implements EventHandler<TaskEvent> {
+        @SuppressWarnings("unchecked")
+        @Override
+        public void handle(TaskEvent event) {
+            Task task = context.getJob(event.getTaskID().getJobId()).getTask(
+                    event.getTaskID());
+            ((EventHandler<TaskEvent>)task).handle(event);
+        }
+    }
+
+    private class TaskAttemptEventDispatcher
+            implements EventHandler<TaskAttemptEvent> {
+        @SuppressWarnings("unchecked")
+        @Override
+        public void handle(TaskAttemptEvent event) {
+            Job job = context.getJob(event.getTaskAttemptID().getTaskId().getJobId());
+            Task task = job.getTask(event.getTaskAttemptID().getTaskId());
+            TaskAttempt attempt = task.getAttempt(event.getTaskAttemptID());
+            ((EventHandler<TaskAttemptEvent>) attempt).handle(event);
+        }
+    }
+
+    private class SpeculatorEventDispatcher implements
+            EventHandler<SpeculatorEvent> {
+        private final Configuration conf;
+        private volatile boolean disabled;
+        public SpeculatorEventDispatcher(Configuration config) {
+            this.conf = config;
+        }
+        @Override
+        public void handle(SpeculatorEvent event) {
+            if (disabled) {
+                return;
+            }
+
+            TaskId tId = event.getTaskID();
+            TaskType tType = null;
+      /* event's TaskId will be null if the event type is JOB_CREATE or
+       * ATTEMPT_STATUS_UPDATE
+       */
+            if (tId != null) {
+                tType = tId.getTaskType();
+            }
+            boolean shouldMapSpec =
+                    conf.getBoolean(MRJobConfig.MAP_SPECULATIVE, false);
+            boolean shouldReduceSpec =
+                    conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE, false);
+
+      /* The point of the following is to allow the MAP and REDUCE speculative
+       * config values to be independent:
+       * IF spec-exec is turned on for maps AND the task is a map task
+       * OR IF spec-exec is turned on for reduces AND the task is a reduce task
+       * THEN call the speculator to handle the event.
+       */
+            if ( (shouldMapSpec && (tType == null || tType == TaskType.MAP))
+                    || (shouldReduceSpec && (tType == null || tType == TaskType.REDUCE))) {
+                // Speculator IS enabled, direct the event to there.
+                speculator.handle(event);
+            }
+        }
+
+        public void disableSpeculation() {
+            disabled = true;
+        }
+
+    }
+
+    /**
+     * Eats events that are not needed in some error cases.
+     */
+    private static class NoopEventHandler implements EventHandler<Event> {
+
+        @Override
+        public void handle(Event event) {
+            //Empty
+        }
+    }
+
+    private static void validateInputParam(String value, String param)
+            throws IOException {
+        if (value == null) {
+            String msg = param + " is null";
+            LOG.error(msg);
+            throw new IOException(msg);
+        }
+    }
+
+    public static void main(String[] args) {
+        try {
+            GranularLoggerStore.load();
+
+            Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());
+            String containerIdStr =
+                    System.getenv(Environment.CONTAINER_ID.name());
+            String nodeHostString = System.getenv(Environment.NM_HOST.name());
+            String nodePortString = System.getenv(Environment.NM_PORT.name());
+            String nodeHttpPortString =
+                    System.getenv(Environment.NM_HTTP_PORT.name());
+            String appSubmitTimeStr =
+                    System.getenv(ApplicationConstants.APP_SUBMIT_TIME_ENV);
+            String maxAppAttempts =
+                    System.getenv(ApplicationConstants.MAX_APP_ATTEMPTS_ENV);
+
+            validateInputParam(containerIdStr,
+                    Environment.CONTAINER_ID.name());
+            validateInputParam(nodeHostString, Environment.NM_HOST.name());
+            validateInputParam(nodePortString, Environment.NM_PORT.name());
+            validateInputParam(nodeHttpPortString,
+                    Environment.NM_HTTP_PORT.name());
+            validateInputParam(appSubmitTimeStr,
+                    ApplicationConstants.APP_SUBMIT_TIME_ENV);
+            validateInputParam(maxAppAttempts,
+                    ApplicationConstants.MAX_APP_ATTEMPTS_ENV);
+
+            ContainerId containerId = ConverterUtils.toContainerId(containerIdStr);
+            ApplicationAttemptId applicationAttemptId =
+                    containerId.getApplicationAttemptId();
+            long appSubmitTime = Long.parseLong(appSubmitTimeStr);
+
+
+            MRAppMaster appMaster =
+                    new MRAppMaster(applicationAttemptId, containerId, nodeHostString,
+                            Integer.parseInt(nodePortString),
+                            Integer.parseInt(nodeHttpPortString), appSubmitTime,
+                            Integer.parseInt(maxAppAttempts));
+            ShutdownHookManager.get().addShutdownHook(
+                    new MRAppMasterShutdownHook(appMaster), SHUTDOWN_HOOK_PRIORITY);
+            JobConf conf = new JobConf(new YarnConfiguration());
+            conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));
+
+            MRWebAppUtil.initialize(conf);
+            String jobUserName = System
+                    .getenv(ApplicationConstants.Environment.USER.name());
+            conf.set(MRJobConfig.USER_NAME, jobUserName);
+            // Do not automatically close FileSystem objects so that in case of
+            // SIGTERM I have a chance to write out the job history. I'll be closing
+            // the objects myself.
+            conf.setBoolean("fs.automatic.close", false);
+            initAndStartAppMaster(appMaster, conf, jobUserName);
+        } catch (Throwable t) {
+            LOG.fatal("Error starting MRAppMaster", t);
+            ExitUtil.terminate(1, t);
+        }
+    }
+
+    // The shutdown hook that runs when a signal is received AND during normal
+    // close of the JVM.
+    static class MRAppMasterShutdownHook implements Runnable {
+        MRAppMaster appMaster;
+        MRAppMasterShutdownHook(MRAppMaster appMaster) {
+            this.appMaster = appMaster;
+        }
+        public void run() {
+            LOG.info("MRAppMaster received a signal. Signaling RMCommunicator and "
+                    + "JobHistoryEventHandler.");
+
+            // Notify the JHEH and RMCommunicator that a SIGTERM has been received so
+            // that they don't take too long in shutting down
+            if(appMaster.containerAllocator instanceof ContainerAllocatorRouter) {
+                ((ContainerAllocatorRouter) appMaster.containerAllocator)
+                        .setSignalled(true);
+            }
+            appMaster.notifyIsLastAMRetry(appMaster.isLastAMRetry);
+            appMaster.stop();
+        }
+    }
+
+    public void notifyIsLastAMRetry(boolean isLastAMRetry){
+        if(containerAllocator instanceof ContainerAllocatorRouter) {
+            LOG.info("Notify RMCommunicator isAMLastRetry: " + isLastAMRetry);
+            ((ContainerAllocatorRouter) containerAllocator)
+                    .setShouldUnregister(isLastAMRetry);
+        }
+        if(jobHistoryEventHandler != null) {
+            LOG.info("Notify JHEH isAMLastRetry: " + isLastAMRetry);
+            jobHistoryEventHandler.setForcejobCompletion(isLastAMRetry);
+        }
+    }
+
+    protected static void initAndStartAppMaster(final MRAppMaster appMaster,
+                                                final JobConf conf, String jobUserName) throws IOException,
+            InterruptedException {
+        UserGroupInformation.setConfiguration(conf);
+        // Security framework already loaded the tokens into current UGI, just use
+        // them
+        Credentials credentials =
+                UserGroupInformation.getCurrentUser().getCredentials();
+        LOG.info("Executing with tokens:");
+        for (Token<?> token : credentials.getAllTokens()) {
+            LOG.info(token);
+        }
+
+        UserGroupInformation appMasterUgi = UserGroupInformation
+                .createRemoteUser(jobUserName);
+        appMasterUgi.addCredentials(credentials);
+
+        // Now remove the AM->RM token so tasks don't have it
+        Iterator<Token<?>> iter = credentials.getAllTokens().iterator();
+        while (iter.hasNext()) {
+            Token<?> token = iter.next();
+            if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {
+                iter.remove();
+            }
+        }
+        conf.getCredentials().addAll(credentials);
+        appMasterUgi.doAs(new PrivilegedExceptionAction<Object>() {
+            @Override
+            public Object run() throws Exception {
+                appMaster.init(conf);
+                appMaster.start();
+                if(appMaster.errorHappenedShutDown) {
+                    throw new IOException("Was asked to shut down.");
+                }
+                return null;
+            }
+        });
+    }
+
+    @Override
+    protected void serviceStop() throws Exception {
+        super.serviceStop();
+        LogManager.shutdown();
+    }
+
+    public ClientService getClientService() {
+        return clientService;
+    }
+}
diff --git a/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java b/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
new file mode 100644
index 0000000..ee14631
--- /dev/null
+++ b/hadoop-client/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java
@@ -0,0 +1,456 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.mapreduce.v2.app.rm;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.MRJobConfig;
+import org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId;
+import org.apache.hadoop.mapreduce.v2.app.AppContext;
+import org.apache.hadoop.mapreduce.v2.app.client.ClientService;
+import org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest;
+import org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse;
+import org.apache.hadoop.yarn.api.records.ContainerId;
+import org.apache.hadoop.yarn.api.records.Priority;
+import org.apache.hadoop.yarn.api.records.Resource;
+import org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest;
+import org.apache.hadoop.yarn.api.records.ResourceRequest;
+import org.apache.hadoop.yarn.exceptions.YarnException;
+import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;
+import org.apache.hadoop.yarn.factories.RecordFactory;
+import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;
+
+
+/**
+ * Keeps the data structures to send container requests to RM.
+ */
+public abstract class RMContainerRequestor extends RMCommunicator {
+
+    private static final Log LOG = LogFactory.getLog(RMContainerRequestor.class);
+
+    private int lastResponseID;
+    private Resource availableResources;
+
+    private final RecordFactory recordFactory =
+            RecordFactoryProvider.getRecordFactory(null);
+    //Key -> Priority
+    //Value -> Map
+    //Key->ResourceName (e.g., hostname, rackname, *)
+    //Value->Map
+    //Key->Resource Capability
+    //Value->ResourceRequest
+    private final Map<Priority, Map<String, Map<Resource, ResourceRequest>>>
+            remoteRequestsTable =
+            new TreeMap<Priority, Map<String, Map<Resource, ResourceRequest>>>();
+
+    // use custom comparator to make sure ResourceRequest objects differing only in
+    // numContainers dont end up as duplicates
+    private final Set<ResourceRequest> ask = new TreeSet<ResourceRequest>(
+            new org.apache.hadoop.yarn.api.records.ResourceRequest.ResourceRequestComparator());
+    private final Set<ContainerId> release = new TreeSet<ContainerId>();
+
+    private boolean nodeBlacklistingEnabled;
+    private int blacklistDisablePercent;
+    private AtomicBoolean ignoreBlacklisting = new AtomicBoolean(false);
+    private int blacklistedNodeCount = 0;
+    private int lastClusterNmCount = 0;
+    private int clusterNmCount = 0;
+    private int maxTaskFailuresPerNode;
+    private final Map<String, Integer> nodeFailures = new HashMap<String, Integer>();
+    private final Set<String> blacklistedNodes = Collections
+            .newSetFromMap(new ConcurrentHashMap<String, Boolean>());
+    private final Set<String> blacklistAdditions = Collections
+            .newSetFromMap(new ConcurrentHashMap<String, Boolean>());
+    private final Set<String> blacklistRemovals = Collections
+            .newSetFromMap(new ConcurrentHashMap<String, Boolean>());
+
+    public RMContainerRequestor(ClientService clientService, AppContext context) {
+        super(clientService, context);
+    }
+
+    static class ContainerRequest {
+        final TaskAttemptId attemptID;
+        final Resource capability;
+        final String[] hosts;
+        final String[] racks;
+        //final boolean earlierAttemptFailed;
+        final Priority priority;
+
+        public ContainerRequest(ContainerRequestEvent event, Priority priority) {
+            this(event.getAttemptID(), event.getCapability(), event.getHosts(),
+                    event.getRacks(), priority);
+        }
+
+        public ContainerRequest(TaskAttemptId attemptID,
+                                Resource capability, String[] hosts, String[] racks,
+                                Priority priority) {
+            this.attemptID = attemptID;
+            this.capability = capability;
+            this.hosts = hosts;
+            this.racks = racks;
+            this.priority = priority;
+        }
+
+        public String toString() {
+            StringBuilder sb = new StringBuilder();
+            sb.append("AttemptId[").append(attemptID).append("]");
+            sb.append("Capability[").append(capability).append("]");
+            sb.append("Priority[").append(priority).append("]");
+            return sb.toString();
+        }
+    }
+
+    @Override
+    protected void serviceInit(Configuration conf) throws Exception {
+        super.serviceInit(conf);
+        nodeBlacklistingEnabled =
+                conf.getBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);
+        LOG.info("nodeBlacklistingEnabled:" + nodeBlacklistingEnabled);
+        maxTaskFailuresPerNode =
+                conf.getInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 3);
+        blacklistDisablePercent =
+                conf.getInt(
+                        MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT,
+                        MRJobConfig.DEFAULT_MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERCENT);
+        LOG.info("maxTaskFailuresPerNode is " + maxTaskFailuresPerNode);
+        if (blacklistDisablePercent < -1 || blacklistDisablePercent > 100) {
+            throw new YarnRuntimeException("Invalid blacklistDisablePercent: "
+                    + blacklistDisablePercent
+                    + ". Should be an integer between 0 and 100 or -1 to disabled");
+        }
+        LOG.info("blacklistDisablePercent is " + blacklistDisablePercent);
+    }
+
+    protected AllocateResponse makeRemoteRequest() throws IOException {
+        ResourceBlacklistRequest blacklistRequest =
+                ResourceBlacklistRequest.newInstance(new ArrayList<String>(blacklistAdditions),
+                        new ArrayList<String>(blacklistRemovals));
+        AllocateRequest allocateRequest =
+                AllocateRequest.newInstance(lastResponseID,
+                        super.getApplicationProgress(), new ArrayList<ResourceRequest>(ask),
+                        new ArrayList<ContainerId>(release), blacklistRequest);
+        AllocateResponse allocateResponse;
+        try {
+            allocateResponse = scheduler.allocate(allocateRequest);
+        } catch (YarnException e) {
+            throw new IOException(e);
+        }
+        lastResponseID = allocateResponse.getResponseId();
+        availableResources = allocateResponse.getAvailableResources();
+        lastClusterNmCount = clusterNmCount;
+        clusterNmCount = allocateResponse.getNumClusterNodes();
+
+        if (ask.size() > 0 || release.size() > 0) {
+            LOG.info("getResources() for " + applicationId + ":" + " ask="
+                    + ask.size() + " release= " + release.size() + " newContainers="
+                    + allocateResponse.getAllocatedContainers().size()
+                    + " finishedContainers="
+                    + allocateResponse.getCompletedContainersStatuses().size()
+                    + " resourcelimit=" + availableResources + " knownNMs="
+                    + clusterNmCount);
+        }
+
+        ask.clear();
+        release.clear();
+
+        if (blacklistAdditions.size() > 0 || blacklistRemovals.size() > 0) {
+            LOG.info("Update the blacklist for " + applicationId +
+                    ": blacklistAdditions=" + blacklistAdditions.size() +
+                    " blacklistRemovals=" +  blacklistRemovals.size());
+        }
+        blacklistAdditions.clear();
+        blacklistRemovals.clear();
+        return allocateResponse;
+    }
+
+    // May be incorrect if there's multiple NodeManagers running on a single host.
+    // knownNodeCount is based on node managers, not hosts. blacklisting is
+    // currently based on hosts.
+    protected void computeIgnoreBlacklisting() {
+        if (!nodeBlacklistingEnabled) {
+            return;
+        }
+        if (blacklistDisablePercent != -1
+                && (blacklistedNodeCount != blacklistedNodes.size() ||
+                clusterNmCount != lastClusterNmCount)) {
+            blacklistedNodeCount = blacklistedNodes.size();
+            if (clusterNmCount == 0) {
+                LOG.info("KnownNode Count at 0. Not computing ignoreBlacklisting");
+                return;
+            }
+            int val = (int) ((float) blacklistedNodes.size() / clusterNmCount * 100);
+            if (val >= blacklistDisablePercent) {
+                if (ignoreBlacklisting.compareAndSet(false, true)) {
+                    LOG.info("Ignore blacklisting set to true. Known: " + clusterNmCount
+                            + ", Blacklisted: " + blacklistedNodeCount + ", " + val + "%");
+                    // notify RM to ignore all the blacklisted nodes
+                    blacklistAdditions.clear();
+                    blacklistRemovals.addAll(blacklistedNodes);
+                }
+            } else {
+                if (ignoreBlacklisting.compareAndSet(true, false)) {
+                    LOG.info("Ignore blacklisting set to false. Known: " + clusterNmCount
+                            + ", Blacklisted: " + blacklistedNodeCount + ", " + val + "%");
+                    // notify RM of all the blacklisted nodes
+                    blacklistAdditions.addAll(blacklistedNodes);
+                    blacklistRemovals.clear();
+                }
+            }
+        }
+    }
+
+    protected void containerFailedOnHost(String hostName) {
+        if (!nodeBlacklistingEnabled) {
+            return;
+        }
+        if (blacklistedNodes.contains(hostName)) {
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("Host " + hostName + " is already blacklisted.");
+            }
+            return; //already blacklisted
+        }
+        Integer failures = nodeFailures.remove(hostName);
+        failures = failures == null ? Integer.valueOf(0) : failures;
+        failures++;
+        LOG.info(failures + " failures on node " + hostName);
+        if (failures >= maxTaskFailuresPerNode) {
+            blacklistedNodes.add(hostName);
+            if (!ignoreBlacklisting.get()) {
+                blacklistAdditions.add(hostName);
+            }
+            //Even if blacklisting is ignored, continue to remove the host from
+            // the request table. The RM may have additional nodes it can allocate on.
+            LOG.info("Blacklisted host " + hostName);
+
+            //remove all the requests corresponding to this hostname
+            for (Map<String, Map<Resource, ResourceRequest>> remoteRequests
+                    : remoteRequestsTable.values()){
+                //remove from host if no pending allocations
+                boolean foundAll = true;
+                Map<Resource, ResourceRequest> reqMap = remoteRequests.get(hostName);
+                if (reqMap != null) {
+                    for (ResourceRequest req : reqMap.values()) {
+                        if (!ask.remove(req)) {
+                            foundAll = false;
+                            // if ask already sent to RM, we can try and overwrite it if possible.
+                            // send a new ask to RM with numContainers
+                            // specified for the blacklisted host to be 0.
+                            ResourceRequest zeroedRequest =
+                                    ResourceRequest.newInstance(req.getPriority(),
+                                            req.getResourceName(), req.getCapability(),
+                                            req.getNumContainers(), req.getRelaxLocality());
+
+                            zeroedRequest.setNumContainers(0);
+                            // to be sent to RM on next heartbeat
+                            addResourceRequestToAsk(zeroedRequest);
+                        }
+                    }
+                    // if all requests were still in ask queue
+                    // we can remove this request
+                    if (foundAll) {
+                        remoteRequests.remove(hostName);
+                    }
+                }
+                // TODO handling of rack blacklisting
+                // Removing from rack should be dependent on no. of failures within the rack
+                // Blacklisting a rack on the basis of a single node's blacklisting
+                // may be overly aggressive.
+                // Node failures could be co-related with other failures on the same rack
+                // but we probably need a better approach at trying to decide how and when
+                // to blacklist a rack
+            }
+        } else {
+            nodeFailures.put(hostName, failures);
+        }
+    }
+
+    protected Resource getAvailableResources() {
+        return availableResources;
+    }
+
+    protected void addContainerReq(ContainerRequest req) {
+        // Create resource requests
+        for (String host : req.hosts) {
+            // Data-local
+            if (!isNodeBlacklisted(host)) {
+                addResourceRequest(req.priority, host, req.capability);
+            }
+        }
+
+        // Nothing Rack-local for now
+        for (String rack : req.racks) {
+            addResourceRequest(req.priority, rack, req.capability);
+        }
+
+        // Off-switch
+        addResourceRequest(req.priority, ResourceRequest.ANY, req.capability);
+    }
+
+    protected void decContainerReq(ContainerRequest req) {
+        // Update resource requests
+        for (String hostName : req.hosts) {
+            decResourceRequest(req.priority, hostName, req.capability);
+        }
+
+        for (String rack : req.racks) {
+            decResourceRequest(req.priority, rack, req.capability);
+        }
+
+        decResourceRequest(req.priority, ResourceRequest.ANY, req.capability);
+    }
+
+    private void addResourceRequest(Priority priority, String resourceName,
+                                    Resource capability) {
+        Map<String, Map<Resource, ResourceRequest>> remoteRequests =
+                this.remoteRequestsTable.get(priority);
+        if (remoteRequests == null) {
+            remoteRequests = new HashMap<String, Map<Resource, ResourceRequest>>();
+            this.remoteRequestsTable.put(priority, remoteRequests);
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("Added priority=" + priority);
+            }
+        }
+        Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);
+        if (reqMap == null) {
+            reqMap = new HashMap<Resource, ResourceRequest>();
+            remoteRequests.put(resourceName, reqMap);
+        }
+        ResourceRequest remoteRequest = reqMap.get(capability);
+        if (remoteRequest == null) {
+            remoteRequest = recordFactory.newRecordInstance(ResourceRequest.class);
+            remoteRequest.setPriority(priority);
+            remoteRequest.setResourceName(resourceName);
+            remoteRequest.setCapability(capability);
+            remoteRequest.setNumContainers(0);
+            reqMap.put(capability, remoteRequest);
+        }
+        remoteRequest.setNumContainers(remoteRequest.getNumContainers() + 1);
+
+        // Note this down for next interaction with ResourceManager
+        addResourceRequestToAsk(remoteRequest);
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("addResourceRequest:" + " applicationId="
+                    + applicationId.getId() + " priority=" + priority.getPriority()
+                    + " resourceName=" + resourceName + " numContainers="
+                    + remoteRequest.getNumContainers() + " #asks=" + ask.size());
+        }
+    }
+
+    private void decResourceRequest(Priority priority, String resourceName,
+                                    Resource capability) {
+        Map<String, Map<Resource, ResourceRequest>> remoteRequests =
+                this.remoteRequestsTable.get(priority);
+        Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);
+        if (reqMap == null) {
+            // as we modify the resource requests by filtering out blacklisted hosts
+            // when they are added, this value may be null when being
+            // decremented
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("Not decrementing resource as " + resourceName
+                        + " is not present in request table");
+            }
+            return;
+        }
+        ResourceRequest remoteRequest = reqMap.get(capability);
+
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("BEFORE decResourceRequest:" + " applicationId="
+                    + applicationId.getId() + " priority=" + priority.getPriority()
+                    + " resourceName=" + resourceName + " numContainers="
+                    + remoteRequest.getNumContainers() + " #asks=" + ask.size());
+        }
+
+        if(remoteRequest.getNumContainers() > 0) {
+            // based on blacklisting comments above we can end up decrementing more
+            // than requested. so guard for that.
+            remoteRequest.setNumContainers(remoteRequest.getNumContainers() -1);
+        }
+
+        if (remoteRequest.getNumContainers() == 0) {
+            reqMap.remove(capability);
+            if (reqMap.size() == 0) {
+                remoteRequests.remove(resourceName);
+            }
+            if (remoteRequests.size() == 0) {
+                remoteRequestsTable.remove(priority);
+            }
+        }
+
+        // send the updated resource request to RM
+        // send 0 container count requests also to cancel previous requests
+        addResourceRequestToAsk(remoteRequest);
+
+        if (LOG.isDebugEnabled()) {
+            LOG.info("AFTER decResourceRequest:" + " applicationId="
+                    + applicationId.getId() + " priority=" + priority.getPriority()
+                    + " resourceName=" + resourceName + " numContainers="
+                    + remoteRequest.getNumContainers() + " #asks=" + ask.size());
+        }
+    }
+
+    private void addResourceRequestToAsk(ResourceRequest remoteRequest) {
+        // because objects inside the resource map can be deleted ask can end up
+        // containing an object that matches new resource object but with different
+        // numContainers. So exisintg values must be replaced explicitly
+        if(ask.contains(remoteRequest)) {
+            ask.remove(remoteRequest);
+        }
+        ask.add(remoteRequest);
+    }
+
+    protected void release(ContainerId containerId) {
+        release.add(containerId);
+    }
+
+    protected boolean isNodeBlacklisted(String hostname) {
+        if (!nodeBlacklistingEnabled || ignoreBlacklisting.get()) {
+            return false;
+        }
+        return blacklistedNodes.contains(hostname);
+    }
+
+    protected ContainerRequest getFilteredContainerRequest(ContainerRequest orig) {
+        ArrayList<String> newHosts = new ArrayList<String>();
+        for (String host : orig.hosts) {
+            if (!isNodeBlacklisted(host)) {
+                newHosts.add(host);
+            }
+        }
+        String[] hosts = newHosts.toArray(new String[newHosts.size()]);
+        ContainerRequest newReq = new ContainerRequest(orig.attemptID, orig.capability,
+                hosts, orig.racks, orig.priority);
+        return newReq;
+    }
+
+    public Set<String> getBlacklistedNodes() {
+        return blacklistedNodes;
+    }
+}
diff --git a/hadoop-common-project/hadoop-annotations/pom.xml b/hadoop-common-project/hadoop-annotations/pom.xml
index c3e1aa1..c5e845a 100644
--- a/hadoop-common-project/hadoop-annotations/pom.xml
+++ b/hadoop-common-project/hadoop-annotations/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-annotations</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Annotations</description>
   <name>Apache Hadoop Annotations</name>
   <packaging>jar</packaging>
diff --git a/hadoop-common-project/hadoop-auth-examples/pom.xml b/hadoop-common-project/hadoop-auth-examples/pom.xml
index e005f32..f4b6f7f 100644
--- a/hadoop-common-project/hadoop-auth-examples/pom.xml
+++ b/hadoop-common-project/hadoop-auth-examples/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-auth-examples</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>war</packaging>
 
   <name>Apache Hadoop Auth Examples</name>
diff --git a/hadoop-common-project/hadoop-auth/pom.xml b/hadoop-common-project/hadoop-auth/pom.xml
index b9d6c60..4717f10 100644
--- a/hadoop-common-project/hadoop-auth/pom.xml
+++ b/hadoop-common-project/hadoop-auth/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-auth</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>jar</packaging>
 
   <name>Apache Hadoop Auth</name>
diff --git a/hadoop-common-project/hadoop-common/pom.xml b/hadoop-common-project/hadoop-common/pom.xml
index 7cf67a3..7b8fbaa 100644
--- a/hadoop-common-project/hadoop-common/pom.xml
+++ b/hadoop-common-project/hadoop-common/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project-dist</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project-dist</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-common</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Common</description>
   <name>Apache Hadoop Common</name>
   <packaging>jar</packaging>
diff --git a/hadoop-common-project/hadoop-minikdc/pom.xml b/hadoop-common-project/hadoop-minikdc/pom.xml
index b42a435..c7d4c74 100644
--- a/hadoop-common-project/hadoop-minikdc/pom.xml
+++ b/hadoop-common-project/hadoop-minikdc/pom.xml
@@ -18,13 +18,13 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-minikdc</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop MiniKDC</description>
   <name>Apache Hadoop MiniKDC</name>
   <packaging>jar</packaging>
diff --git a/hadoop-common-project/hadoop-nfs/pom.xml b/hadoop-common-project/hadoop-nfs/pom.xml
index f1ad185..806b63f 100644
--- a/hadoop-common-project/hadoop-nfs/pom.xml
+++ b/hadoop-common-project/hadoop-nfs/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-nfs</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>jar</packaging>
 
   <name>Apache Hadoop NFS</name>
diff --git a/hadoop-common-project/pom.xml b/hadoop-common-project/pom.xml
index ac650a8..197846d 100644
--- a/hadoop-common-project/pom.xml
+++ b/hadoop-common-project/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-common-project</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Common Project</description>
   <name>Apache Hadoop Common Project</name>
   <packaging>pom</packaging>
diff --git a/hadoop-dist/pom.xml b/hadoop-dist/pom.xml
index ced3c5c..907a488 100644
--- a/hadoop-dist/pom.xml
+++ b/hadoop-dist/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-dist</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Distribution</description>
   <name>Apache Hadoop Distribution</name>
   <packaging>jar</packaging>
diff --git a/hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml b/hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml
index d01a32f..c4263b3 100644
--- a/hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs-httpfs/pom.xml
@@ -22,12 +22,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-hdfs-httpfs</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>war</packaging>
 
   <name>Apache Hadoop HttpFS</name>
diff --git a/hadoop-hdfs-project/hadoop-hdfs-nfs/pom.xml b/hadoop-hdfs-project/hadoop-hdfs-nfs/pom.xml
index 5ee5841..0d5b1a7 100644
--- a/hadoop-hdfs-project/hadoop-hdfs-nfs/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs-nfs/pom.xml
@@ -20,12 +20,12 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-hdfs-nfs</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop HDFS-NFS</description>
   <name>Apache Hadoop HDFS-NFS</name>
   <packaging>jar</packaging>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/pom.xml b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
index 420e5d2..6345878 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs/pom.xml
@@ -20,12 +20,12 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project-dist</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project-dist</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-hdfs</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop HDFS</description>
   <name>Apache Hadoop HDFS</name>
   <packaging>jar</packaging>
diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/pom.xml b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/pom.xml
index 9b267fed..9bf670d 100644
--- a/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/pom.xml
+++ b/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/pom.xml
@@ -20,13 +20,13 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../../../../hadoop-project</relativePath>
   </parent>
 
   <groupId>org.apache.hadoop.contrib</groupId>
   <artifactId>hadoop-hdfs-bkjournal</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop HDFS BookKeeper Journal</description>
   <name>Apache Hadoop HDFS BookKeeper Journal</name>
   <packaging>jar</packaging>
diff --git a/hadoop-hdfs-project/pom.xml b/hadoop-hdfs-project/pom.xml
index f75bf240..380358b 100644
--- a/hadoop-hdfs-project/pom.xml
+++ b/hadoop-hdfs-project/pom.xml
@@ -20,12 +20,12 @@ http://maven.apache.org/xsd/maven-4.0.0.xsd">
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-hdfs-project</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop HDFS Project</description>
   <name>Apache Hadoop HDFS Project</name>
   <packaging>pom</packaging>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml
index 79e7cca..f708afe 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-mapreduce-client</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client-app</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client-app</name>
 
   <properties>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/pom.xml
index 3922e74..6240394 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-mapreduce-client</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client-common</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client-common</name>
 
   <properties>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/pom.xml
index 2ca2886..540db81 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-mapreduce-client</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client-core</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client-core</name>
 
   <properties>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs-plugins/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs-plugins/pom.xml
index 986d8ef..299ca90 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs-plugins/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs-plugins/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-mapreduce-client</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client-hs-plugins</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client-hs-plugins</name>
 
   <properties>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/pom.xml
index ec776b7..9a03564 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-mapreduce-client</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client-hs</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client-hs</name>
 
   <properties>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/pom.xml
index 911c638..b541946 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-mapreduce-client</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client-jobclient</name>
 
   <properties>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml
index df750c5..c299ee2 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-mapreduce-client</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client-shuffle</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client-shuffle</name>
 
   <properties>
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-client/pom.xml
index 8ae5809..d846ad4 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-client</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-mapreduce-client</name>
   <packaging>pom</packaging>
 
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-examples/pom.xml b/hadoop-mapreduce-project/hadoop-mapreduce-examples/pom.xml
index 5f3a5a6..ccdf502 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-examples/pom.xml
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-examples/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce-examples</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop MapReduce Examples</description>
   <name>Apache Hadoop MapReduce Examples</name>
   <packaging>jar</packaging>
diff --git a/hadoop-mapreduce-project/pom.xml b/hadoop-mapreduce-project/pom.xml
index 8f1d2b0..d2ef7c4 100644
--- a/hadoop-mapreduce-project/pom.xml
+++ b/hadoop-mapreduce-project/pom.xml
@@ -18,12 +18,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-mapreduce</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>pom</packaging>
   <name>hadoop-mapreduce</name>
   <url>http://hadoop.apache.org/mapreduce/</url>
diff --git a/hadoop-maven-plugins/pom.xml b/hadoop-maven-plugins/pom.xml
index 448ee3f..4313664 100644
--- a/hadoop-maven-plugins/pom.xml
+++ b/hadoop-maven-plugins/pom.xml
@@ -19,7 +19,7 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
diff --git a/hadoop-minicluster/pom.xml b/hadoop-minicluster/pom.xml
index 60bbfe7..8198393 100644
--- a/hadoop-minicluster/pom.xml
+++ b/hadoop-minicluster/pom.xml
@@ -18,12 +18,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-minicluster</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>jar</packaging>
 
   <description>Apache Hadoop Mini-Cluster</description>
diff --git a/hadoop-project-dist/pom.xml b/hadoop-project-dist/pom.xml
index bd3c555..c46afd9 100644
--- a/hadoop-project-dist/pom.xml
+++ b/hadoop-project-dist/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-project-dist</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Project Dist POM</description>
   <name>Apache Hadoop Project Dist POM</name>
   <packaging>pom</packaging>
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index b315e2b..19a7941 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -20,11 +20,11 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-main</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-project</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Project POM</description>
   <name>Apache Hadoop Project POM</name>
   <packaging>pom</packaging>
diff --git a/hadoop-tools/hadoop-archives/pom.xml b/hadoop-tools/hadoop-archives/pom.xml
index 76bc2ff..0ee41db 100644
--- a/hadoop-tools/hadoop-archives/pom.xml
+++ b/hadoop-tools/hadoop-archives/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-archives</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Archives</description>
   <name>Apache Hadoop Archives</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-datajoin/pom.xml b/hadoop-tools/hadoop-datajoin/pom.xml
index a934ebb..374cf2c 100644
--- a/hadoop-tools/hadoop-datajoin/pom.xml
+++ b/hadoop-tools/hadoop-datajoin/pom.xml
@@ -17,12 +17,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-datajoin</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Data Join</description>
   <name>Apache Hadoop Data Join</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-distcp/pom.xml b/hadoop-tools/hadoop-distcp/pom.xml
index e4781bf..de84db4 100644
--- a/hadoop-tools/hadoop-distcp/pom.xml
+++ b/hadoop-tools/hadoop-distcp/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-distcp</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Distributed Copy</description>
   <name>Apache Hadoop Distributed Copy</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-extras/pom.xml b/hadoop-tools/hadoop-extras/pom.xml
index bea7109..9435a2d 100644
--- a/hadoop-tools/hadoop-extras/pom.xml
+++ b/hadoop-tools/hadoop-extras/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-extras</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Extras</description>
   <name>Apache Hadoop Extras</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-gridmix/pom.xml b/hadoop-tools/hadoop-gridmix/pom.xml
index b5af535..d3ff6d0 100644
--- a/hadoop-tools/hadoop-gridmix/pom.xml
+++ b/hadoop-tools/hadoop-gridmix/pom.xml
@@ -17,12 +17,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-gridmix</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Gridmix</description>
   <name>Apache Hadoop Gridmix</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-openstack/pom.xml b/hadoop-tools/hadoop-openstack/pom.xml
index 2701e15..307419c 100644
--- a/hadoop-tools/hadoop-openstack/pom.xml
+++ b/hadoop-tools/hadoop-openstack/pom.xml
@@ -19,11 +19,11 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <artifactId>hadoop-openstack</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>Apache Hadoop OpenStack support</name>
   <description>
     This module contains code to support integration with OpenStack.
diff --git a/hadoop-tools/hadoop-pipes/pom.xml b/hadoop-tools/hadoop-pipes/pom.xml
index 7b0f397..a4279d4 100644
--- a/hadoop-tools/hadoop-pipes/pom.xml
+++ b/hadoop-tools/hadoop-pipes/pom.xml
@@ -17,12 +17,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-pipes</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Pipes</description>
   <name>Apache Hadoop Pipes</name>
   <packaging>pom</packaging>
diff --git a/hadoop-tools/hadoop-rumen/pom.xml b/hadoop-tools/hadoop-rumen/pom.xml
index 2243f2e..4a902ac 100644
--- a/hadoop-tools/hadoop-rumen/pom.xml
+++ b/hadoop-tools/hadoop-rumen/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-rumen</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Rumen</description>
   <name>Apache Hadoop Rumen</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-sls/pom.xml b/hadoop-tools/hadoop-sls/pom.xml
index 6166725..20f9b22 100644
--- a/hadoop-tools/hadoop-sls/pom.xml
+++ b/hadoop-tools/hadoop-sls/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-sls</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Scheduler Load Simulator</description>
   <name>Apache Hadoop Scheduler Load Simulator</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-streaming/pom.xml b/hadoop-tools/hadoop-streaming/pom.xml
index 7f5cfb6..b03b40d 100644
--- a/hadoop-tools/hadoop-streaming/pom.xml
+++ b/hadoop-tools/hadoop-streaming/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-streaming</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop MapReduce Streaming</description>
   <name>Apache Hadoop MapReduce Streaming</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/hadoop-tools-dist/pom.xml b/hadoop-tools/hadoop-tools-dist/pom.xml
index d09b476..38843cb 100644
--- a/hadoop-tools/hadoop-tools-dist/pom.xml
+++ b/hadoop-tools/hadoop-tools-dist/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project-dist</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project-dist</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-tools-dist</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Tools Dist</description>
   <name>Apache Hadoop Tools Dist</name>
   <packaging>jar</packaging>
diff --git a/hadoop-tools/pom.xml b/hadoop-tools/pom.xml
index 13618c3..d17716d 100644
--- a/hadoop-tools/pom.xml
+++ b/hadoop-tools/pom.xml
@@ -20,12 +20,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-tools</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Tools</description>
   <name>Apache Hadoop Tools</name>
   <packaging>pom</packaging>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/pom.xml
index fe2955a..a65bd44 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-api</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-api</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
index c639de8..991118c 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn-applications</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-applications-distributedshell</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-applications-distributedshell</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/pom.xml
index 35d1a42..ab2a12c 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-unmanaged-am-launcher/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn-applications</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-applications-unmanaged-am-launcher</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-applications-unmanaged-am-launcher</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
index ce21d42..39c96dd 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-applications</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-applications</name>
   <packaging>pom</packaging>
 
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/pom.xml
index 82d66cb..d7720ef 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/pom.xml
@@ -17,11 +17,11 @@
   <parent>
     <artifactId>hadoop-yarn</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-client</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-client</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml
index a19a78c..23c8aed 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-common</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-common</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/pom.xml
index 8a4e6f5..8ff2db8 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/pom.xml
@@ -22,12 +22,12 @@
   <parent>
     <artifactId>hadoop-yarn-server</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-server-applicationhistoryservice</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-server-applicationhistoryservice</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/pom.xml
index 294f969..449a83a 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn-server</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-server-common</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-server-common</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/pom.xml
index 0fbafd2..b1cedde 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn-server</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-server-nodemanager</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-server-nodemanager</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/pom.xml
index 3e78e02..37e08e6 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn-server</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-server-resourcemanager</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml
index 44076eb..ac115e7 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml
@@ -19,11 +19,11 @@
   <parent>
     <artifactId>hadoop-yarn-server</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-server-tests</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-server-tests</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/pom.xml
index 10f243c..7aeff10 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn-server</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-server-web-proxy</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-server-web-proxy</name>
 
   <properties>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/pom.xml
index f946588..0043832 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-server</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-server</name>
   <packaging>pom</packaging>
 
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/pom.xml
index 974f31e..7a1e0cb 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/pom.xml
@@ -19,12 +19,12 @@
   <parent>
     <artifactId>hadoop-yarn</artifactId>
     <groupId>org.apache.hadoop</groupId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
   </parent>
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-site</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <name>hadoop-yarn-site</name>
   <packaging>pom</packaging>
 
diff --git a/hadoop-yarn-project/hadoop-yarn/pom.xml b/hadoop-yarn-project/hadoop-yarn/pom.xml
index fe653b0..7d397e0 100644
--- a/hadoop-yarn-project/hadoop-yarn/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/pom.xml
@@ -16,12 +16,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>pom</packaging>
   <name>hadoop-yarn</name>
 
diff --git a/hadoop-yarn-project/pom.xml b/hadoop-yarn-project/pom.xml
index 9e4ab88..17196db 100644
--- a/hadoop-yarn-project/pom.xml
+++ b/hadoop-yarn-project/pom.xml
@@ -18,12 +18,12 @@
   <parent>
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project</artifactId>
-    <version>2.4.1</version>
+    <version>2.4.1-withGranula</version>
     <relativePath>../hadoop-project</relativePath>
   </parent>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-yarn-project</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <packaging>pom</packaging>
   <name>hadoop-yarn-project</name>
   <url>http://hadoop.apache.org/yarn/</url>
diff --git a/pom.xml b/pom.xml
index 13dbf49..55e8978 100644
--- a/pom.xml
+++ b/pom.xml
@@ -18,7 +18,7 @@ xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xs
   <modelVersion>4.0.0</modelVersion>
   <groupId>org.apache.hadoop</groupId>
   <artifactId>hadoop-main</artifactId>
-  <version>2.4.1</version>
+  <version>2.4.1-withGranula</version>
   <description>Apache Hadoop Main</description>
   <name>Apache Hadoop Main</name>
   <packaging>pom</packaging>
-- 
1.9.1

